
### Delta Lake Versioning


In Databricks Delta Lake, you can leverage Delta's versioning and time travel capabilities to access different versions of a table and track its history. Here's how to achieve both tasks:

1. Getting a Baseline of Different Versions

Delta Lake allows you to retrieve a specific version of the data using Time Travel. To get the baseline of different versions, you can specify the version number or timestamp when querying the Delta table.

Example: Querying by Version Number
You can specify a particular version of the Delta table using the VERSION AS OF syntax.

```sql
SELECT * 
FROM delta.`/path/to/delta-table`
VERSION AS OF <versionNumber>;
```

Example: Querying by Timestamp
Alternatively, you can use a timestamp to retrieve the data as it was at a particular point in time:
    
```sql
SELECT * 
FROM delta.`/path/to/delta-table`
TIMESTAMP AS OF '2023-09-30T00:00:00.000Z';
```

2. Listing and Checking the History of Updates

Delta Lake keeps a transaction log for every table, which can be accessed to see the entire history of operations, such as updates, inserts, deletes, etc.

You can use the DESCRIBE HISTORY command to list all the changes made to the Delta table.

Example: Listing the History
To get a complete history of changes made to the Delta table:    
    
```sql
DESCRIBE HISTORY delta.`/path/to/delta-table`;
```

 This will show you all versions of the table, along with metadata like the operation (e.g., update, insert), the user who made the change, and the timestamp.

Example: Limiting the History
If you want to limit the number of rows returned, you can add the LIMIT clause:

```sql 
DESCRIBE HISTORY delta.`/path/to/delta-table` LIMIT 10;
```

3. Additional Metadata from History

The DESCRIBE HISTORY command gives you a detailed history, including:

Version: The version number of the table.
Timestamp: When the operation occurred.
Operation: What kind of operation (write, update, delete).
Operation Parameters: Specific details about the operation.
User Metadata: Any custom information set by the user during the operation.
Cluster ID: The ID of the cluster used for the operation.


Combining the Two:
Once you have the version number from the DESCRIBE HISTORY command, you can query those specific versions using the time travel syntax shown earlier to get the baseline for each version.

This will allow you to inspect any historical version of your Delta table in Databricks Delta Lake.


###  Version Retention
   
In Databricks Delta Lake, the number of versions that can be kept and the retention period for old versions can be controlled through retention settings. By default, Delta Lake keeps a history of all changes indefinitely, but you can configure how long the old versions and their associated logs are retained.


1. Default Version Retention Policy

By default, Delta Lake retains:
- 30 days of version history.
- 7 days of log files for commits in the transaction log.

This means that, unless configured otherwise, Delta Lake will retain at least 30 days of historical data (including versions and snapshots), which can grow quickly if there are frequent updates to the table.

2. How to Configure Retention Policy

You can configure how long Delta Lake keeps old versions by adjusting the retention period using two methods:

a. Delta Table Properties (delta.logRetentionDuration and delta.deletedFileRetentionDuration)

Delta Lake allows you to configure table properties to control the retention of log and data files.

- delta.logRetentionDuration: Controls how long the transaction logs are retained (default is 30 days).
- delta.deletedFileRetentionDuration: Controls how long the physical data files of deleted records are kept (default is 7 days).

You can set these properties at the table level:

``` sql
-- Set the retention duration for logs and deleted files
ALTER TABLE delta.`/path/to/delta-table`
SET TBLPROPERTIES (
'delta.logRetentionDuration' = '7 days',         -- adjust the log retention period
'delta.deletedFileRetentionDuration' = '7 days'  -- adjust the retention for deleted files
);
```

For example, reducing delta.logRetentionDuration to 7 days will remove old version logs after 7 days, and setting delta.deletedFileRetentionDuration to 7 days ensures that files marked for deletion will be purged after 7 days.

b. Automatic Cleanup of Old Versions with VACUUM

The VACUUM command removes old files that are no longer needed by Delta Lake. It cleans up files older than the configured retention threshold (default is 7 days for deleted files). You can use this command to remove old versions and associated files manually or on a scheduled basis.

Example of Vacuuming to Remove Old Data:

``` sql
-- Clean up old data files older than a specified retention period (in this case, 7 days)
VACUUM delta.`/path/to/delta-table` RETAIN 7 HOURS;~
```

Important: The minimum retention period for vacuuming is 168 hours (7 days) to prevent accidental data loss. However, you can force a shorter retention period by overriding this restriction using the spark.databricks.delta.retentionDurationCheck.enabled configuration.

Then you can run the VACUUM command with a shorter retention period (like hours or minutes).


3. Best Practices for Managing Versions and Retention

- Set appropriate retention policies: Adjust the delta.logRetentionDuration and delta.deletedFileRetentionDuration settings based on your use case. For high-frequency updates, reducing the retention window can help manage storage consumption.

- Use VACUUM regularly: Schedule the VACUUM command to automatically clean up old data that is no longer needed. However, be cautious when using vacuuming to ensure you don’t accidentally delete data that might be needed for time travel queries.

- Archiving: If you still need to preserve historical data or versions but don’t want them taking up space in Delta Lake, consider archiving the older data to external storage or backup locations.

By tuning these settings and regularly vacuuming your Delta tables, you can effectively manage the number of versions retained and control storage costs.


### Hanlde Incrementals

Delta Lake provides an API to retrieve only the latest versions of records, even if multiple updates have occurred. The main approach to achieve this is by using Delta Lake’s latest snapshot of the table, which shows only the most recent version of each record. However, if you need to filter updates based on a specific timestamp, you may need to use additional filtering and window functions. Here’s how you can approach this:

#### Method 1: Direct Query for Latest Snapshot
By default, reading a Delta table without specifying any time travel version will give you the latest version of each record:

```scala
// Load the latest snapshot of the Delta table, which includes only the most recent version of each record
// Load the latest snapshot of the Delta table
val table_name = "`ag-content-pre-prod`.org_master_workflow_dev_v_2_0_0.org_record_log"
val latestSnapshotDF = spark.read.table(table_name)

latestSnapshotDF.printSchema()
latestSnapshotDF.show(5)

```


#### Method 2: Query with Version Filtering (Only Get Updates After a Specific Time)
If you want the latest version of records that were modified after a specific timestamp, use this approach with a combination of a timestamp filter and window functions.
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

```scala
val table_name = "`ag-content-pre-prod`.org_master_workflow_dev_v_2_0_0.org_record_log"
// Load Delta table
val deltaTable = spark.read.format("delta").table(table_name)
// Specify the timestamp after which you want to consider updates
val specifiedDate = "2024-01-01T00:00:00"

// Filter the records modified after the specified date and select the latest version for each ID
val latestUpdates = deltaTable
  .filter(col("_change_type") === "update") // if Delta table uses `MERGE` or has a `_change_type` column
  .filter(col("timestamp") > lit(specifiedDate)) // Consider records updated after this date
  .withColumn("rank", row_number().over(Window.partitionBy("id").orderBy(desc("timestamp"))))
  .filter(col("rank") === 1) // Get the latest version for each unique ID after the specified date
  .drop("rank")

latestUpdates.show(5)
```

#### Method 3: Use Delta Lake’s Time Travel to Read Specific Versions
If you have a specific version (or timestamp) from which you want to see the most recent records, you can also use Delta Lake’s time travel API to read as of a particular version and find changes since that point:

```scala
val table_name = "`ag-content-pre-prod`.org_master_workflow_dev_v_2_0_0.org_record_log"
// Specify a version to time travel to
val version = 10  // for example, replace 10 with the version of interest

// Load the Delta table as of a specific version
val deltaTableVersion = spark.read.format("delta").option("versionAsOf", version).table(table_name)
```

```scala
// Specify the timestamp for time travel
val timestamp = "2024-01-01T00:00:00"

// Load the Delta table as of a specific timestamp
val deltaTableTimestamp = spark.read.format("delta").option("timestampAsOf", timestamp).table(table_name)
```

#### Explanation
- Latest Snapshot: By default, Delta tables show the latest state of each record.
- Timestamp/Version Filtering: Filters records modified after a certain timestamp, retrieving only the most recent update for each record.
- Time Travel: Allows access to Delta tables as they existed at a particular version or timestamp, useful for historical analysis.

This approach provides flexibility for retrieving the latest updates under specific conditions without needing to manage duplicate entries or outdated records manually.


#### Method 4: Approach Using Delta Lake Change Data Feed (CDF)

To get records updated only on a specific date, such as "10/5," you can leverage Delta Lake’s Change Data Feed (CDF) if it’s enabled. Delta Lake CDF can track row-level changes, making it easier to filter records by the specific date they were updated.

##### Approach Using Delta Lake Change Data Feed (CDF)
Assuming CDF is enabled on your Delta table, you can use a WHERE clause with _change_type to capture the rows changed on a particular date:

```scala
import org.apache.spark.sql.functions._

// Specify the date you're interested in
val specifiedDate = "2024-10-05"

// Read CDF changes from the Delta table
val updatesOnSpecifiedDate = spark.read.format("delta")
  .option("readChangeData", "true")
  .option("startingTimestamp", specifiedDate)
  .option("endingTimestamp", specifiedDate)
  .table("your_catalog.your_schema.your_table")
  .filter(col("_change_type") === "update_postimage")

updatesOnSpecifiedDate.show()

```

#### Explanation:

- readChangeData: Enables reading the change data for the Delta table.
- startingTimestamp & endingTimestamp: Restrict the read to a specific date range, here set to "10/5" only.
- _change_type = "update_postimage": Retrieves only the latest version of each updated row after each update, so no duplicates are included.


### USE CASES


#### Use case 1: Update after specific time

To get updates that occurred only after a specific timestamp (e.g., 10/3) and ensure that only the most recent update for each record is included in your results, you can follow these steps using Delta Lake and Spark:

##### Steps
1. Read the Delta Table: Load the Delta table as a DataFrame.
2. Filter by Timestamp: Filter records where the lastUpdatedDate is after 10/3.
3. Window Function: Use a window function to get the most recent update for each record.
4. Select Latest Records: Filter the DataFrame to include only the latest version of each record.

##### Example Code
Here’s how you can implement this in Scala with Spark:

```scala
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

// Load the Delta table
val deltaTableDF = spark.read.format("delta").table("catalog.schema.tableName")

// Define the timestamp
val specifiedTimestamp = "2024-10-03"

// Filter records that have been updated after the specified timestamp
val updatedRecordsDF = deltaTableDF.filter($"lastUpdatedDate" > specifiedTimestamp)

// Use a window function to find the latest update for each record
val windowSpec = Window.partitionBy("id").orderBy($"lastUpdatedDate".desc)

// Add a rank column to identify the most recent record for each id
val latestRecordsDF = updatedRecordsDF
  .withColumn("rank", row_number().over(windowSpec))
  .filter($"rank" === 1) // Keep only the most recent update for each record
  .drop("rank") // Optionally drop the rank column

// Show the results
latestRecordsDF.show()

```

#### Use case 2: Update between specific duration

 you can return records that were updated within a specific duration (e.g., from 10/2 to 10/20) in Delta Lake while ensuring that you get the latest version of each record without duplicates. To achieve this, you can follow these steps:

##### Steps
1. Use Time Travel to query the Delta table within the specified time range.
2. Filter the DataFrame based on the update timestamps.
3. Use Window Functions to get the latest version of each record.

Here's how you can implement this in Scala:

##### Example Code
 
```scala
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

// Define the date range
val startDate = "2024-10-02T00:00:00"
val endDate = "2024-10-20T23:59:59" // End of the day for inclusive filtering

// Load the Delta table with time travel option
val deltaTable = spark.read.format("delta")
  .option("timestampAsOf", endDate) // Load the state of the table at endDate
  .table("your_catalog.your_schema.your_table")

// Filter records updated in the specified duration
val filteredDF = deltaTable.filter(col("lastUpdatedDate").between(startDate, endDate))

// Define a window specification to get the latest version for each record
val windowSpec = Window.partitionBy("id").orderBy(col("lastUpdatedDate").desc)

// Get the latest version of each record
val latestRecordsDF = filteredDF
  .withColumn("rank", row_number().over(windowSpec))
  .filter(col("rank") === 1) // Keep only the latest version of each record
  .drop("rank") // Drop the rank column if not needed

// Show the result
latestRecordsDF.show()
```

#### Use case 3: Update before specific time

To get all unique updates before a specific timestamp using Delta Lake and Spark, you can follow a similar approach as before but adjust the filter condition to capture records updated before the specified time. Here's how to do it step by step in Scala:

##### Steps 
1. Load the Delta Table: Load the Delta table as a DataFrame.
2. Filter by Timestamp: Filter records where the lastUpdatedDate is before the specified timestamp.
3. Window Function: Use a window function to get the most recent update for each record before the specified time.
4. Select Latest Records: Filter the DataFrame to include only the latest version of each record.

##### Example Code
Here’s an example of how you can implement this:
```scala
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

// Load the Delta table
val deltaTableDF = spark.read.format("delta").table("catalog.schema.tableName")

// Define the timestamp
val specifiedTimestamp = "2024-10-03"

// Filter records that have been updated before the specified timestamp
val updatedRecordsDF = deltaTableDF.filter($"lastUpdatedDate" < specifiedTimestamp)

// Use a window function to find the latest update for each record
val windowSpec = Window.partitionBy("id").orderBy($"lastUpdatedDate".desc)

// Add a rank column to identify the most recent record for each id
val latestRecordsDF = updatedRecordsDF
  .withColumn("rank", row_number().over(windowSpec))
  .filter($"rank" === 1) // Keep only the most recent update for each record
  .drop("rank") // Optionally drop the rank column

// Show the results
latestRecordsDF.show()

```

#### Use case 3a: Update before specific time

```scala
spark.read.format("delta").option("timestampAsOf", timestamp).table(table_name),
```
All Records Updated on and Before the Specified Timestamp: It will return all records that were updated on or before the specified timestamp. So, if your timestampAsOf is set to "10/5", it will include:
- Records that were updated on "10/5".
- Records that were updated prior to "10/5".

##### Key Points
- Behavior: Includes updates up to and including the specified timestamp.
- No Filtering for Exact Matches: The timestamp does not filter to only exact matches; it includes all previous updates.
