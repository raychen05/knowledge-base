
--- 

### Common Delta Table Properties

1. delta.enableChangeDataCapture:

Type: Boolean
Description: Enables Change Data Capture (CDC) for the Delta table. Set to true to enable CDC.

2. delta.mergeSchema:

Type: Boolean
Description: If set to true, allows merging of schemas when writing data into the Delta table.

3. delta.retentionDuration:

Type: String (Duration format)
Description: Specifies how long to retain the history of changes for this table. Default is 30 days.

4. delta.minReaderVersion:

Type: Integer
Description: Specifies the minimum reader version that is required to read the Delta table.

5. delta.minWriterVersion:

Type: Integer
Description: Specifies the minimum writer version that is required to write to the Delta table.

6. delta.isolationLevel:

Type: String
Description: Defines the isolation level for transactions on the Delta table. Common values are Serializable or WriteSerializable.

7. delta.dataSkippingNumIndexedCols:

Type: Integer
Description: Specifies the number of indexed columns for data skipping.

8. delta.columnMapping.mode:

Type: String
Description: Specifies how column mapping should be handled. Values can be none, name, or id.

9. delta.checkpoint.interval:

Type: Integer
Description: Specifies the number of transactions after which a checkpoint should be created. Default is 10.

10. delta.optimizeWrite:

Type: Boolean
Description: If set to true, optimizes the layout of files for writes to improve performance.

11. delta.autoOptimize.optimizeWrite:

Type: Boolean
Description: If set to true, automatically optimizes the layout of files when writing to the Delta table.

12. delta.autoOptimize.autoCompact:

Type: Boolean
Description: If set to true, automatically compacts small files when writing to the Delta table.

--- 

#### Example 
The following are examples to create the delta table with the specific proproties
Example 1:

```sql
  CREATE TABLE my_table
  USING delta
  TBLPROPERTIES (
    'delta.enableChangeDataCapture' = 'true',
    'delta.mergeSchema' = 'true',
    'delta.retentionDuration' = 'interval 30 days'
  )
  AS SELECT * FROM source_table;

```

Example 2:

```scala
  // Set additional properties after creating the table
  spark.sql(s"""
    ALTER TABLE delta.`$deltaTablePath`
    SET TBLPROPERTIES (
      'delta.enableChangeDataCapture' = 'true',
      'delta.mergeSchema' = 'true',
    )
""")

```

Example 3:

```scala
  import org.apache.spark.sql.Row
  import org.apache.spark.sql.SparkSession
  import io.delta.tables._

  // Initialize Spark Session
  val spark: SparkSession = SparkSession.builder()
    .appName("Delta Table Example")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .getOrCreate()

  // Define your schema (wos_schema) here
  val wos_schema = ??? // Define your schema

  // Create an empty DataFrame using the schema
  val emptyDF = spark.createDataFrame(
    spark.sparkContext.emptyRDD[Row],  // Empty RDD
    wos_schema                             // Schema defined earlier
  )

  // Write the empty DataFrame to the Delta table with properties
  emptyDF.write
    .format("delta")
    .mode("overwrite")  // Overwrite if any table exists at this location
    .option("delta.enableChangeDataCapture", "true")  // Example property
    .option("delta.mergeSchema", "true")  // Example property to merge schemas
    .save(deltaTablePath)

```

--- 

### List of _change_type  values

In Delta Lake tables that have Change Data Feed (CDF) enabled, the _change_type column provides information about the type of change applied to each row. The valid values for _change_type are as follows:

- **insert**: Indicates that the row was newly inserted into the table in this version.
- **update_preimage**: Represents the old version of a row that was updated. It shows the state of the row before the update took place.
- **update_postimage**: Represents the new version of a row after an update. It shows the updated state of the row.
- **delete**: Indicates that the row was deleted from the table in this version.

--- 

###  Version vs. readVersion 

In Delta Lake, the terms version and readVersion are used to track different aspects of table versions, often seen when checking the Delta table's history or metadata.

#### Key Differences:

1. version:

- Refers to the latest commit version of the Delta table.
- Each time data is added, modified, or deleted in the Delta table, a new version is created, representing the most recent state of the table after that change.
- For example, if there are updates or upserts, the version number increments with each change committed to the table.
- Use Case: Use version to specify which version of the Delta table you want to read or revert to.

2. readVersion:

- Refers to the specific version that was read or queried when performing a read operation.
- This is usually visible when using Change Data Feed (CDF) or examining the Delta table’s transaction logs.
- Use Case: Use readVersion in diagnostic or history tracking contexts to know which version was accessed during a read operation.

--- 

### Delta Table Read Options

Delta Lake provides several options you can use when reading data from Delta tables, allowing for specific configurations based on your use case. Here’s a list of common options you can use:


1. Version-based Options

- .option("versionAsOf", versionNumber): Reads the table as it was at a specific version.
- .option("timestampAsOf", "timestamp"): Reads the table as it was at a specific timestamp. Format is yyyy-MM-dd HH:mm:ss or an ISO 8601-compliant timestamp.

2. Change Data Feed (CDF) Options

- .option("readChangeData", "true"): Enables reading of Change Data Feed (CDF) if the Delta table has CDF enabled. Returns only the data that has changed.
- .option("startingVersion", versionNumber): Specifies the starting version for CDF reading.
- .option("endingVersion", versionNumber): Specifies the ending version for CDF reading.
- .option("startingTimestamp", "timestamp"): Specifies the starting timestamp for CDF reading.
- .option("endingTimestamp", "timestamp"): Specifies the ending timestamp for CDF reading.

3. Time Travel Options

- These are similar to version-based options but allow you to retrieve historical versions based on specific points in time:
  - .option("versionAsOf", versionNumber): Reads the table as it was at a specific version.
  - .option("timestampAsOf", "timestamp"): Reads the table as it was at a specific timestamp.

4. File-based Options

- .option("path", "/path/to/table"): Specifies the path of the Delta table if it’s not registered in the metastore.
- .option("recursiveFileLookup", "true"): Enables recursive file lookup to load data from nested directories. Primarily used when data is spread across multiple folders.
  
5. Column Mapping Options

- .option("mergeSchema", "true"): Allows merging of schemas when reading or writing data, which is useful if columns have been added over time.
- .option("overwriteSchema", "true"): Allows you to overwrite the schema of an existing Delta table, useful when the schema has changed.
  
6. Data Skipping and Caching Options

- .option("dataSkippingNumIndexColumns", numColumns): Controls the number of indexed columns used for data skipping. This can optimize query performance for specific columns.
- .option("skipChangeCommits", "true"): Skips change data for commits when reading data, ignoring any non-data changes.

7. Format-specific Options

- .option("format", "delta"): Specifies the Delta format explicitly, though .format("delta") is often enough without this additional option.


##### Example

Here’s how you might use multiple options together:

```scala
  val df = spark.read
    .format("delta")
    .option("readChangeData", "true")
    .option("startingVersion", 2)
    .option("endingVersion", 5)
    .option("mergeSchema", "true")
    .load("/path/to/delta-table")
```

--- 

### Modes when writing data to a Delta table

- **append**: Adds new records to the existing table without affecting the existing data. Use this mode to add rows without overwriting or modifying existing records.
- **overwrite**: Replaces the entire table content with the DataFrame being written. This mode will overwrite existing data in the table.
- **ignore**: Does not write anything if the table already exists, essentially leaving the table untouched. This mode can be useful when you want to avoid overwriting data if the table is already created.
- **error or errorifexists** (default mode): Throws an error if the table already exists, preventing any data write in case of conflict.

Example Usage for Managed Tables

```scala
  df.write
    .format("delta")
    .mode("append")
    .saveAsTable("database_name.table_name")
```

- .save(deltaTablePath):  Writes to the specified Delta table path. 
- .saveAsTable("tableName"): if writing to a managed table by name.

These modes provide flexibility for different write behaviors based on your data management requirements.

--- 

### cloudFiles Options

When using Databricks Auto Loader with the cloudFiles format, several options can be configured to customize how files are ingested from cloud storage (such as AWS S3 or Azure Blob Storage). Below is a comprehensive list of options and properties that you can use with Auto Loader's cloudFiles:

#### General Options

1. cloudFiles.format:
- Specifies the file format of the incoming data. Supported formats include:
    "json"
    "csv"
    "parquet"
    "orc"
    "avro"
    "text"
    "delta"
2. cloudFiles.inferColumnTypes:
- If set to true, Auto Loader will infer the schema from the files. Default is true.

3. cloudFiles.schemaLocation:
- Path to a Delta table or a directory to store the schema of the incoming files. This can speed up schema inference.

4. cloudFiles.maxBytesPerTrigger:
- Maximum number of bytes to be processed per trigger. This is useful for managing load.

5. cloudFiles.maxFilesPerTrigger:
- Maximum number of files to be processed per trigger. Helps to avoid processing too many files at once.

6. cloudFiles.schemaEvolutionMode:
- Controls how schema changes are handled. Options include:
  - "addColumns": Only allows adding new columns.
  - "mergeSchema": Merges the schemas of incoming files.

7. cloudFiles.allowOverwriting:
- When set to true, allows overwriting files in the target location. Default is false.

8. cloudFiles.includeExistingFiles:
- If set to true, existing files in the directory will be included in the stream.

9. cloudFiles.useNotifications:
- When set to true, uses cloud storage notifications to detect new files, which can be more efficient. Default is false.
AWS-Specific Options


#### AWS-Specific Options

1. cloudFiles.credentialProvider:
- Specifies the type of credentials to use when accessing AWS S3, such as:
  - "default": Uses default credentials.
  - "instanceProfile": Uses EC2 instance profile credentials.
  
2. cloudFiles.s3Endpoint:
- Endpoint for accessing S3. Useful for using custom or regional endpoints.

#### Azure-Specific Options

1. cloudFiles.azurite.enabled:
- If set to true, enables compatibility with Azurite, a local emulator for Azure Storage.

2. cloudFiles.storageAccount:
- Specifies the Azure Storage account name.

3. cloudFiles.storageAccountKey:
- Key for the Azure Storage account.


#### Other Options

1. cloudFiles.triggerInterval:
- Custom interval for triggering the stream processing. Default is set by the processing engine.

2. cloudFiles.fileNameOnly:
- When set to true, only the filenames will be tracked, and the contents will not be read until they are processed.

3. cloudFiles.overrideSchema:
- If set to true, allows the user to override the schema of the existing Delta table.

4. cloudFiles.inputMode:
- Specifies the input mode. Options include:
  - "append": Appends new data.
  - "overwrite": Overwrites existing data.

This list includes many of the key options available for configuring Auto Loader in Databricks. Depending on your specific requirements and the characteristics of your data, you can customize these settings to optimize your streaming ingestion process.