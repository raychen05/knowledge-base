
--- 

### Streaming Write with a Checkpoint

--- 

#### Using Delta Lake with Delta Table in a File Path

In the provided code, a Delta Lake streaming write operation is being set up with a checkpoint, which helps track the progress of the stream. The writeStream method starts a continuous write to a Delta table, saving data incrementally.

```scala
    val wosLocation="s3://wos_delta_bucket"
    df.writeStream
    .format("delta")                                            // Specify Delta as the format
    .option("checkpointLocation", s"$wosLocation/checkpoint")   // Define checkpoint location
    .start(s"$wosLocation/delta/wos_record_log")                // Define the output Delta table path
```

**Explanation of Each Part**
- writeStream: Enables writing a streaming DataFrame to a target.
- .format("delta"): Specifies Delta Lake as the target format, which allows ACID transactions and optimizations.
- .option("checkpointLocation", ...): Sets the checkpoint directory. Checkpointing is essential for stream processing because it:
    - Tracks progress of the streaming job.
    - Manages exactly-once processing by saving metadata of processed records.
    - Allows for recovery in case of job failure.
- .start(...): Begins the streaming write to the specified Delta table path (org_record_log). This starts the job in the background, running it until stopped manually.

--- 

#### Using Delta Lake with Managed Table in a Databricks Catalog

Suppose we want to use a managed Delta table in the Databricks catalog instead of a file path. The checkpoint directory will still be specified, but the Delta table is managed by Databricks and resides in a catalog (database).

```scala
    // Define catalog (database) and table names
    val databaseName = "my_catalog.my_database"
    val tableName = "org_record_log"

    // Specify the checkpoint directory
    val checkpointLocation = s"/path/to/checkpoints/$databaseName/$tableName"

    // Start the streaming write to a Delta table in the catalog
    df.writeStream
    .format("delta")
    .option("checkpointLocation", checkpointLocation) // Specify checkpoint directory
    .table(s"$databaseName.$tableName")               // Write to a Delta table in the catalog
    .start()

```
**Key Points in this Example**
- table(s"$databaseName.$tableName"): Instead of a file path, we specify a cataloged Delta table using database.table.
- Checkpoint Location: The checkpoint location must still be set, as it manages state information.

This setup is particularly useful in Databricks, where tables managed in the catalog can be accessed across multiple notebooks and jobs, maintaining data consistency and ease of access.

--- 

#### Checkpoint Management

To manage checkpoints effectively:
- Path Naming Conventions: Organize checkpoint paths to avoid overlap with other jobs.
- Checkpoint Cleanup: Periodically clear out unnecessary checkpoints for completed streams to manage storage.

Both examples ensure efficient and reliable streaming with Delta Lake by using checkpoints, which allow for exactly-once processing, progress tracking, and recovery in case of failures.

--- 
### Steaming over a specific duration

To set up a checkpoint in Delta Lake that tracks changes over a specific duration (e.g., 5 days), you can configure the stream to trigger once every 5 days. This approach will batch the updates and process them periodically without needing continuous stream processing. Here’s how to achieve it:

**Step-by-Step Setup**
- Set Up the Trigger Interval: Specify the trigger interval to read and process updates every 5 days.
- Define the Checkpoint Location: Checkpointing helps track the last processed data, allowing for incremental reads.
- Configure the Stream to Process Data Since the Last Checkpoint: Use the readChangeData option if you have Change Data Feed (CDF) enabled on the Delta table to read incremental changes based on a checkpointed version.

Here’s the code example in Scala to read and process updates in batches with a 5-day interval.

```scala
    // Define Delta table path or catalog table name
    val deltaTablePath = "path/to/delta-table" // or "my_catalog.my_database.table_name"

    // Define checkpoint location
    val checkpointLocation = "/path/to/checkpoint/location"

    // Set up a streaming read with a 5-day trigger interval
    val deltaStream = spark.readStream
    .format("delta")
    .option("readChangeData", "true")     // Read changes only if CDF is enabled
    .option("startingTimestamp", "5 days ago") // Optional: reads data only after a specific timestamp
    .load(deltaTablePath)                 // Load from Delta table

    // Start writing stream to Delta table with 5-day intervals and checkpointing
    deltaStream.writeStream
    .format("delta")
    .option("checkpointLocation", checkpointLocation)
    .trigger(Trigger.Once)                // Triggers once, processes all changes since last checkpoint
    .start(deltaTablePath)                // Write processed changes to Delta table
```

**Explanation of Key Configurations**

- trigger(Trigger.Once): Runs the stream once, capturing all changes since the last checkpoint, effectively batching updates every time it’s triggered.
- Checkpoint Location: The checkpoint directory manages the state for each 5-day read, tracking the last processed point. When the stream is restarted, it resumes from the last checkpoint to avoid duplicate reads.
- readChangeData Option: This reads changes (inserts, updates, deletes) when Delta’s Change Data Feed is enabled. Without CDF, only newly appended data would be processed.
  
**Running the Stream at Intervals**
To run this stream once every 5 days, you could:
- Set up a scheduled job (e.g., using a cron job or Databricks Jobs) to execute this Spark code every 5 days.
This setup ensures that the stream reads and processes data updates periodically without needing continuous operation.

--- 

### Deduplication

When processing Delta Lake changes in a streaming job, if a record is updated multiple times within a set duration (e.g., 5 days), using the Change Data Feed (CDF) in Delta Lake can indeed retrieve multiple versions of that record. To get only the latest version for each record without duplicates, you can apply a window function or deduplication step on the stream to retain the most recent update.


Here’s a breakdown of the approach:

1. Use Change Data Feed (CDF) with Delta Table
Ensure CDF is enabled on the Delta table, allowing the stream to capture all changes (including inserts, updates, and deletes).

```sql
    -- Enable CDF on the Delta table
    ALTER TABLE my_delta_table SET TBLPROPERTIES (delta.enableChangeDataFeed = true);

```

2. Set Up the Stream to Read CDF Data
Use readChangeData to read CDF changes in the stream, filtering only the latest version of each record based on a timestamp or version column.

3. Deduplicate by Key and Timestamp
To keep only the latest version of each record within the window, use a deduplication logic with window functions. Here’s the full example in Scala:

```scala
    import org.apache.spark.sql.functions._
    import org.apache.spark.sql.expressions.Window

    // Define Delta table path or catalog table name
    val deltaTablePath = "path/to/delta-table"  // or "my_catalog.my_database.table_name"

    // Define checkpoint location
    val checkpointLocation = "/path/to/checkpoint/location"

    // Set up the streaming read with CDF
    val deltaStream = spark.readStream
    .format("delta")
    .option("readChangeData", "true")
    .option("startingTimestamp", "5 days ago")  // Read changes starting 5 days ago
    .load(deltaTablePath)

    // Window specification to select the latest version of each record
    val windowSpec = Window.partitionBy("recordId")  // Replace with the primary key column
                        .orderBy(desc("timestamp"))  // Use update timestamp for ordering

    // Apply the deduplication logic
    val latestUpdates = deltaStream
    .withColumn("rank", row_number().over(windowSpec)) // Rank by latest timestamp
    .filter($"rank" === 1) // Keep only the latest record per key
    .drop("rank") // Drop rank column

    // Write the deduplicated stream to Delta table
    latestUpdates.writeStream
    .format("delta")
    .option("checkpointLocation", checkpointLocation)
    .trigger(Trigger.Once) // Trigger once every 5 days, if scheduled
    .start("path/to/output/delta-table")
```
**Explanation of the Key Parts**
- Window Spec: This groups records by a unique identifier, such as recordId, ordering them by timestamp to retain only the latest version.
- Deduplication Filter: The filter($"rank" === 1) step selects the most recent update per recordId within the streaming window.

This approach ensures that only the latest update per record is retained, avoiding duplicates even if multiple updates occurred within the same duration.

--- 
### Spark Structured Streaming Read

To set up a streaming read from an S3 location where incremental data is delivered in regular intervals, you can configure a structured streaming job in Spark to monitor this location and process incoming data as it arrives. Spark Structured Streaming can continuously watch the S3 directory for new files and ingest them into your Delta Lake or other target systems.

#### Steps to Set Up Streaming Read from S3 Directory

1. Configure S3 Access: Make sure Spark has access to the S3 bucket, either through credentials set in environment variables or through IAM roles if running on EMR.
2. Define Schema: Define the schema of the incoming data if the files are in formats like JSON, CSV, or Parquet.
3. Set Up the Stream to Monitor the S3 Directory: Configure readStream to read from the S3 directory.
4. Process and Write the Stream to Delta Lake: Write the stream to a Delta table or other formats, specifying checkpointing for fault tolerance.

#### Example in Scala for Streaming Read from S3

Here’s how you can set up a structured streaming job in Spark to read new files from an S3 path as they arrive:

```scala
    import org.apache.spark.sql.SparkSession
    import org.apache.spark.sql.streaming.Trigger

    val spark = SparkSession.builder
    .appName("S3 Incremental Data Streaming")
    .getOrCreate()

    // Define schema of the incoming data if required (e.g., for JSON/CSV)
    val dataSchema = new StructType()
    .add("column1", StringType)
    .add("column2", IntegerType)
    .add("timestamp", TimestampType) // Example schema, update based on your data structure

    // Define the S3 path where the incremental data is stored
    val s3Path = "s3://your-bucket/path/to/daily/incremental/data"

    // Set up the streaming read from the S3 path
    val streamingInputDF = spark.readStream
    .format("parquet")  // Specify the format of your data, e.g., "csv", "json", "parquet"
    .schema(dataSchema) // If CSV or JSON, specify schema; Parquet infers schema automatically
    .load(s3Path)       // Load data from the S3 path

    // Optional: Apply transformations as needed
    val processedDF = streamingInputDF
    .withColumn("processed_timestamp", current_timestamp())  // Example transformation

    // Write the stream to a Delta Lake table or another sink
    val checkpointLocation = "s3://your-bucket/path/to/checkpoints"

    processedDF.writeStream
    .format("delta")  // Use "delta" to write to a Delta table, or "parquet" to write to S3 in Parquet format
    .option("checkpointLocation", checkpointLocation)  // Specify checkpoint location for fault tolerance
    .trigger(Trigger.ProcessingTime("1 minute"))       // Adjust trigger interval as needed
    .outputMode("append")                              // "append" mode for incremental loads
    .start("s3://your-bucket/path/to/output/delta-table") // Output path or Delta table location
    .awaitTermination()  // Keep the streaming job running
```

**Explanation of Key Components**

1. Schema Definition: Defines the schema of incoming data if necessary. This example assumes the data format is Parquet, which infers the schema. For formats like CSV or JSON, schema specification is required.
2. Read Stream: spark.readStream monitors the S3 directory, ingesting new files as they arrive.
3. Transformations: You can apply transformations, aggregations, or add new columns as needed.
4. Checkpointing: The checkpoint location ensures the streaming job resumes from where it left off in case of failure.
5. Trigger: Trigger.ProcessingTime("1 minute") checks for new data every minute. Adjust as per your data arrival frequency and latency requirements.
6. Output Mode: The append mode is ideal for incremental loads. You can switch to update or complete modes depending on your processing needs.

This setup will ensure that Spark reads new batches of data from the specified S3 path as they become available, processes them, and writes them to the Delta Lake or your chosen output format in an efficient and fault-tolerant manner.

--- 

### Streaming read using Databricks' Auto Loader

This code snippet demonstrates how to configure a streaming read using Databricks' Auto Loader, which is designed to efficiently ingest files incrementally from cloud storage locations like S3 or Azure Blob Storage. Here, it loads data in JSON format from a specified location, inferring schema based on a defined structure and controlling file and data processing throughput. Below, I’ll break down the purpose of each option and provide the Scala equivalent.

#### Explanation of Key Components

1. Format: format("cloudFiles") tells Spark to use Databricks' Auto Loader, which is designed for handling streaming data from cloud storage.

2. Options:
    - cloudFiles.format: Specifies the file format of the source data—in this case, "json".
    - cloudFiles.inferColumnTypes: When set to "false", prevents Auto Loader from automatically inferring column types, which can help improve performance.
    - cloudFiles.maxBytesPerTrigger: Limits the maximum amount of data processed in each trigger, set here to 100gb, which allows Auto Loader to process up to 100GB per trigger cycle.
    - cloudFiles.maxFilesPerTrigger: Limits the maximum number of files processed per trigger cycle; here, it's set to 100000.

3. Schema:
    - .schema(wos_schema): Specifies the schema for JSON files, required here because column type inference is disabled. wos_schema should be a StructType that matches the structure of the JSON data.

4. Load Location:
    - .load(wosFileLocation): Specifies the cloud storage path where files are located. Auto Loader will monitor this location and automatically process any new files.


#### Scala Code for the Example

```scala
    import org.apache.spark.sql.SparkSession
    import org.apache.spark.sql.types.StructType

    // Assuming `wos_schema` is a StructType defined to match the structure of your JSON data
    val wos_schema = new StructType()
    .add("column1", "String")
    .add("column2", "Integer")
    // Define other fields as per JSON structure

    // Define the path to the JSON files in the cloud storage
    val wosFileLocation = "s3://your-bucket/path/to/json/files"

    val orgRecordLog = spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json")
    .option("cloudFiles.inferColumnTypes", "false")
    .option("cloudFiles.maxBytesPerTrigger", "100gb")
    .option("cloudFiles.maxFilesPerTrigger", "100000")
    .schema(wos_schema)  // Apply predefined schema
    .load(wosFileLocation)  // Load files from cloud location

    // Optionally show a preview of the loaded streaming data
    orgRecordLog.writeStream
    .format("console")
    .start()
    .awaitTermination()
```

--- 

### Streaming Read Trigger Processing Based on Checksum File

To use a checksum file as a trigger for processing data after all uploads are complete, you can configure a structured streaming job to monitor the S3 directory and process files only when the checksum file is detected. Here’s a high-level approach to implement this in Spark with Scala:

1. Upload Data and Checksum File: Ensure that all data files are uploaded to the S3 path before the checksum file, which acts as a signal for completeness.
2. Read Files with Auto Loader: Configure Spark to continuously monitor the S3 location with cloudFiles, but add a check that only processes data after the checksum file is detected.
3. Conditionally Process Data: Use the presence of the checksum file to start processing all files in the folder.


#### Example Scala Code to Trigger Processing Based on Checksum File

```scala
    import org.apache.spark.sql.SparkSession
    import org.apache.spark.sql.functions._

    // Define path variables
    val dataLocation = "s3://your-bucket/path/to/data/"
    val checksumFilePath = "s3://your-bucket/path/to/data/checksum.txt"

    // Define a streaming DataFrame for the data location
    val dataStreamDF = spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json")  // Replace with actual format if different
    .option("cloudFiles.inferColumnTypes", "false")
    .option("cloudFiles.includeExistingFiles", "true") // Load existing files upon start
    .load(dataLocation)

    // Define a function to check for the checksum file in S3
    def isChecksumFilePresent: Boolean = {
    val fs = org.apache.hadoop.fs.FileSystem.get(spark.sparkContext.hadoopConfiguration)
    fs.exists(new org.apache.hadoop.fs.Path(checksumFilePath))
    }

    // Start the stream processing with a trigger condition
    val query = dataStreamDF.writeStream
    .format("delta")
    .option("checkpointLocation", "s3://your-bucket/path/to/checkpoint")
    .foreachBatch { (batchDF, _) =>
        // Check for checksum file presence before processing
        if (isChecksumFilePresent) {
        batchDF.write
            .format("delta")
            .mode("append")
            .save("s3://your-bucket/path/to/delta-table")
        }
    }
    .start()

    query.awaitTermination()

```

**Explanation**
- File Monitoring: The code initializes dataStreamDF to monitor files in dataLocation, but processes them only if the checksum file exists.
- Checksum Check: The isChecksumFilePresent function checks for the existence of the checksum file before processing each batch.
- Batch Processing: The .foreachBatch method lets you manage data batches conditionally. Here, it only saves to the Delta table if the checksum file is detected.

This way, the job will wait until all files are available in S3 before processing, ensuring complete data integrity.

--- 

### Structured Streaming vs.  Databricks’ Auto Loader

Between Spark's Structured Streaming with a Trigger.ProcessingTime and Databricks’ Auto Loader using cloudFiles, the best approach generally depends on:

- Scale of data ingestion (frequency and volume of new data)
- Performance requirements (latency and throughput)
- File source characteristics (such as if you’re reading from cloud storage with frequent uploads)


#### Comparison

#### 1. Spark Structured Streaming with Trigger ProcessingTime

- Trigger Type: The .trigger(Trigger.ProcessingTime("1 minute")) approach runs every minute regardless of new data availability.
- Use Case: Suitable for scenarios where latency is not critical and data arrives in predictable intervals.
- Limitations: It does not optimize for varying data arrival rates and may process empty batches if no new data is available. You may also experience performance issues for large or highly variable datasets, as it processes all available data in each interval.

#### 2. Databricks Auto Loader (cloudFiles)

- Trigger Type: Auto Loader dynamically adjusts data ingestion based on new files detected in the source. Options like cloudFiles.maxFilesPerTrigger and cloudFiles.maxBytesPerTrigger allow fine-grained control over how many files or bytes to process per trigger.
- Use Case: Ideal for handling data ingested from cloud storage, especially in unpredictable or high-volume data pipelines.
- Advantages:
    - Auto-scaling: It handles scaling based on the number and size of new files rather than fixed intervals.
    - Efficiency: It avoids empty trigger intervals since it only processes when new files are available.
    - Scalability: Works well with large data volumes or frequent updates, especially for cloud storage such as S3 or ADLS.
    - Reliability: Integrates well with Delta Lake for ACID-compliant incremental data loading.
  
- Performance Tuning: Use cloudFiles.maxFilesPerTrigger and cloudFiles.maxBytesPerTrigger to adjust how much data is processed per trigger. cloudFiles is particularly effective for high-throughput pipelines because it manages file-based metadata efficiently.

#### Recommendations

For high-frequency or high-volume cloud data ingestion, Databricks Auto Loader (cloudFiles) is generally the better option due to its adaptive design, which improves performance and resource utilization. Auto Loader works seamlessly with cloud storage, handles dynamic ingestion patterns, and optimizes data retrieval, making it ideal for demanding streaming use cases.

In contrast, Spark Structured Streaming with Trigger.ProcessingTime is simpler but better suited to smaller, more predictable workloads or scenarios where batch-style processing in fixed intervals is sufficient.

--- 

### Processing a batch of files with a signal file

When processing a batch of files with a signal file indicating that all files have been uploaded to S3, and where data arrival times are unpredictable and may occur several times a day, the best streaming read approach would be Databricks Auto Loader using the cloudFiles format. Here’s why:


#### Reasons for Choosing Databricks Auto Loader

1. Dynamic Ingestion:
- File Detection: Auto Loader can continuously monitor the S3 bucket for new files, processing them as they appear. This is especially useful for unpredictable file arrivals.
- Signal File Handling: You can implement a mechanism that checks for the presence of a signal file (e.g., a completion marker) before processing the batch of files. This approach ensures that you only start processing once all files are confirmed to be uploaded.

2. Efficient Resource Management:
- Controlled Processing: Using options like cloudFiles.maxFilesPerTrigger and cloudFiles.maxBytesPerTrigger, you can control how many files or bytes are processed in each trigger, ensuring efficient resource usage without overwhelming the system.

3. Handling Variable File Sizes:
- Auto Loader can adapt to different file sizes and arrival rates, processing smaller batches quickly when data arrives and scaling to handle larger batches effectively when needed.

4.Checkpointing and Fault Tolerance:
- Auto Loader integrates seamlessly with Delta Lake, which provides built-in support for ACID transactions and fault tolerance. This means you can recover gracefully from failures and avoid data duplication.

5. Simplicity of Implementation:
- Implementing Auto Loader is straightforward. You can write code to check for the presence of the signal file, and once detected, you can trigger the data processing.


#### Example Code
Here’s a simplified Scala example of how to implement this with Databricks Auto Loader, including checking for the signal file:


```scala
    import org.apache.spark.sql.SparkSession
    import org.apache.spark.sql.streaming.Trigger

    val spark = SparkSession.builder.appName("Auto Loader Example").getOrCreate()

    // Define your schema for incoming data
    val schema = ... // Define your schema here

    // Path to your S3 bucket
    val s3BucketPath = "s3://your-bucket/path/to/files"
    val signalFilePath = "s3://your-bucket/path/to/signal-file"

    // Set up Auto Loader to read from S3
    val streamingData = spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json") // or your specific file format
    .option("cloudFiles.inferColumnTypes", "false")
    .schema(schema)
    .load(s3BucketPath)

    // Add logic to check for the presence of the signal file
    val signalFileExists = spark.read.text(signalFilePath).count() > 0

    // Stream processing: Only start if the signal file exists
    if (signalFileExists) {
    streamingData.writeStream
        .format("delta")
        .outputMode("append")
        .option("checkpointLocation", "s3://your-bucket/path/to/checkpoint")
        .start("s3://your-bucket/path/to/delta/table")
    } else {
    println("Signal file not found, waiting for file upload...")
    }

    // Await termination of the streaming query
    spark.streams.awaitAnyTermination()
```

**Conclusion**
Using Databricks Auto Loader is the most suitable approach for this scenario. It provides the necessary flexibility and efficiency to handle unpredictable file arrivals while ensuring that data processing is initiated only once all relevant files are uploaded and confirmed via the signal file. This method optimizes resource usage and simplifies management, making it ideal for dynamic data environments.

