
### 25. Approaches to Access Delta Lake Tables

Yes, you can use AWS services to access Delta Lake tables in Databricks without spinning up a cluster by utilizing the Databricks SQL service, combined with AWS tools. Here are your options:


#### 1. Databricks SQL (Serverless)
   
- Description: Databricks SQL is a serverless offering that allows you to query Delta Lake tables without needing a Databricks cluster.
- How it Works: You can connect AWS services (e.g., Lambda, Glue, or an EC2-based application) to the Databricks SQL endpoint via the Databricks SQL Connector or JDBC/ODBC drivers.
- Key Features:
    - No cluster management.
    - Serverless execution.
    - Pay-per-query pricing model.

**Steps**:

1. Set up a Databricks SQL Warehouse in your Databricks workspace.
2. Use the Databricks SQL Connector for Python, or the JDBC/ODBC driver to query Delta Lake tables.

Example:
```python
    from databricks import sql

    connection = sql.connect(
        server_hostname="<your-databricks-server-hostname>",
        http_path="<your-databricks-sql-http-path>",
        access_token="<your-databricks-personal-access-token>"
    )

    cursor = connection.cursor()
    cursor.execute("SELECT * FROM your_delta_table LIMIT 10")
    results = cursor.fetchall()
    print(results)

    cursor.close()
    connection.close()

```

**Use Cases:**

- Serverless access to Delta Lake.
- Query execution from AWS Lambda, Glue, or other services.


#### 2. AWS Glue with Delta Lake Connector

- Description: AWS Glue can process and query Delta Lake tables using the Delta Lake connector. Glue jobs can read Delta tables stored in S3 without requiring a Databricks cluster.
- Key Features:
    - ETL capabilities.
    - Serverless processing.
    - Native support for Delta Lake.

**Steps:**

1. Store your Delta Lake tables in AWS S3.
2. Use AWS Glue's Delta Lake connector to query the tables.
3. Run SQL queries or Spark jobs directly in Glue.

Example (PySpark):
``` python
    from pyspark.sql import SparkSession

    spark = SparkSession.builder \
        .appName("GlueDeltaExample") \
        .config("spark.jars.packages", "io.delta:delta-core_2.12:2.1.0") \
        .getOrCreate()

    # Load Delta table from S3
    delta_table = spark.read.format("delta").load("s3://your-bucket/delta-table-path")

    delta_table.show()
```

**Use Cases:**

- Data processing or ETL pipelines.
- Integration with AWS Glue Data Catalog.


#### 3. Athena via Delta Lake Manifest

- Description: If your Delta Lake tables are stored in S3, you can use Apache Iceberg or Delta Lake manifest files to expose them as Athena tables.
- How it Works:
    - Generate manifest files for Delta tables.
    - Query those tables via Athena (serverless SQL engine).

**Steps:**

1. Enable Delta Lake manifest generation using:
```sql
ALTER TABLE delta_table_name SET TBLPROPERTIES(delta.compatibility.symlinkFormatManifest.enabled = true);
```

2, Use Athena to query the generated manifests.

**Use Cases:**

- Serverless analytics via Athena.
- Delta tables exposed as Presto-compatible formats.

#### 4. AWS Lambda with Databricks SQL

- Use AWS Lambda to query Databricks SQL endpoints for lightweight serverless data queries.
- Leverage the Databricks SQL Connector for Python or JDBC.


#### Choosing the Right Option:

- For Serverless Query Execution: Use Databricks SQL.
- For ETL Workflows: Use AWS Glue.
- For Lightweight Querying via AWS Native Services: Use Athena with Delta Manifest.








