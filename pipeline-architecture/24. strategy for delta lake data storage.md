
--- 

### A. Strategy for Delta Lake Data Storage


When using a Delta table as a streaming table in Delta Lake, it is generally best practice to upsert (update or insert) data rather than inserting duplicate records for the same record key. This ensures that you maintain a single version of each unique record, which is critical in a streaming context to prevent data inconsistencies and duplicates.

--- 

### Two Main Approaches for Stream Processing with Delta Lake

1. Append-Only Mode (for immutable data):

- If your use case allows for appending new records without needing to modify existing ones, you can stream data directly into the Delta table as new rows.
- This is useful for log data, sensor readings, or other cases where each event is distinct and doesn’t require updates.

2. Upsert Mode (for mutable data):

- If your records need to be updated (such as customer profiles or latest product prices), use upsert logic in Delta Lake. This approach merges new data with existing records based on a unique identifier (e.g., a primary key or unique ID).
- Delta Lake’s MERGE INTO operation is specifically designed to perform this kind of operation efficiently.

--- 

### Example: Using MERGE INTO for Upserts in Streaming
Here’s how you can use MERGE INTO to upsert records from a streaming source to a Delta table:

```scala
import org.apache.spark.sql.SparkSession
import io.delta.tables._

// Assuming `spark` session is already created and the streaming DataFrame `streamDF` is defined
val deltaTable = DeltaTable.forPath(spark, "/path/to/delta_table")

// Define the streaming query with upsert logic

// approach-: Perform the upsert operation
val query = streamDF.writeStream
  .format("delta")
  .foreachBatch { (batchDF: DataFrame, batchId: Long) =>
    deltaTable.as("target")
      .merge(
        batchDF.as("source"),
        "target.id = source.id"  // Matching condition based on unique identifier
      )
      .whenMatched()
      .updateAll()
      .whenNotMatched()
      .insertAll()
  }
  .outputMode("update")
  .start()


    // approach-2: Perform the upsert operation
    deltaTable.as("target")
    .merge(
        newData.as("source"),     // DataFrame to upsert
        "target.id = source.id"   // Condition to match rows
    )
    .whenMatched
    .updateAll()                // Update matched rows with source data
    .whenNotMatched
    .insertAll()                // Insert new rows if not matched
    .execute()


```

--- 

#### Explanation of Key Components
- deltaTable.as("target"): Creates an alias for the Delta table as the target for merging.
- merge(...).whenMatched().updateAll(): Updates records in the Delta table that match the unique identifier (id in this case).
- whenNotMatched().insertAll(): Inserts records if they do not already exist in the Delta table.
- foreachBatch: Allows processing the streaming data in batches for efficient upsert operations.


```python
  from delta.tables import DeltaTable
  from pyspark.sql import SparkSession

  # Assuming `spark` session is already created and data source (stream_df) is defined
  deltaTable = DeltaTable.forPath(spark, "/path/to/delta_table")

  # Process the streaming data with upserts
  query = stream_df.writeStream \
      .format("delta") \
      .foreachBatch(lambda batch_df, batch_id: 
          deltaTable.alias("target").merge(
              batch_df.alias("source"),
              "target.id = source.id"  # Matching condition based on unique identifier
          ).whenMatchedUpdateAll() \
          .whenNotMatchedInsertAll() \
      ) \
      .outputMode("update") \
      .start()
```
#### Explanation of Key Components
- whenMatchedUpdateAll() updates existing records if they match on the unique identifier.
- whenNotMatchedInsertAll() inserts new records that don’t have a matching identifier.

--- 

### Key Considerations

- Primary Key: Ensure you have a unique identifier (like a primary key) in your data to avoid ambiguity when merging records.
- Performance: Upserting in a streaming context can be resource-intensive, especially with high-frequency streams. 
- Optimize your Delta Lake table by partitioning on the primary key or a timestamp to improve write performance.
- Compaction and Cleanup: Periodically optimize and vacuum the Delta table to clean up outdated versions and improve performance.

--- 

### Benefits of Using MERGE INTO for Streaming Delta Tables

- Consistent Records: You avoid having multiple records for the same entity, which is critical in analytics and reporting.
- Efficient Storage: Upserts reduce storage costs by keeping only the latest version of each record.
- Time Travel: Delta Lake's time travel allows you to revert to a previous state, even when using MERGE, as older versions of each update are stored.

This approach allows Delta Lake to handle updates efficiently while preserving the table’s integrity for downstream analysis.

--- 

### B. Rollback to Previous Version

In Delta Lake, you can roll back to a previous version of a Delta table thanks to its time travel feature. Delta Lake stores all changes to the table as "versions," allowing you to query or restore any previous version of the data. Here’s how you can do it:

--- 
1. Query a Previous Version of the Table (Time Travel)
   
- You can directly query an older version of the table without permanently changing the current data:

- Using Version Number:
```sql
  SELECT * FROM my_delta_table VERSION AS OF 5;
  ```

  ```scala
  val table_name = "`wos-family-data`.wos_profile_dev.wos_baseline"
  // Specify the Version for time travel
  val startingVersion = 4
  // Load the Delta table as of a specific timestamp
  val deltaTableTimestamp = spark.read.format("delta").option("startingVersion", 4).table(table_name)

```

- Using Timestamp:
```sql
  SELECT * FROM my_delta_table TIMESTAMP AS OF '2024-10-01T00:00:00';
```

```scala
  val table_name = "`wos-family-data`.wos_profile_dev.wos_baseline"
  // Specify the timestamp for time travel
  val timestamp = "2024-10-29 16:09:04"
  // Load the Delta table as of a specific timestamp
  val deltaTableTimestamp = spark.read.format("delta").option("timestampAsOf", timestamp).table(table_name)

```

- This is useful for auditing or comparing different versions without making permanent changes.

--- 
2. Roll Back the Table to an Older Version (Restore)

- To permanently roll back the table to an older version, you can overwrite the current version with the data from a previous version.
- Using RESTORE (if supported):
  - Some Delta Lake implementations, like Databricks, offer a RESTORE command to revert the table to a previous version.

```sql
RESTORE TABLE my_delta_table TO VERSION AS OF 5;
```

```scala
import io.delta.tables._
val table_name = "`wos-family-data`.wos_profile_dev.wos_baseline"

// Load the Delta table by path or table name (catalog)
val deltaTable = DeltaTable.forName(spark, table_name)

// Restore the Delta table to a specific version
deltaTable.restoreToVersion(5)
```

- Manual Approach:
  - Alternatively, if RESTORE is unavailable, you can create a new table based on a previous version and overwrite the existing table:

```python
  # Using PySpark
  df = spark.read.format("delta").option("versionAsOf", 5).load("/path/to/my_delta_table")
  df.write.format("delta").mode("overwrite").save("/path/to/my_delta_table")

```

```scala
  // Read Delta table at a specific version (version 5)
  val df = spark.read.format("delta")
    .option("versionAsOf", 5)
    .load("/path/to/my_delta_table")

  // Overwrite the Delta table with the data from version 5
  df.write
    .format("delta")
    .mode("overwrite")
    .save("/path/to/my_delta_table")
```

#### Explanation

- Read with versionAsOf: Reads the Delta table as it was at version 5.
- Overwrite Mode: Saves the DataFrame back to the Delta table path, overwriting the current version with data from version 5.

--- 
3. Using the Delta Table History to Determine Rollback Points
   
View the table’s history to find version numbers and timestamps, which can help in selecting the right rollback point.

```sql
DESCRIBE HISTORY my_delta_table;
```

```scala
  import io.delta.tables._

  val table_name = "`wos-family-data`.wos_profile_dev.wos_baseline"
  val deltaTable = DeltaTable.forName(table_name)

  //1.  Retrieve full history of the Delta table
  val fullHistoryDF = deltaTable.history()
  //2.  Retrieve the last 5 versions of the Delta table
  val recentHistoryDF = deltaTable.history(5)

  val jsonDF = recentHistoryDF.toJSON
  jsonDF.show(truncate = false)

  // Display recent updates
  recentHistoryDF.select("version", "timestamp", "userName", "operation", "readVersion").show(truncate = false)
```

#### Things to Consider
- Rolling back removes any data changes made after the chosen version.
- Time travel and rollbacks are dependent on Delta Lake's retention policy, so ensure that older versions aren’t deleted if you plan to rely on rollbacks.

--- 

### C. Insert vs. Upsert (Merge)

In Delta Lake, whether to update an existing record or insert a new record for the same entry depends on your use case and the design of your data pipeline:

1. Insert:

- Performance: Inserting data is generally faster than upserting because it avoids the need to identify and overwrite existing records.
- Use Case: Best for appending new records where there is no overlap with existing data.
- Recommendation: If your data pipeline primarily involves appending new records without frequent updates to existing records, insert is more efficient.

2. Upsert (Merge):

- Performance: Merge operations are slower than insert for large datasets because Delta Lake must check for matching records to determine which rows to update and which to insert.
- Optimization: If upsert is required, ensure that the Delta table has optimized indexing (via Z-ordering on the merge key), and try to limit the frequency of upserts or the size of the incoming data batches.

- Recommendation: If you need to update existing records and cannot afford stale data, merge is necessary but may need optimization. For example:
  - Partitioning: Partitioning the Delta table on columns that filter the data (like dates or unique identifiers) can significantly improve upsert performance.
  - Z-ordering: If queries involve range scans on specific columns, using Z-order clustering on those columns can also optimize merges.

3. Hybrid Approach: Batch Upserts

For even better performance, consider:
- Mini-Batch Upserts: Break up large DataFrame updates into smaller, manageable batches, if possible. This reduces the load and can improve overall throughput.
- Scheduled Upserts: If there’s flexibility in when updates are needed, running upserts during off-peak hours can reduce contention.

4. Summary
- Use Insert New Record if historical tracking is required.
- Use Update Existing Record if only the latest state is needed and storage is a concern.
- Use a Hybrid Approach if both the latest state and historical data are important.

--- 

### D. Explicit createdDate and updatedDate columns?

When using Delta Lake, adding explicit createdDate and updatedDate columns is optional but can still be beneficial depending on your specific use case. Delta Lake provides transactional metadata that allows tracking changes over time, making it easy to access historical versions of the data and enabling time travel. However, this doesn’t replace the purpose of having explicit timestamp columns. Here’s a breakdown:

1. When to Add createdDate and updatedDate Columns
 
- Application-level tracking: If applications or business logic require explicit timestamps to track the exact creation and update times of each record.
- Ease of querying: Timestamp columns can simplify queries, especially when you need to filter, sort, or retrieve records based on when they were created or last updated without relying on Delta Lake's versioning.

2. When You Might Not Need Them

- Data versioning and auditing: Delta Lake provides versioning with transactional log records, so if you primarily need historical tracking or time-based snapshots, you can use Delta Lake's time travel without adding timestamp columns.
- Lightweight schema: If maintaining a lean schema is a priority and you don't have specific use cases for explicit timestamps in your data, relying on Delta Lake versioning can suffice.

In summary, while Delta Lake’s time travel can replace some tracking needs, explicit createdDate and updatedDate columns offer convenience for direct queries and application use. It’s usually beneficial to include them if they align with your business requirements.

--- 

### E. Performance  - Upinsert

Upserting (merging) data from a DataFrame into a Delta Lake table can impact performance, especially as the table size grows. The performance impact largely depends on several factors:


1. Size of the Target Table and DataFrame
- Larger tables require more processing time to locate matching records and determine necessary updates or inserts. For tables with millions of records, upserts can become resource-intensive.
- 
2. Complexity of Merge Conditions
- The more complex the merge conditions are (e.g., multiple columns in the matching conditions or nested logic), the longer the operation may take as it requires additional processing to determine record matches.
  
3. Partitioning Strategy
- If the Delta table is partitioned effectively (e.g., based on frequently filtered columns), Spark can scan only relevant partitions, significantly improving merge performance.
- However, if partitioning is not done well, it may result in slower read times, as more partitions have to be read and processed.
 
4. Indexing and Z-Ordering
-  Z-ordering can help improve query and update performance by clustering frequently accessed columns together, reducing the data Spark needs to scan.
- Delta Lake does not have traditional indexes, but optimizing with Z-ordering or maintaining a well-thought-out partitioning strategy can help with upsert efficiency.
  
5. Caching and Data Skipping
- Delta Lake supports data skipping, which uses statistics (such as min and max values) to filter out files during reads, reducing the workload when upserting large data.
- Leveraging Delta Lake optimizations like OPTIMIZE or VACUUM on the table regularly can also reduce unnecessary data scanning, speeding up merges.

6. Concurrency and Transaction Overhead
- Each merge is a transaction. If there are frequent concurrent updates, Delta Lake’s ACID transaction handling may add overhead, impacting update speeds.

#### Tips to Improve Upsert Performance

- Use partition pruning effectively to reduce the amount of data scanned.
- Run OPTIMIZE on your Delta table after heavy insertions, updates, or deletions to compact files.
- Use ZORDER by columns often filtered in your queries or updates.
- When possible, upsert data in batches rather than row by row.
- For very large datasets, consider breaking up the merge operation into smaller chunks.

--- 

#### Z-ordering in Delta Lake

Z-ordering in Delta Lake is a clustering technique that optimizes data layout for efficient range queries and merge operations by colocating related information. This is especially beneficial when performing frequent queries or updates on columns with range filters.

Here’s a Scala example of Z-ordering on specific columns, such as date and customer_id, to optimize queries and merges that filter on these columns.

**Example**: Z-ordering in Delta Lake using Scala
```scala
  import io.delta.tables.DeltaTable
  import org.apache.spark.sql.SparkSession

  val spark = SparkSession.builder
    .appName("ZOrderClusteringExample")
    .getOrCreate()

  // Assuming the Delta table is located at this path
  val deltaTablePath = "/path/to/my_delta_table"

  // Load the Delta table
  val deltaTable = DeltaTable.forPath(spark, deltaTablePath)

  // Optimize the Delta table by Z-ordering on `date` and `customer_id`
  deltaTable.optimize()
    .where("date >= '2023-01-01'") // Optional: only optimize recent records, if relevant
    .zOrderBy("date", "customer_id") // Specify columns for Z-order clustering
```

**Explanation**
- optimize(): Triggers the Delta optimization command.
- where clause (optional): Applies Z-ordering to a subset of data if full-table optimization is unnecessary (e.g., optimizing only recent data).
- zOrderBy("date", "customer_id"): Specifies the columns to Z-order by. When queries filter by date or customer_id, Z-order clustering will improve performance.

**Running Z-order Optimization Periodically**
- Since Z-ordering does not automatically apply to new data, schedule periodic runs for large tables, particularly when substantial new data has been added.

--- 

#### Mini-Batch Upserts

To perform mini-batch upserts in Delta Lake with Scala and Spark, you can divide a large DataFrame into smaller batches and then merge each batch individually into the Delta table. This process helps optimize memory usage and improve performance for very large datasets.

```scala
  import org.apache.spark.sql.{DataFrame, SparkSession}
  import org.apache.spark.sql.functions._
  import io.delta.tables._
  import scala.util.Try

  // Initialize SparkSession
  val spark = SparkSession.builder()
    .appName("Mini-Batch Upserts")
    .getOrCreate()

  // Define batch size
  val batchSize = 100000 // Number of rows per batch, adjust based on your cluster size

  // Assuming df is the large DataFrame with data to upsert
  val largeDataFrame: DataFrame = spark.read.parquet("/path/to/large/input/data")

  // Path to Delta Lake table
  val deltaTablePath = "/path/to/delta-table"
  val deltaTable = DeltaTable.forPath(spark, deltaTablePath)

  // Define the primary key column used for merging
  val primaryKeyCol = "id"

  // Partition the large DataFrame into mini-batches
  val totalBatches = math.ceil(largeDataFrame.count().toDouble / batchSize).toInt
  val miniBatches = largeDataFrame.randomSplit(Array.fill(totalBatches)(1.0 / totalBatches))

  // Upsert each mini-batch sequentially
  miniBatches.zipWithIndex.foreach { case (batchDF, index) =>
    println(s"Processing batch ${index + 1} of $totalBatches")

    // Perform upsert (merge) for the current batch
    deltaTable.as("target")
      .merge(
        batchDF.as("source"),
        s"target.$primaryKeyCol = source.$primaryKeyCol"
      )
      .whenMatched()
      .updateAll()
      .whenNotMatched()
      .insertAll()
      .execute()
  }

  // Verify results or take additional actions
  deltaTable.toDF.show()
```

**Explanation of the Code:**
- Define Batch Size: Set batchSize to a manageable number based on your cluster’s memory and processing capabilities.
- Split DataFrame into Mini-Batches: Use randomSplit to divide the large DataFrame into smaller DataFrames.
- Upsert Each Mini-Batch: Use merge for each mini-batch, merging it into the Delta table. The example assumes that id is the primary key and handles whenMatched and whenNotMatched scenarios to perform upserts.
- Monitor Progress: Print out the current batch number to monitor progress during execution.

This method can significantly reduce memory pressure and make updates manageable on large datasets by limiting the amount of data processed at one time. Adjust batchSize based on available resources and observe any performance gains.






