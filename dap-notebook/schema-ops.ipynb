{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. deepCloneSchemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c46af601-d5cd-48a3-a0cb-02afd27790a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "deepCloneSchemas"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdeep_clone\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def deepCloneSchemas(srcCatalog: String, srcSchema: String, tgtCatalog: String, tgtSchema: String ): Unit = {\n",
    "  // 1. Get list of tables from source schema\n",
    "  val tables = spark.sql(s\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM ${srcCatalog}.information_schema.tables\n",
    "    WHERE table_schema = '${srcSchema}'\n",
    "  \"\"\").collect().map(_.getString(0))\n",
    "\n",
    "  // 2. Loop and deep clone each table\n",
    "  tables.foreach { tableName =>\n",
    "    val cloneSql =\n",
    "      s\"\"\"\n",
    "        |CREATE OR REPLACE TABLE ${tgtCatalog}.${tgtSchema}.${tableName}\n",
    "        |DEEP CLONE ${srcCatalog}.${srcSchema}.${tableName}\n",
    "      \"\"\".stripMargin\n",
    "\n",
    "    println(s\"Cloning table: $tableName\")\n",
    "    spark.sql(cloneSql)\n",
    "  }\n",
    "\n",
    "  println(\"Deep clone completed for all tables.\")\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. shallowCloneSchemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "219758cf-40a9-40bf-adcd-189ca50268d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "shallowCloneSchemas"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def shallowCloneSchemas(srcCatalog: String, srcSchema: String, tgtCatalog: String, tgtSchema: String): Unit = {\n",
    "    \n",
    "  // 1. Get list of tables in the source schema\n",
    "  val tables = spark.sql(s\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM ${srcCatalog}.information_schema.tables\n",
    "    WHERE table_schema = '${srcSchema}'\n",
    "  \"\"\").collect().map(_.getString(0))\n",
    "\n",
    "  // 2. Loop through tables and perform SHALLOW CLONE\n",
    "  tables.foreach { tableName =>\n",
    "    val cloneSql =\n",
    "      s\"\"\"\n",
    "        |CREATE OR REPLACE TABLE ${tgtCatalog}.${tgtSchema}.${tableName}\n",
    "        |SHALLOW CLONE ${srcCatalog}.${srcSchema}.${tableName}\n",
    "      \"\"\".stripMargin\n",
    "\n",
    "    println(s\"Shallow cloning table: $tableName\")\n",
    "    spark.sql(cloneSql)\n",
    "  }\n",
    "\n",
    "  println(\"Shallow clone completed for all tables.\")\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. createVersionedSchemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "07cc02d8-bcaa-4074-8caf-221649ec5fd4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "createVersionedSchemas"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def createVersionedSchemas(\n",
    "    schemaNames: List[String],\n",
    "    version: String,\n",
    "    env: String\n",
    ")(implicit spark: SparkSession): Unit = {\n",
    "\n",
    "  // Build catalog name based on env\n",
    "  val catalog = s\"ag_ra_search_analytics_data_${env}\"\n",
    "\n",
    "  // Loop through schema list and create each versioned schema\n",
    "  schemaNames.foreach { schema =>\n",
    "    val fullSchemaName = s\"${catalog}.${schema}_${version}\"\n",
    "\n",
    "    val sql =\n",
    "      s\"\"\"\n",
    "         |CREATE SCHEMA IF NOT EXISTS $fullSchemaName;\n",
    "       \"\"\".stripMargin\n",
    "\n",
    "    println(s\"Creating schema: $fullSchemaName\")\n",
    "    spark.sql(sql)\n",
    "  }\n",
    "\n",
    "  println(s\"✅ Completed creating versioned schemas for environment: $env\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. dropVersionedSchemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8052db16-e839-4dcc-8cd7-a476a0891aef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dropVersionedSchemas"
    }
   },
   "outputs": [],
   "source": [
    "def dropVersionedSchemas(\n",
    "    schemaNames: List[String],\n",
    "    version: String,\n",
    "    env: String,\n",
    "    dropTable: Boolean = false\n",
    ")(implicit spark: SparkSession): Unit = {\n",
    "\n",
    "  val catalog = s\"ag_ra_search_analytics_data_${env}\"\n",
    "\n",
    "  schemaNames.foreach { schema =>\n",
    "    val fullSchema = s\"${catalog}.${schema}_${version}\"\n",
    "\n",
    "    println(s\"Processing drop for schema: $fullSchema\")\n",
    "\n",
    "    // 1. Fetch all tables in this schema\n",
    "    if(dropTable) {\n",
    "      val tables = spark.sql(\n",
    "        s\"\"\"\n",
    "          |SELECT table_name\n",
    "          |FROM ${catalog}.information_schema.tables\n",
    "          |WHERE table_schema = '${schema}_${version}'\n",
    "        \"\"\".stripMargin\n",
    "      ).collect().map(_.getString(0))\n",
    "\n",
    "      // 2. Drop each table\n",
    "      tables.foreach { table =>\n",
    "        val fullTableName = s\"$fullSchema.$table\"\n",
    "        println(s\"Dropping table: $fullTableName\")\n",
    "        spark.sql(s\"DROP TABLE IF EXISTS $fullTableName\")\n",
    "      }\n",
    "    }\n",
    "\n",
    "    // 3. Drop the schema itself\n",
    "    println(s\"Dropping schema: $fullSchema\")\n",
    "    spark.sql(s\"DROP SCHEMA IF EXISTS $fullSchema CASCADE\")\n",
    "  }\n",
    "\n",
    "  println(s\"✅ Completed dropping versioned schemas for env: $env\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Define List of Schema (DAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val schemas = List(\n",
    "  \"dap_entity_wos\",\n",
    "  \"dap_metrics_wos\",\n",
    "  \"dap_entity_pprn\",\n",
    "  \"dap_metrics_pprn\",\n",
    "  \"dap_docs\",\n",
    "  \"dap_reference\",\n",
    "  \"dap_sort_ref\",\n",
    "  \"dap_entity_enrich\",\n",
    "  \"dap_prod_core\",\n",
    "  \"dap_ops\",\n",
    "  \"dap_work\"\n",
    ")\n",
    "\n",
    "val acsSchemas = List(\n",
    "  \"gold_entity\",\n",
    "  \"gold_wos\",\n",
    "  \"gold_pprn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Run createVersionedSchemas for DAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bb6559c4-35b2-420e-8831-41eed5b40761",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "run  createVersionedSchemas"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "createVersionedSchemas(\n",
    "  schemaNames = schemas,\n",
    "  version = \"v1_0\",\n",
    "  env = \"dev\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Run dropVersionedSchemas for DAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "48225058-1bd1-4e45-b48d-a024bcebb895",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "run dropVersionedSchemas"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dropVersionedSchemas(\n",
    "  schemaNames = schemas,\n",
    "  version = \"v1_0\",\n",
    "  env = \"dev\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Run deep_clone for DAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// Init\n",
    "val start = System.nanoTime()\n",
    "\n",
    "\n",
    "val src_env = \"dev\"\n",
    "val tgt_env = \"dev\"\n",
    "val src_version = \"v1_0\"\n",
    "val tgt_version = \"v1_1\"\n",
    "\n",
    "val src_catalog = \"ag_ra_search_analytics_data_${src_env}\"\n",
    "val tgt_catalog = \"ag_ra_search_analytics_data_${tgt_env}\"\n",
    "\n",
    "\n",
    "def processSchemas(src: String, tgt: String): Unit = {\n",
    "  println(s\"Processing: source = $src, target = $tgt\")\n",
    "  // your logic here…\n",
    "  deep_clone(catalog, src, catalog, tgt )\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "schemas.foreach { case schema =>\n",
    "  val src_schema = s\"${schema}_${src_version}\"\n",
    "  val tgt_schema = s\"${schema}_${tgt_version}\"\n",
    "  println(s\"Processing: source = $src_schema, target = $tgt_schema\")\n",
    "\n",
    "  val start = System.nanoTime()\n",
    "  deep_clone(src_catalog,  src_schema, tgt_catalog, tgt_schema )\n",
    "  println(s\"Time taken: ${(System.nanoTime() - start) / 1e9} seconds\")\n",
    "}\n",
    "\n",
    "println(\"All schemas processed.\")\n",
    "println(s\"Time taken: ${(System.nanoTime() - start) / 1e9} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Run deep_clone for List of Spefified Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed82e95-6bfe-4649-9a48-692e7cf31666",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "run deep_clone"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: source = dap_entity_wos, target = dap_entity_wos_v1_0\n",
      "Cloning table: authorprofile\n",
      "Cloning table: metadata\n",
      "Cloning table: dept_article\n",
      "Cloning table: funding_agency\n",
      "Cloning table: jcr_metrics_more\n",
      "Cloning table: jcr_metrics\n",
      "Cloning table: incites_assemble\n",
      "Cloning table: category\n",
      "Cloning table: category_article_metrics\n",
      "Cloning table: org_article\n",
      "Cloning table: wos\n",
      "Cloning table: en_grants\n",
      "Cloning table: funding_agency_article_metrics\n",
      "Cloning table: ap_article\n",
      "Cloning table: authorprofile_assemble\n",
      "Cloning table: region_test\n",
      "Cloning table: patents\n",
      "Cloning table: region\n",
      "Cloning table: region_article_metrics\n",
      "Cloning table: organization\n",
      "Cloning table: journal\n",
      "Deep clone completed for all tables.\n",
      "Processing: source = dap_metrics_wos, target = dap_metrics_wos_v1_0\n",
      "Cloning table: funding_agency_article_metrics\n",
      "Cloning table: en_research_topics\n",
      "Cloning table: en_societal_facet\n",
      "Cloning table: en_societal_impact\n",
      "Cloning table: article_normalized_metrics\n",
      "Cloning table: category_metrics\n",
      "Cloning table: article_normalized_metrics_woscore\n",
      "Deep clone completed for all tables.\n",
      "Processing: source = dap_entity_pprn, target = dap_entity_pprn_v1_0\n",
      "Cloning table: ap_article\n",
      "Cloning table: org_article\n",
      "Cloning table: authorprofile\n",
      "Cloning table: dept_article\n",
      "Cloning table: organization\n",
      "Cloning table: category\n",
      "Cloning table: pprn\n",
      "Cloning table: category_article_metrics\n",
      "Deep clone completed for all tables.\n",
      "Processing: source = dap_metrics_pprn, target = dap_metrics_pprn_v1_0\n",
      "Cloning table: article_normalized_metrics\n",
      "Cloning table: category_metrics\n",
      "Deep clone completed for all tables.\n",
      "Processing: source = reference_data_v1_0, target = dap_reference_v1_0\n",
      "Cloning table: d_alc_complete_labels_v1_0\n",
      "Cloning table: d_patent_code_type_v1_0\n",
      "Cloning table: d_innovators_top100_v1_0\n",
      "Cloning table: region_country_link\n",
      "Cloning table: funder_catalog\n",
      "Cloning table: quantiles\n",
      "Cloning table: revised_publisher_unification\n",
      "Cloning table: minimum_threshold_for_specific_indicators\n",
      "Cloning table: set_of_possible_thresholds\n",
      "Cloning table: nuts_code_region\n",
      "Cloning table: minimum_inclusion_percentage\n",
      "Cloning table: taxonomy_details\n",
      "Deep clone completed for all tables.\n",
      "Processing: source = dap_work, target = dap_work_v1_0\n",
      "Cloning table: org_hierarchy_v0_3\n",
      "Cloning table: org_top_parent_mapping\n",
      "Cloning table: patent_org\n",
      "Cloning table: grants_ri_grant_ut_funder\n",
      "Cloning table: grants_grants_funder\n",
      "Cloning table: grants_ri_grant_ut_org\n",
      "Cloning table: grants_ri_research_topic\n",
      "Cloning table: patent_granted_application\n",
      "Cloning table: grants_ri_grants_uts\n",
      "Cloning table: grants_grants_root_funder\n",
      "Cloning table: grants_ri_grant_funder\n",
      "Cloning table: incites_ri_funding_organizations\n",
      "Cloning table: grants_ri_grants_items\n",
      "Cloning table: grants_grants_sort\n",
      "Cloning table: grants_ri_grant_org\n",
      "Cloning table: grants_ri_authorprofile\n",
      "Cloning table: incites_ri_organizations\n",
      "Deep clone completed for all tables.\n",
      "Time taken: 1341.135042287 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mstart\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m576002314729L\u001b[39m\n",
       "\u001b[36mschemaPairs\u001b[39m: \u001b[32mList\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"dap_entity_wos\"\u001b[39m, \u001b[32m\"dap_entity_wos_v1_0\"\u001b[39m),\n",
       "  (\u001b[32m\"dap_metrics_wos\"\u001b[39m, \u001b[32m\"dap_metrics_wos_v1_0\"\u001b[39m),\n",
       "  (\u001b[32m\"dap_entity_pprn\"\u001b[39m, \u001b[32m\"dap_entity_pprn_v1_0\"\u001b[39m),\n",
       "  (\u001b[32m\"dap_metrics_pprn\"\u001b[39m, \u001b[32m\"dap_metrics_pprn_v1_0\"\u001b[39m),\n",
       "  (\u001b[32m\"reference_data_v1_0\"\u001b[39m, \u001b[32m\"dap_reference_v1_0\"\u001b[39m),\n",
       "  (\u001b[32m\"dap_work\"\u001b[39m, \u001b[32m\"dap_work_v1_0\"\u001b[39m)\n",
       ")\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprocessSchemas\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Init\n",
    "val start = System.nanoTime()\n",
    "\n",
    "\n",
    "val schemaPairs: List[(String, String)] = List(\n",
    "  (\"dap_entity_wos\",    \"dap_entity_wos_v1_0\"),\n",
    "  (\"dap_metrics_wos\",   \"dap_metrics_wos_v1_0\"),\n",
    "  (\"dap_entity_pprn\",   \"dap_entity_pprn_v1_0\"),\n",
    "  (\"dap_metrics_pprn\",  \"dap_metrics_pprn_v1_0\"),\n",
    "  (\"reference_data_v1_0\", \"dap_reference_v1_0\"),\n",
    "  (\"dap_work\",          \"dap_work_v1_0\")\n",
    ")\n",
    "\n",
    "val catalog = \"ag_ra_search_analytics_data_dev\"\n",
    "\n",
    "def processSchemas(src: String, tgt: String): Unit = {\n",
    "  println(s\"Processing: source = $src, target = $tgt\")\n",
    "  // your logic here…\n",
    "  deep_clone(catalog, src, catalog, tgt )\n",
    "\n",
    "}\n",
    "\n",
    "schemaPairs.foreach { case (srcSchema, tgtSchema) =>\n",
    "  processSchemas(srcSchema, tgtSchema)\n",
    "}\n",
    "\n",
    "println(s\"Time taken: ${(System.nanoTime() - start) / 1e9} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Run deep_clone for Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68122fcb-401d-4d76-a645-adb76ff4212e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val schemaPairs: List[(String, String)] = List(\n",
    "  (\"reference_data_v1_0\",     \"dap_reference_v1_0\"),\n",
    ")\n",
    "\n",
    "def processSchemas(src: String, tgt: String): Unit = {\n",
    "  println(s\"Processing: source = $src, target = $tgt\")\n",
    "  // your logic here…\n",
    "  deep_clone(\"ag_ra_search_analytics_data_dev\", src, \"ag_ra_search_analytics_data_uat\", tgt )\n",
    "  deep_clone(\"ag_ra_search_analytics_data_dev\", src, \"ag_ra_search_analytics_data_preprod\", tgt )\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.  listFirstTwoColumns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "def listFirstTwoColumns(schemaName: String)(implicit spark: SparkSession): DataFrame = {\n",
    "\n",
    "  // Get all table names in the schema\n",
    "  val tables = spark.sql(s\"SHOW TABLES IN $schemaName\")\n",
    "    .select(\"tableName\")\n",
    "    .as[String]\n",
    "    .collect()\n",
    "\n",
    "  // Extract first and second column for each table\n",
    "  val results = tables.map { tableName =>\n",
    "    val df = spark.table(s\"$schemaName.$tableName\")\n",
    "    val cols = df.columns\n",
    "\n",
    "    val firstColumn  = if (cols.length >= 1) cols(0) else null\n",
    "    val secondColumn = if (cols.length >= 2) cols(1) else null\n",
    "\n",
    "    (schemaName, tableName, firstColumn, secondColumn)\n",
    "  }\n",
    "\n",
    "  // Convert to DataFrame\n",
    "  spark.createDataFrame(results)\n",
    "    .toDF(\"schema\", \"table_name\", \"first_column\", \"second_column\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.  Run listFirstTwoColumns for ACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val env = \"dev\"\n",
    "val version = \"v1_0\"\n",
    "val catalog = s\"ag_content_ims_${env}\"\n",
    "\n",
    "\n",
    "acsSchemas.foreach { case schema =>\n",
    "  val schemaName = s\"${catalog}.${schema}\"\n",
    " // val schemaName = s\"${catalog}.${schema}_${version}\"\n",
    "  listFirstTwoColumns(schemaName)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {},
   "notebookName": "databrick-ops",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
