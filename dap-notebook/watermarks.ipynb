{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fb6176",
   "metadata": {},
   "source": [
    "#### 1. Init  widgets parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73debdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame, Dataset, Row, Column}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import io.delta.tables._\n",
    "import java.time.Year\n",
    "import java.time.LocalDate\n",
    "import spark.implicits._ \n",
    "\n",
    "\n",
    "//  pass the parameters\n",
    "dbutils.widgets.text(\"environment\", \"dev\")\n",
    "dbutils.widgets.text(\"version\", \"v1_0\")\n",
    "dbutils.widgets.text(\"pipeline\", \"\")\n",
    "\n",
    "// dynamic paramters\n",
    "val env = dbutils.widgets.get(\"environment\")\n",
    "val udm = dbutils.widgets.get(\"version\")\n",
    "val pipeline = dbutils.widgets.get(\"pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaac49d",
   "metadata": {},
   "source": [
    "#### 2. WatermarkTracker definition - Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.sql.Timestamp\n",
    "import java.time.Instant\n",
    "\n",
    "case class WatermarkRecord(\n",
    "    watermark_id: Option[Int] = None,\n",
    "    pipeline_name: String,\n",
    "    job_id: String,\n",
    "    last_processed_ts: Option[Timestamp],\n",
    "    current_run_ts: Option[Timestamp],\n",
    "    last_processed_version: Option[Long],\n",
    "    status: String = \"RUNNING\",\n",
    "    created_at: Option[Timestamp] = Some(Timestamp.from(Instant.now())),\n",
    "    updated_at: Option[Timestamp] = Some(Timestamp.from(Instant.now()))\n",
    ")\n",
    "\n",
    "object WatermarkTracker {\n",
    "\n",
    "  val checkpointTable = \"ag_ra_search_analytics_data_dev.dap_ops_v1_0.watermarks\"\n",
    "\n",
    "  /** Step 1: Start a new run for a pipeline **/\n",
    "  def startPipelineRun(pipeline: String, jobId: String)(implicit spark: SparkSession): WatermarkRecord = {\n",
    "    import spark.implicits._\n",
    "\n",
    "    val lastRecordOpt = spark.table(checkpointTable)\n",
    "      .filter($\"pipeline_name\" === pipeline)\n",
    "      .orderBy(desc(\"created_at\"))\n",
    "      .limit(1)\n",
    "      .as[WatermarkRecord]\n",
    "      .collect()\n",
    "      .headOption\n",
    "\n",
    "    val lastProcessedTs = lastRecordOpt.flatMap(_.current_run_ts.orElse(_.last_processed_ts))\n",
    "    val nowTs = Timestamp.from(Instant.now())\n",
    "\n",
    "    // Update the current checkpoint to indicate ongoing run\n",
    "    lastRecordOpt.foreach { rec =>\n",
    "      spark.table(checkpointTable)\n",
    "        .filter($\"watermark_id\" === rec.watermark_id.getOrElse(-1))\n",
    "        .withColumn(\"current_run_ts\", lit(nowTs))\n",
    "        .withColumn(\"status\", lit(\"RUNNING\"))\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"replaceWhere\", s\"watermark_id = ${rec.watermark_id.getOrElse(-1)}\")\n",
    "        .saveAsTable(checkpointTable)\n",
    "    }\n",
    "\n",
    "    WatermarkRecord(\n",
    "      pipeline_name = pipeline,\n",
    "      job_id = jobId,\n",
    "      last_processed_ts = lastProcessedTs,\n",
    "      current_run_ts = Some(nowTs),\n",
    "      status = \"RUNNING\"\n",
    "    )\n",
    "  }\n",
    "\n",
    "  /** Step 2: Get delta extraction window **/\n",
    "  def getDeltaWindow(pipeline: String)(implicit spark: SparkSession): (Option[Timestamp], Timestamp) = {\n",
    "    val lastRecordOpt = spark.table(checkpointTable)\n",
    "      .filter($\"pipeline_name\" === pipeline)\n",
    "      .orderBy(desc(\"created_at\"))\n",
    "      .limit(1)\n",
    "      .collect()\n",
    "      .headOption\n",
    "\n",
    "    val startTs = lastRecordOpt.flatMap(row => Option(row.getAs[Timestamp](\"last_processed_ts\")))\n",
    "    val endTs = Timestamp.from(Instant.now())\n",
    "    (startTs, endTs)\n",
    "  }\n",
    "\n",
    "  /** Step 3: Finalize checkpoint for SUCCESS, FAILED, or SKIPPED **/\n",
    "  def finalizePipelineRun(record: WatermarkRecord, finalStatus: String)(implicit spark: SparkSession): Unit = {\n",
    "    import spark.implicits._\n",
    "\n",
    "    require(Set(\"SUCCESS\", \"FAILED\", \"SKIPPED\").contains(finalStatus.toUpperCase),\n",
    "      s\"Invalid status: $finalStatus. Must be SUCCESS, FAILED, or SKIPPED.\")\n",
    "\n",
    "    val nowTs = Timestamp.from(Instant.now())\n",
    "\n",
    "    // 1. Update current record with final status\n",
    "    spark.table(checkpointTable)\n",
    "      .filter($\"pipeline_name\" === record.pipeline_name && $\"job_id\" === record.job_id)\n",
    "      .withColumn(\"status\", lit(finalStatus.toUpperCase))\n",
    "      .withColumn(\"updated_at\", current_timestamp())\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"replaceWhere\", s\"pipeline_name = '${record.pipeline_name}' AND job_id = '${record.job_id}'\")\n",
    "      .saveAsTable(checkpointTable)\n",
    "\n",
    "    // 2. Only create a new record if the run was SUCCESS\n",
    "    if (finalStatus.toUpperCase == \"SUCCESS\") {\n",
    "      val nextRecord = WatermarkRecord(\n",
    "        pipeline_name = record.pipeline_name,\n",
    "        job_id = java.util.UUID.randomUUID().toString,\n",
    "        last_processed_ts = record.current_run_ts,\n",
    "        current_run_ts = None,\n",
    "        last_processed_version = record.current_run_ts.map(_.getTime),\n",
    "        status = \"READY\"\n",
    "      )\n",
    "\n",
    "      Seq(nextRecord).toDF()\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(checkpointTable)\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2045753",
   "metadata": {},
   "source": [
    "#### 3. Test WatermarkTracker - Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 1: Start a new run for a pipeline\n",
    "\n",
    "val wmRecord = WatermarkTracker.startPipelineRun(\"wos_pipeline\", \"job_123\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77139e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 2: Get delta extraction window\n",
    "\n",
    "\n",
    "val (startTsOpt, endTs) = WatermarkTracker.getDeltaWindow(\"wos_pipeline\")\n",
    "\n",
    "val deltaDF = spark.table(\"acs_source_table\")\n",
    "  .filter(col(\"updated_at\") > startTsOpt.getOrElse(Timestamp.valueOf(\"1970-01-01 00:00:00\")) &&\n",
    "          col(\"updated_at\") <= endTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 3: Finalize checkpoint for SUCCESS\n",
    "\n",
    "\n",
    "WatermarkTracker.finalizePipelineRun(wmRecord, finalStatus = \"SUCCESS\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08455e7b",
   "metadata": {},
   "source": [
    "#### 4. WatermarkTracker definition - Navtive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c14db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.sql.Timestamp\n",
    "import java.time.Instant\n",
    "import java.time.{Instant, LocalDate, ZoneId}\n",
    "\n",
    "\n",
    "case class WatermarkRecord(\n",
    "    watermark_id: String,\n",
    "    pipeline_name: String,\n",
    "    batch_id: String,\n",
    "    last_processed_ts: Timestamp = Timestamp.valueOf(\"1970-01-01 00:00:00\"),\n",
    "    current_run_ts: Timestamp = Timestamp.from(Instant.now()),\n",
    "    last_processed_version: Long = 0L,\n",
    "    status: String = \"RUNNING\",\n",
    "    created_at: Timestamp = Timestamp.from(Instant.now()),\n",
    "    updated_at: Timestamp = Timestamp.from(Instant.now())\n",
    ")\n",
    "\n",
    "object WatermarkTracker {\n",
    "\n",
    "  private def getWidget(name: String, default: String): String = {\n",
    "    try {\n",
    "      val value = dbutils.widgets.get(name)\n",
    "      if (value == null || value.isEmpty) default else value\n",
    "    } catch {\n",
    "      case _: Throwable => default\n",
    "    }\n",
    "  }\n",
    "\n",
    "  private def batchNumber(): String = {\n",
    "    // Get current date\n",
    "    val today = LocalDate.now(ZoneId.systemDefault())\n",
    "    \n",
    "    // Generate a unique number based on year, month, and day\n",
    "    val uniqueNumber: Long = today.getYear * 10000L + today.getMonthValue * 100L + today.getDayOfMonth\n",
    "    \n",
    "    // Return as string\n",
    "    uniqueNumber.toString\n",
    "  }\n",
    "\n",
    "  private val environment = getWidget(\"environment\", \"dev\")\n",
    "  private val version = getWidget(\"version\", \"v1_0\")\n",
    "  private val catalog = s\"ag_ra_search_analytics_data_${environment}\"\n",
    "  private val checkpointTable = s\"$catalog.dap_ops_${version}.watermarks\"\n",
    "  private val pipeline_name = getWidget(\"pipeline\", \"dummy\")\n",
    "\n",
    "  /** Fetch the latest checkpoint for a pipeline, or return None if not exists **/\n",
    "  def getLastCheckpoint(pipelineName: String = null)(implicit spark: SparkSession): Option[WatermarkRecord] = {\n",
    "    spark.table(checkpointTable)\n",
    "      .filter($\"pipeline_name\" === pipelineName)\n",
    "      .orderBy(desc(\"created_at\"))\n",
    "      .limit(1)\n",
    "      .as[WatermarkRecord]\n",
    "      .collect()\n",
    "      .headOption\n",
    "  }\n",
    "\n",
    "  /** Start a new pipeline run; creates a checkpoint if first run **/\n",
    "  def initializePipelineRun(nowTs: Timestamp = Timestamp.from(Instant.now()), pipelineName: String = null)(implicit spark: SparkSession): WatermarkRecord = {\n",
    "\n",
    "    val curPipelineName = if (pipelineName == null) pipeline_name else pipelineName\n",
    "    val lastRecordOpt = getLastCheckpoint(curPipelineName)\n",
    "\n",
    "    val record = lastRecordOpt match {\n",
    "      case Some(lastRecord) =>\n",
    "        // Update current run timestamp\n",
    "        spark.table(checkpointTable)\n",
    "          .filter($\"watermark_id\" === lastRecord.watermark_id)\n",
    "          .withColumn(\"current_run_ts\", lit(nowTs))\n",
    "          .withColumn(\"status\", lit(\"RUNNING\"))\n",
    "          .withColumn(\"updated_at\", current_timestamp())\n",
    "          .write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .option(\"replaceWhere\", s\"watermark_id = '${lastRecord.watermark_id}'\")\n",
    "          .saveAsTable(checkpointTable)\n",
    "\n",
    "        lastRecord.copy(current_run_ts = nowTs, status = \"RUNNING\", batch_id = lastRecord.batch_id)\n",
    "\n",
    "      case None =>\n",
    "        // First run, create new record\n",
    "        val newRecord = WatermarkRecord(\n",
    "          watermark_id = java.util.UUID.randomUUID().toString,\n",
    "          pipeline_name = curPipelineName,\n",
    "          batch_id = java.util.UUID.randomUUID().toString,\n",
    "          last_processed_ts = Timestamp.valueOf(\"1970-01-01 00:00:00\"),\n",
    "          current_run_ts = nowTs,\n",
    "          last_processed_version = 1L,\n",
    "          status = \"RUNNING\"\n",
    "        )\n",
    "\n",
    "        Seq(newRecord).toDF()\n",
    "          .write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"append\")\n",
    "          .saveAsTable(checkpointTable)\n",
    "\n",
    "        newRecord\n",
    "    }\n",
    "\n",
    "    record\n",
    "  }\n",
    "\n",
    "\n",
    "  def initializePipelineWorkflow(\n",
    "      pipelineNames: Seq[String],\n",
    "      nowTs: Timestamp = Timestamp.from(Instant.now())\n",
    "  )(implicit spark: SparkSession): Seq[WatermarkRecord] = {\n",
    "\n",
    "  // Fetch last checkpoint records for all pipelines\n",
    "  val lastRecords: Map[String, WatermarkRecord] = pipelineNames.flatMap { pname =>\n",
    "    getLastCheckpoint(pname).map(r => pname -> r)\n",
    "  }.toMap\n",
    "\n",
    "  val batchId = \"20251120\" // batchNumber()\n",
    "  \n",
    "  val newRecords = pipelineNames.map { pname =>\n",
    "    lastRecords.get(pname) match {\n",
    "      case Some(lastRecord) =>\n",
    "        // Update current run timestamp and status in checkpoint table\n",
    "        spark.table(checkpointTable)\n",
    "          .filter($\"watermark_id\" === lastRecord.watermark_id)\n",
    "          .withColumn(\"current_run_ts\", lit(nowTs))\n",
    "          .withColumn(\"status\", lit(\"RUNNING\"))\n",
    "          .withColumn(\"updated_at\", current_timestamp())\n",
    "          .write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .option(\"replaceWhere\", s\"watermark_id = '${lastRecord.watermark_id}'\")\n",
    "          .saveAsTable(checkpointTable)\n",
    "\n",
    "        lastRecord.copy(current_run_ts = nowTs, status = \"RUNNING\", batch_id = lastRecord.batch_id)\n",
    "\n",
    "      case None =>\n",
    "        // First run, create new record\n",
    "        val newRecord = WatermarkRecord(\n",
    "          watermark_id = java.util.UUID.randomUUID().toString,\n",
    "          pipeline_name = pname,\n",
    "          batch_id = batchId,\n",
    "          last_processed_ts = Timestamp.valueOf(\"1970-01-01 00:00:00\"),\n",
    "          current_run_ts = nowTs,\n",
    "          last_processed_version = 1L,\n",
    "          status = \"RUNNING\"\n",
    "        )\n",
    "\n",
    "        Seq(newRecord).toDF()\n",
    "          .write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"append\")\n",
    "          .saveAsTable(checkpointTable)\n",
    "\n",
    "        newRecord\n",
    "    }\n",
    "  }\n",
    "\n",
    "  newRecords\n",
    "}\n",
    "\n",
    "\n",
    "  /** Get the extraction window (start/end timestamp) for a pipeline run **/\n",
    "  def getExtractionWindow(pipelineName: String = null)(implicit spark: SparkSession): (Timestamp, Timestamp) = {\n",
    "\n",
    "    val curPipelineName = if (pipelineName == null) pipeline_name else pipelineName\n",
    "    getLastCheckpoint(curPipelineName) match {\n",
    "      case Some(record) => (record.last_processed_ts, record.current_run_ts)\n",
    "      case None => (Timestamp.valueOf(\"1970-01-01 00:00:00\"), Timestamp.from(Instant.now()))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /** Finalize a pipeline run: update status and create next checkpoint if SUCCESS **/\n",
    "  def completePipelineRun( finalStatus: String = \"SUCCESS\", pipelineName: String = null)(implicit spark: SparkSession): Unit = {\n",
    "\n",
    "    require(Set(\"SUCCESS\", \"FAILED\", \"SKIPPED\").contains(finalStatus.toUpperCase),\n",
    "      s\"Invalid status: $finalStatus. Must be SUCCESS, FAILED, or SKIPPED.\")\n",
    "    \n",
    "    val curPipelineName = if (pipelineName == null) pipeline_name else pipelineName\n",
    "    val nowTs = Timestamp.from(Instant.now())\n",
    "    val record = getLastCheckpoint(curPipelineName).getOrElse(\n",
    "      throw new IllegalStateException(s\"No checkpoint found for pipeline: $curPipelineName\")\n",
    "    )\n",
    "\n",
    "    // 1. Update current record with final status\n",
    "    spark.table(checkpointTable)\n",
    "      .filter($\"pipeline_name\" === record.pipeline_name && $\"batch_id\" === record.batch_id)\n",
    "      .withColumn(\"status\", lit(finalStatus.toUpperCase))\n",
    "      .withColumn(\"updated_at\", current_timestamp())\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"replaceWhere\", s\"pipeline_name = '${record.pipeline_name}' AND batch_id = '${record.batch_id}'\")\n",
    "      .saveAsTable(checkpointTable)\n",
    "\n",
    "    // 2. Create new checkpoint if SUCCESS\n",
    "    if (finalStatus.toUpperCase == \"SUCCESS\") {\n",
    "      val lastVersion = spark.table(checkpointTable)\n",
    "        .filter($\"pipeline_name\" === record.pipeline_name)\n",
    "        .agg(max($\"last_processed_version\").alias(\"max_version\"))\n",
    "        .collect()\n",
    "        .head\n",
    "        .getAs[Long](\"max_version\")\n",
    "\n",
    "      val nextRecord = WatermarkRecord(\n",
    "        watermark_id = java.util.UUID.randomUUID().toString,\n",
    "        pipeline_name = record.pipeline_name,\n",
    "        batch_id = batchNumber(),\n",
    "        last_processed_ts = record.current_run_ts,\n",
    "        current_run_ts = null,\n",
    "        last_processed_version = lastVersion + 1,\n",
    "        status = \"READY\"\n",
    "      )\n",
    "\n",
    "      Seq(nextRecord).toDF()\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(checkpointTable)\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c2cbef",
   "metadata": {},
   "source": [
    "#### 5. Test WatermarkTracker - Native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " // Step 1: init extraction window based on checkpoint\n",
    " \n",
    " val currentRun = WatermarkTracker.initializePipelineRun() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc32ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 2:read Delta data\n",
    "\n",
    "\n",
    "val (startTs, endTs) = WatermarkTracker.getExtractionWindow() \n",
    "// Perform delta read...\n",
    "\n",
    "val deltaDF = spark.table(\"ag_content_ims_acs_dev.gold_entity.d_citation_patent\")\n",
    "  .filter(col(\"__END_AT\") > lit(startTs) && col(\"__END_AT\") <= lit(endTs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 3: Finalize checkpoint for SUCCESS\n",
    "\n",
    "WatermarkTracker.completePipelineRun()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab25112",
   "metadata": {},
   "source": [
    "#### 6. Test WatermarkTracker - Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " // Step 1: Initialize extraction window  for a list of pipelines\n",
    "\n",
    "// Example list of pipelines to initialize\n",
    "val pipelines = Seq(\"pipelineA\", \"pipelineB\", \"pipelineC\")\n",
    "\n",
    "// Call the function to initialize workflows for all pipelines\n",
    "val initializedRecords = WatermarkTracker.initializePipelineWorkflow(pipelines)\n",
    "\n",
    "// Print the results\n",
    "initializedRecords.foreach { record =>\n",
    "  println(s\"Pipeline: ${record.pipeline_name}, \" +\n",
    "          s\"Batch ID: ${record.batch_id}, \" ++\n",
    "          s\"Watermark ID: ${record.watermark_id}, \" +\n",
    "          s\"Current Run TS: ${record.current_run_ts}, \" +\n",
    "          s\"Status: ${record.status}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 2: Get extraction windows for all pipelines\n",
    "// Print the results\n",
    "pipelines.foreach { record =>\n",
    "  val (startTs, endTs) = WatermarkTracker.getExtractionWindow(record)\n",
    "\n",
    "   println(s\"Pieplie: ${record}, \" +\n",
    "          s\"startTs: ${startTs}, \" +\n",
    "          s\"endTs: ${endTs}, \" )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// Step 3: Finalize checkpoints for all pipelines as SUCCESS\n",
    "// Print the results\n",
    "pipelines.foreach { record =>\n",
    "   WatermarkTracker.completePipelineRun(\"SUCCESS\", record)\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1e8ad",
   "metadata": {},
   "source": [
    "#### 7. Create watermarks Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f681f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "  CREATE TABLE IF NOT EXISTS ag_ra_search_analytics_data_dev.dap_ops_v1_0.watermarks (\n",
    "      watermark_id             STRING,                     -- Unique ID for each watermark record\n",
    "      pipeline_name            STRING,                  -- Name of the pipeline\n",
    "      batch_id                 STRING,                  -- Job or run identifier\n",
    "      last_processed_ts        TIMESTAMP,               -- Last processed event timestamp\n",
    "      current_run_ts           TIMESTAMP,               -- Current processing run timestamp\n",
    "      last_processed_version   BIGINT,                  -- Version of last processed data\n",
    "      status                   STRING,                  -- Status of the run (e.g., SUCCESS, FAILED, RUNNING)\n",
    "      created_at               TIMESTAMP, -- Record creation time\n",
    "      updated_at               TIMESTAMP  -- Last update time\n",
    "  )\n",
    "  USING DELTA\n",
    "  PARTITIONED BY (pipeline_name); \n",
    "\"\"\")\n",
    "\n",
    "// SHOW TABLES IN ag_ra_search_analytics_data_dev.dap_reference_v1_0;\n",
    "// DROP TABLE IF EXISTS  ag_ra_search_analytics_data_dev.dap_entity_wos_v1_0.journal;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
