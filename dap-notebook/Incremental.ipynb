{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fa3787",
   "metadata": {},
   "source": [
    "#### 1. simulateDeltaChanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cea091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.Random\n",
    "\n",
    "/**\n",
    " * Simulate CRUD operations on a Delta table (generic for any schema)\n",
    " *\n",
    " * @param tablePath        Path or table name of the Delta table\n",
    " * @param primaryKey       Primary key column for identifying rows\n",
    " * @param count            Number of rows to ADD, UPDATE, DELETE\n",
    " */\n",
    "def simulateDeltaChanges(\n",
    "    tablePath: String,\n",
    "    primaryKey: String,\n",
    "    count: Int = 10\n",
    ")(implicit spark: SparkSession): Unit = {\n",
    "\n",
    "  // -------------------------------------------\n",
    "  // 1. Read current table\n",
    "  // -------------------------------------------\n",
    "  val df = spark.read.format(\"delta\").load(tablePath)\n",
    "\n",
    "  val totalCount = df.count()\n",
    "  require(totalCount > 0, \"Table must not be empty\")\n",
    "  require(df.columns.contains(primaryKey), s\"Primary key $primaryKey not found\")\n",
    "\n",
    "  // -------------------------------------------\n",
    "  // 2. Pick rows for UPDATE + DELETE\n",
    "  // -------------------------------------------\n",
    "  val dfWithRand = df.withColumn(\"_rand\", rand())\n",
    "\n",
    "  val rowsForUpdate = dfWithRand.orderBy(\"_rand\").limit(count).drop(\"_rand\")\n",
    "  val rowsForDelete = dfWithRand.orderBy(rand()).limit(count).drop(\"_rand\")\n",
    "\n",
    "  // -------------------------------------------\n",
    "  // 3. UPDATE simulation: modify values\n",
    "  //    We apply a simple \"random value modifier\"\n",
    "  // -------------------------------------------\n",
    "  val updatedRows = rowsForUpdate.columns.foldLeft(rowsForUpdate) { (acc, colName) =>\n",
    "    if (colName != primaryKey)\n",
    "      acc.withColumn(colName, lit(s\"updated_${Random.nextInt(10000)}\"))\n",
    "    else acc\n",
    "  }\n",
    "\n",
    "  // -------------------------------------------\n",
    "  // 4. DELETE simulation: just collect keys\n",
    "  // -------------------------------------------\n",
    "  val deleteKeys = rowsForDelete.select(primaryKey).as[String].collect().toSet\n",
    "\n",
    "  // -------------------------------------------\n",
    "  // 5. ADD simulation: create new records\n",
    "  // -------------------------------------------\n",
    "  val cols = df.columns\n",
    "  val addRows = (1 to count).map { _ =>\n",
    "    val values = cols.map { c =>\n",
    "      if (c == primaryKey) java.util.UUID.randomUUID().toString\n",
    "      else s\"new_${Random.nextInt(10000)}\"\n",
    "    }\n",
    "    values.toSeq\n",
    "  }\n",
    "\n",
    "  val addDf = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(addRows.map(Row.fromSeq)),\n",
    "    df.schema\n",
    "  )\n",
    "\n",
    "  // -------------------------------------------\n",
    "  // 6. Build final dataframe for write-back\n",
    "  // -------------------------------------------\n",
    "\n",
    "  // Remove deleted rows\n",
    "  val dfAfterDelete = df.filter(!col(primaryKey).isin(deleteKeys.toSeq: _*))\n",
    "\n",
    "  // Replace updated rows (by excluding old ones and unioning updated ones)\n",
    "  val updateKeys = updatedRows.select(primaryKey).as[String].collect().toSet\n",
    "\n",
    "  val dfAfterUpdate = dfAfterDelete\n",
    "    .filter(!col(primaryKey).isin(updateKeys.toSeq: _*))\n",
    "    .union(updatedRows)\n",
    "\n",
    "  // Insert new rows\n",
    "  val finalDf = dfAfterUpdate.union(addDf)\n",
    "\n",
    "  // -------------------------------------------\n",
    "  // 7. Overwrite table with new data\n",
    "  // -------------------------------------------\n",
    "  finalDf.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(tablePath)\n",
    "\n",
    "  println(s\"âœ” Simulation complete on $tablePath\")\n",
    "  println(s\"Updated: ${updateKeys.size}, Deleted: ${deleteKeys.size}, Inserted: ${count}\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b065b58",
   "metadata": {},
   "source": [
    "#### 2. simulateDeltaChangesSchemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21778f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulateDeltaChangesSchemas(catalog: String, schema: String, count: Int = 10): Unit = {\n",
    "  // 1. Get list of tables from source schema\n",
    "  val tables = spark.sql(s\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM ${catalog}.information_schema.tables\n",
    "    WHERE table_schema = '${schema}'\n",
    "  \"\"\").collect().map(_.getString(0))\n",
    "\n",
    "  // 2. Loop and deep clone each table\n",
    "  tables.foreach { tableName =>\n",
    "      val tb = s\"${catalog}.${schema}.${tableName}\"\n",
    "\n",
    "      simulateDeltaChanges(\n",
    "        tablePath = tb,  \n",
    "        primaryKey = \"id\",\n",
    "        count  // simulate 10 inserts, 10 updates, 10 deletes\n",
    "      )\n",
    "\n",
    "    println(s\"simulateDeltaChanges table: $tb\")\n",
    "  }\n",
    "\n",
    "  println(\"simulateDeltaChanges completed for all tables.\")\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9025dd",
   "metadata": {},
   "source": [
    "#### 3. ACS Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val acsSchemas = List(\n",
    "  \"gold_entity\",\n",
    "  \"gold_wos\",\n",
    "  \"gold_pprn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81f0e4",
   "metadata": {},
   "source": [
    "#### 4. Run simulateDeltaChangesSchemas fod ACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val env = \"dev\"\n",
    "val version = \"v1_0\"\n",
    "val catalog = s\"ag_ra_search_analytics_data_${env}\"\n",
    "\n",
    "acsSchemas.foreach { case schema =>\n",
    "  val src_schema = s\"${schema}_${version}\"\n",
    "  println(s\"Processing: source = $src_schema\")\n",
    "  val start = System.nanoTime() \n",
    "  simulateDeltaChangesSchemas(catalog, src_schema, count = 10)\n",
    "  println(s\"Time taken: ${(System.nanoTime() - start) / 1e9} seconds\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852304da",
   "metadata": {},
   "source": [
    "#### 5. Test  simulateDeltaChanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simulateDeltaChanges(\n",
    "  tablePath = \"schema.table\",  // or \"schema.table\"\n",
    "  primaryKey = \"id\",\n",
    "  count = 10  // simulate 100 inserts, 100 updates, 100 deletes\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
