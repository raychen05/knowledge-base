{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6033ae4a",
   "metadata": {},
   "source": [
    "#### 1. DeltaLinkGenerator.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f17d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// DeltaLinkGenerator.scala\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import io.delta.tables._\n",
    "import java.sql.Timestamp\n",
    "\n",
    "object DeltaLinkGenerator {\n",
    "\n",
    "  // -------------------------\n",
    "  // Config / Case classes\n",
    "  // -------------------------\n",
    "  case class Upstream(alias: String, tablePath: String, keyCol: String)\n",
    "  case class PipelineConfig(\n",
    "    pipelineName: String,\n",
    "    upstreams: Seq[Upstream],\n",
    "    // Optional join SQL: must produce columns: entity_key, last_commit_version, last_change_ts, upstream_tables (array<string>)\n",
    "    // The SQL may reference temp views with names equal to the upstream.alias (e.g., SELECT ... FROM a JOIN b ON ...)\n",
    "    // If joinSql is None, the engine will UNION distinct keys from each upstream (simple case).\n",
    "    joinSql: Option[String] = None,\n",
    "    // path where per-pipeline link table will be stored (delta)\n",
    "    linkTablePath: String,\n",
    "    // optional checkpoint path for streaming\n",
    "    checkpointPath: Option[String] = None\n",
    "  )\n",
    "\n",
    "  // Tracker table name (metastore) or path; here we use a Delta path for portability\n",
    "  val trackerPathDefault = \"/mnt/delta/delta_link_tracker\"\n",
    "\n",
    "  // -------------------------\n",
    "  // Utilities\n",
    "  // -------------------------\n",
    "  private def spark: SparkSession = SparkSession.builder().getOrCreate()\n",
    "\n",
    "  import spark.implicits._\n",
    "\n",
    "  // Initialize tracker table if not exists\n",
    "  def initTracker(trackerPath: String = trackerPathDefault): Unit = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    if (!io.delta.tables.DeltaTable.isDeltaTable(spark, trackerPath)) {\n",
    "      val empty = Seq.empty[(String, Long, Timestamp)].toDF(\"upstream_table\", \"last_processed_version\", \"last_processed_ts\")\n",
    "      empty.write.format(\"delta\").mode(\"overwrite\").save(trackerPath)\n",
    "      println(s\"Created tracker table at $trackerPath\")\n",
    "    } else {\n",
    "      println(s\"Tracker table already exists at $trackerPath\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // read last processed version for a table (0 if not present)\n",
    "  def getLastProcessedVersion(upstreamTable: String, trackerPath: String = trackerPathDefault): Long = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    if (!io.delta.tables.DeltaTable.isDeltaTable(spark, trackerPath)) {\n",
    "      return 0L\n",
    "    }\n",
    "    val df = spark.read.format(\"delta\").load(trackerPath)\n",
    "    val row = df.filter($\"upstream_table\" === upstreamTable).select(\"last_processed_version\").as[Long].collect()\n",
    "    if (row.isEmpty) 0L else row.head\n",
    "  }\n",
    "\n",
    "  // update tracker entries for multiple upstreams\n",
    "  def updateTracker(updates: Seq[(String, Long, java.sql.Timestamp)], trackerPath: String = trackerPathDefault): Unit = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    // ensure exists\n",
    "    initTracker(trackerPath)\n",
    "\n",
    "    val temp = updates.toDF(\"upstream_table\", \"last_processed_version\", \"last_processed_ts\")\n",
    "    val dt = DeltaTable.forPath(spark, trackerPath)\n",
    "\n",
    "    // Merge: update existing rows, insert new ones\n",
    "    dt.as(\"t\")\n",
    "      .merge(\n",
    "        temp.as(\"s\"),\n",
    "        \"t.upstream_table = s.upstream_table\"\n",
    "      )\n",
    "      .whenMatched()\n",
    "      .updateExpr(Map(\n",
    "        \"last_processed_version\" -> \"s.last_processed_version\",\n",
    "        \"last_processed_ts\" -> \"s.last_processed_ts\"\n",
    "      ))\n",
    "      .whenNotMatched()\n",
    "      .insertExpr(Map(\n",
    "        \"upstream_table\" -> \"s.upstream_table\",\n",
    "        \"last_processed_version\" -> \"s.last_processed_version\",\n",
    "        \"last_processed_ts\" -> \"s.last_processed_ts\"\n",
    "      ))\n",
    "      .execute()\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Batch: read change rows for an upstream from lastProcessedVersion+1 to latest\n",
    "  // returns DataFrame with its columns plus _commit_version and _commit_timestamp\n",
    "  // and renames the key column to the user-specified name for convenience (kept as original)\n",
    "  // -------------------------\n",
    "  def readChangesBatch(up: Upstream, fromVersionExclusive: Long): DataFrame = {\n",
    "    // readChangeData true supports .option(\"startingVersion\", v) or we can filter by _commit_version > v\n",
    "    // We'll read the change feed from beginning (or use starting version) and filter by > fromVersionExclusive\n",
    "    val dfRaw = spark.read\n",
    "      .format(\"delta\")\n",
    "      .option(\"readChangeData\", \"true\")\n",
    "      .load(up.tablePath)\n",
    "\n",
    "    // Some environments might require explicitly filtering _commit_version > fromVersionExclusive\n",
    "    val df = dfRaw\n",
    "      .filter(col(\"_change_type\").isin(\"insert\", \"update\", \"delete\"))\n",
    "      .filter(col(\"_commit_version\") > lit(fromVersionExclusive))\n",
    "      .withColumn(\"upstream_table\", lit(up.tablePath))\n",
    "      .withColumnRenamed(up.keyCol, up.keyCol) // keep original name but it's available in the view\n",
    "\n",
    "    df\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Aggregation & merge helper for link table\n",
    "  // Input: DataFrame expected columns -> entity_key, last_commit_version, last_change_ts, upstream_tables (array<string>)\n",
    "  // -------------------------\n",
    "  def writeLinkTableWithMerge(pipeline: PipelineConfig, df: DataFrame): Unit = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    // Ensure the output table exists; if not create with df.schema\n",
    "    if (!io.delta.tables.DeltaTable.isDeltaTable(spark, pipeline.linkTablePath)) {\n",
    "      df.write.format(\"delta\").mode(\"overwrite\").save(pipeline.linkTablePath)\n",
    "      println(s\"Created link table at ${pipeline.linkTablePath}\")\n",
    "      return\n",
    "    }\n",
    "\n",
    "    val deltaT = DeltaTable.forPath(spark, pipeline.linkTablePath)\n",
    "    // Merge by pipeline + entity_key; pipelineName stored in df or we add it\n",
    "    val src = df.withColumn(\"pipeline_name\", lit(pipeline.pipelineName)).alias(\"s\")\n",
    "    deltaT.as(\"t\")\n",
    "      .merge(\n",
    "        src,\n",
    "        \"t.pipeline_name = s.pipeline_name AND t.entity_key = s.entity_key\"\n",
    "      )\n",
    "      .whenMatched(\"s.last_commit_version > t.last_commit_version\")\n",
    "      .updateExpr(Map(\n",
    "        \"last_commit_version\" -> \"s.last_commit_version\",\n",
    "        \"last_change_ts\" -> \"s.last_change_ts\",\n",
    "        \"upstream_tables\" -> \"s.upstream_tables\",\n",
    "        \"updated_at\" -> \"current_timestamp()\"\n",
    "      ))\n",
    "      .whenNotMatched()\n",
    "      .insertExpr(Map(\n",
    "        \"pipeline_name\" -> \"s.pipeline_name\",\n",
    "        \"entity_key\" -> \"s.entity_key\",\n",
    "        \"last_commit_version\" -> \"s.last_commit_version\",\n",
    "        \"last_change_ts\" -> \"s.last_change_ts\",\n",
    "        \"upstream_tables\" -> \"s.upstream_tables\",\n",
    "        \"created_at\" -> \"current_timestamp()\",\n",
    "        \"updated_at\" -> \"current_timestamp()\"\n",
    "      ))\n",
    "      .execute()\n",
    "    println(s\"MERGE completed for pipeline ${pipeline.pipelineName} into ${pipeline.linkTablePath}\")\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Run batch processing for one pipeline (supports joinSql or union)\n",
    "  // -------------------------\n",
    "  def runPipelineBatch(pipeline: PipelineConfig, trackerPath: String = trackerPathDefault): Unit = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    // 1) Read last processed versions for each upstream\n",
    "    val lastVersions: Map[String, Long] = pipeline.upstreams.map { up =>\n",
    "      up.alias -> getLastProcessedVersion(up.tablePath, trackerPath)\n",
    "    }.toMap\n",
    "\n",
    "    // 2) Find latest versions for upstream tables (from table history) to decide upper bound\n",
    "    // We can find latest version via DeltaTable.forPath(...).history(1) or using snapshot metadata\n",
    "    val latestVersions: Map[String, Long] = pipeline.upstreams.map { up =>\n",
    "      val dt = io.delta.tables.DeltaTable.forPath(spark, up.tablePath)\n",
    "      val latest = dt.history(1).select(\"version\").as[Long].collect().headOption.getOrElse(0L)\n",
    "      up.alias -> latest\n",
    "    }.toMap\n",
    "\n",
    "    // 3) For each upstream, read changes > lastProcessedVersion\n",
    "    // Register as temp views with their alias name so joinSql can reference them.\n",
    "    pipeline.upstreams.foreach { up =>\n",
    "      val fromV = lastVersions(up.alias)\n",
    "      val df = readChangesBatch(up, fromV)\n",
    "      // Register temp view for SQL joins. The view keeps the original key column name.\n",
    "      df.createOrReplaceTempView(up.alias)\n",
    "    }\n",
    "\n",
    "    // 4) Build final DF\n",
    "    val finalDF: DataFrame = pipeline.joinSql match {\n",
    "      case Some(sql) =>\n",
    "        // User-provided SQL must return columns: entity_key, last_commit_version, last_change_ts, upstream_tables (array<string>)\n",
    "        // We run it directly.\n",
    "        val res = spark.sql(sql)\n",
    "        // Validation for expected columns (light)\n",
    "        val cols = res.columns.toSet\n",
    "        require(cols.contains(\"entity_key\") && cols.contains(\"last_commit_version\") && cols.contains(\"last_change_ts\"),\n",
    "          s\"joinSql must return entity_key, last_commit_version, last_change_ts. got: ${cols.mkString(\",\")}\")\n",
    "        // if upstream_tables is missing, try to create it from constants in SQL or fallback\n",
    "        val withUpstreams = if (cols.contains(\"upstream_tables\")) res\n",
    "                            else res.withColumn(\"upstream_tables\", array(lit(pipeline.upstreams.map(_.tablePath): _*)))\n",
    "        withUpstreams\n",
    "      case None =>\n",
    "        // Simple union of distinct keys: we expect each upstream temp view to expose the key column provided in Upstream.keyCol.\n",
    "        // We'll select each upstream's key as entity_key, commit_version, commit_ts, upstream_table then union.\n",
    "        val perUp = pipeline.upstreams.map { up =>\n",
    "          // select key as entity_key\n",
    "          val colExpr = col(up.keyCol).as(\"entity_key\")\n",
    "          spark.table(up.alias)\n",
    "            .select(colExpr, col(\"_commit_version\"), col(\"_commit_timestamp\"))\n",
    "            .withColumn(\"upstream_tables\", array(lit(up.tablePath)))\n",
    "        }\n",
    "        val unioned = perUp.reduce(_.unionByName(_, allowMissingColumns = true))\n",
    "        // collapse to latest commit per entity_key\n",
    "        unioned.groupBy(\"entity_key\")\n",
    "          .agg(\n",
    "            max(\"_commit_version\").as(\"last_commit_version\"),\n",
    "            max(\"_commit_timestamp\").as(\"last_change_ts\"),\n",
    "            flatten(collect_set(\"upstream_tables\")).as(\"upstream_tables\")\n",
    "          )\n",
    "    }\n",
    "\n",
    "    // Add pipeline_name if not present\n",
    "    val finalWithPipeline = if (finalDF.columns.contains(\"pipeline_name\")) finalDF else finalDF.withColumn(\"pipeline_name\", lit(pipeline.pipelineName))\n",
    "\n",
    "    // Normalize columns\n",
    "    val normalized = finalWithPipeline\n",
    "      .selectExpr(\"pipeline_name\", \"entity_key\", \"cast(last_commit_version as long) as last_commit_version\", \"cast(last_change_ts as timestamp) as last_change_ts\", \"upstream_tables\")\n",
    "      .withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "    // 5) Write/MERGE into link table\n",
    "    writeLinkTableWithMerge(pipeline, normalized)\n",
    "\n",
    "    // 6) Update tracker: set last_processed_version to latestVersions per upstream\n",
    "    val updates = pipeline.upstreams.map { up =>\n",
    "      val latestV = latestVersions(up.alias)\n",
    "      (up.tablePath, latestV, new java.sql.Timestamp(System.currentTimeMillis()))\n",
    "    }\n",
    "    updateTracker(updates, trackerPath)\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Run batch for many pipelines\n",
    "  // -------------------------\n",
    "  def runAllPipelinesBatch(pipelines: Seq[PipelineConfig], trackerPath: String = trackerPathDefault): Unit = {\n",
    "    // ensure tracker exists\n",
    "    initTracker(trackerPath)\n",
    "    pipelines.foreach { p =>\n",
    "      println(s\"===== Running batch for pipeline: ${p.pipelineName} =====\")\n",
    "      try {\n",
    "        runPipelineBatch(p, trackerPath)\n",
    "      } catch {\n",
    "        case e: Throwable =>\n",
    "          println(s\"ERROR running pipeline ${p.pipelineName}: ${e.getMessage}\")\n",
    "          e.printStackTrace()\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Streaming (continuous) - union-only mode\n",
    "  // Note: join-based streaming is non-trivial (stateful multi-stream join) and is not implemented here.\n",
    "  // If you need streaming joins, consider using Structured Streaming with event-time watermarks and careful\n",
    "  // windowing, or use DLT which simplifies streaming joins. For now streaming supports UNION mode for each upstream.\n",
    "  // -------------------------\n",
    "  def runPipelineStreamingUnion(pipeline: PipelineConfig): Unit = {\n",
    "    require(pipeline.joinSql.isEmpty, \"Streaming union runner supports only pipelines with no joinSql (simple union).\")\n",
    "\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    // Build streaming union of upstream change streams\n",
    "    val changeStreams = pipeline.upstreams.map { up =>\n",
    "      spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"readChangeData\", \"true\")\n",
    "        .load(up.tablePath)\n",
    "        .filter(col(\"_change_type\").isin(\"insert\", \"update\", \"delete\"))\n",
    "        .select(col(up.keyCol).as(\"entity_key\"), col(\"_commit_version\"), col(\"_commit_timestamp\"))\n",
    "        .withColumn(\"upstream_tables\", array(lit(up.tablePath)))\n",
    "    }\n",
    "\n",
    "    val unioned = changeStreams.reduce(_.unionByName(_, allowMissingColumns = true))\n",
    "\n",
    "    val deduped = unioned\n",
    "      .withWatermark(\"_commit_timestamp\", \"1 day\")\n",
    "      .groupBy(\"entity_key\")\n",
    "      .agg(\n",
    "        max(\"_commit_version\").as(\"last_commit_version\"),\n",
    "        max(\"_commit_timestamp\").as(\"last_change_ts\"),\n",
    "        flatten(collect_set(\"upstream_tables\")).as(\"upstream_tables\")\n",
    "      )\n",
    "      .withColumn(\"pipeline_name\", lit(pipeline.pipelineName))\n",
    "      .withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "    val checkpoint = pipeline.checkpointPath.getOrElse(s\"/mnt/checkpoints/delta_link_${pipeline.pipelineName}\")\n",
    "\n",
    "    val query = deduped.writeStream\n",
    "      .format(\"delta\")\n",
    "      .outputMode(\"update\")\n",
    "      .option(\"checkpointLocation\", checkpoint)\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .trigger(Trigger.ProcessingTime(\"5 minutes\"))\n",
    "      .start(pipeline.linkTablePath)\n",
    "\n",
    "    println(s\"Started streaming query for pipeline ${pipeline.pipelineName}; writing to ${pipeline.linkTablePath} with checkpoint $checkpoint\")\n",
    "    query.awaitTermination()\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Example: helper that constructs a simple JOIN SQL for two upstreams.\n",
    "  // If you prefer to supply custom SQL, skip this and provide joinSql in config.\n",
    "  // Example usage shown below in mainExample.\n",
    "  // -------------------------\n",
    "  def buildTwoTableJoinSql(aAlias: String, aKey: String, bAlias: String, bKey: String, downstreamKeyExpr: String): String = {\n",
    "    // downstreamKeyExpr is an expression using aliases a and b (e.g. \"coalesce(a.some_fk, b.other_fk) as entity_key\")\n",
    "    s\"\"\"\n",
    "      SELECT\n",
    "        ${downstreamKeyExpr},\n",
    "        greatest(a._commit_version, b._commit_version) as last_commit_version,\n",
    "        greatest(a._commit_timestamp, b._commit_timestamp) as last_change_ts,\n",
    "        array('${aAlias}', '${bAlias}') as upstream_tables\n",
    "      FROM $aAlias a\n",
    "      FULL OUTER JOIN $bAlias b\n",
    "        ON <PUT_JOIN_CONDITION_HERE>\n",
    "      -- NOTE: Replace <PUT_JOIN_CONDITION_HERE> with actual join condition, e.g., a.x = b.x\n",
    "    \"\"\"\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Example main showing usage\n",
    "  // -------------------------\n",
    "  def mainExample(): Unit = {\n",
    "    // Example pipelines\n",
    "    val pipeline1 = PipelineConfig(\n",
    "      pipelineName = \"pipelineX\",\n",
    "      upstreams = Seq(\n",
    "        Upstream(\"a\", \"/mnt/delta/upstream_A\", \"a_id\"),\n",
    "        Upstream(\"b\", \"/mnt/delta/upstream_B\", \"b_id\")\n",
    "      ),\n",
    "      // Provide a joinSql that references temp views 'a' and 'b'\n",
    "      // The minimal required output columns are: entity_key, last_commit_version, last_change_ts, upstream_tables\n",
    "      joinSql = Some(\n",
    "        \"\"\"\n",
    "        -- Example: join where downstream entity_key is coalesced foreign key\n",
    "        SELECT\n",
    "          coalesce(a.some_fk, b.some_fk) as entity_key,\n",
    "          greatest(a._commit_version, b._commit_version) as last_commit_version,\n",
    "          greatest(a._commit_timestamp, b._commit_timestamp) as last_change_ts,\n",
    "          array(a.upstream_table, b.upstream_table) as upstream_tables\n",
    "        FROM a\n",
    "        FULL OUTER JOIN b\n",
    "          ON a.some_fk = b.some_fk\n",
    "        \"\"\"\n",
    "      ),\n",
    "      linkTablePath = \"/mnt/delta/delta_link_pipelineX\",\n",
    "      checkpointPath = Some(\"/mnt/checkpoints/delta_link_pipelineX\")\n",
    "    )\n",
    "\n",
    "    val pipeline2 = PipelineConfig(\n",
    "      pipelineName = \"pipelineY\",\n",
    "      upstreams = Seq(\n",
    "        Upstream(\"c\", \"/mnt/delta/upstream_C\", \"c_id\")\n",
    "      ),\n",
    "      joinSql = None,\n",
    "      linkTablePath = \"/mnt/delta/delta_link_pipelineY\",\n",
    "      checkpointPath = Some(\"/mnt/checkpoints/delta_link_pipelineY\")\n",
    "    )\n",
    "\n",
    "    // initialize tracker\n",
    "    initTracker()\n",
    "\n",
    "    // run batch for all pipelines\n",
    "    runAllPipelinesBatch(Seq(pipeline1, pipeline2))\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e931444",
   "metadata": {},
   "source": [
    "#### 2.  Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f60b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8385fbf6",
   "metadata": {},
   "source": [
    "#### 3. DLT  Implemnetation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9e189",
   "metadata": {},
   "source": [
    "##### 3.1 readStream  - outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# outer join doesn't work in streaming\n",
    "\n",
    "from pyspark import pipelines as dp\n",
    "#from databricks import pipelines\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# ------------ CONFIG ------------------\n",
    "\n",
    "#DB = \"ag_ra_search_analytics_data_dev.sandbox_v1_0\"\n",
    "DB = \"ag_content_ims_dev.acs_entity\"\n",
    "DB2= \"ag_content_ims_dev.acs_wos\"\n",
    "\n",
    "SPMASTER = f\"{DB}.d_spmaster\"\n",
    "AFFILIATION = f\"{DB2}.d_daisng_ranked_affiliation\"\n",
    "\n",
    "START_VERSION = \"2\"   # Or load from pipeline checkpoint / control table\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 1. READ CDF CHANGES AS DP VIEWS\n",
    "# =======================================================================\n",
    "\n",
    "@dp.view\n",
    "def spmaster_changes():\n",
    "    return (\n",
    "        spark.readStream.format(\"delta\")\n",
    "            .table(SPMASTER)\n",
    "            #.filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\", \"delete\"))\n",
    "            #.selectExpr(\"diasng_id\", \"_change_type\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "@dp.view\n",
    "def affiliation_changes():\n",
    "    return (\n",
    "        spark.readStream.format(\"delta\")\n",
    "            .table(AFFILIATION)\n",
    "            #.filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\", \"delete\"))\n",
    "            #.selectExpr(\"sp_id\", \"institution_key\", \"_change_type\")\n",
    "    )\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 2. GENERATE DOWNSTREAM AFFECTED KEY PAIRS\n",
    "# =======================================================================\n",
    "\n",
    "@dp.table(\n",
    "    name=\"link_sp_affiliation\",\n",
    "    comment=\"Affected (diasng_id, institution_key) derived from CDF changes from d_spmaster and d_daisng_ranked_affiliation.\"\n",
    ")\n",
    "def link_sp_affiliation():\n",
    "    \n",
    "    # Extract distinct changed keys from each upstream table\n",
    "    sp = (\n",
    "        dp.readStream(\"spmaster_changes\")\n",
    "                 .select(\"diasng_id\")\n",
    "                 .distinct()\n",
    "    )\n",
    "\n",
    "    aff = (\n",
    "        dp.readStream(\"affiliation_changes\")\n",
    "                 .select(\"sp_id\", \"institution_key\")\n",
    "                 .distinct()\n",
    "    )\n",
    "\n",
    "    # FULL OUTER JOIN guarantees:\n",
    "    #  - If spmaster changes → produce pairs\n",
    "    #  - If affiliation changes → produce ALL diasng_id linked\n",
    "    #  - Handles deletes, inserts, updates\n",
    "    joined = (\n",
    "        sp.join(aff, sp.diasng_id == aff.sp_id, \"full_outer\")\n",
    "          .select(\n",
    "              col(\"diasng_id\"),\n",
    "              col(\"institution_key\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    # Affected downstream keys\n",
    "    return joined.filter(col(\"diasng_id\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214f422",
   "metadata": {},
   "source": [
    "##### 3.2 readStream - Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# workable streaming pattern: union of both streams\n",
    "\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# ------------ CONFIG ------------------\n",
    "\n",
    "DB = \"ag_content_ims_dev.acs_entity\"\n",
    "DB2 = \"ag_content_ims_dev.acs_wos\"\n",
    "\n",
    "SPMASTER = f\"{DB}.d_spmaster\"\n",
    "AFFILIATION = f\"{DB2}.d_daisng_ranked_affiliation\"\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 1. READ CDF CHANGES AS DP VIEWS\n",
    "# =======================================================================\n",
    "\n",
    "@dp.view\n",
    "def spmaster_changes():\n",
    "    return (\n",
    "        spark.readStream.format(\"delta\")\n",
    "             .table(SPMASTER)\n",
    "             .select(\"diasng_id\")\n",
    "             .distinct()\n",
    "    )\n",
    "\n",
    "\n",
    "@dp.view\n",
    "def affiliation_changes():\n",
    "    return (\n",
    "        spark.readStream.format(\"delta\")\n",
    "             .table(AFFILIATION)\n",
    "             .select(\"sp_id\", \"institution_key\")\n",
    "             .distinct()\n",
    "    )\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 2. GENERATE AFFECTED KEY PAIRS\n",
    "# =======================================================================\n",
    "\n",
    "@dp.table(\n",
    "    name=\"link_sp_affiliation\",\n",
    "    comment=\"Affected (diasng_id, institution_key) derived from both CDF streams.\"\n",
    ")\n",
    "def link_sp_affiliation():\n",
    "\n",
    "    # 1. SP master change → produce (diasng_id, null)\n",
    "    sp_keys = (\n",
    "        dp.readStream(\"spmaster_changes\")\n",
    "          .select(\n",
    "              col(\"diasng_id\"),\n",
    "              lit(None).cast(\"string\").alias(\"institution_key\"),\n",
    "              lit(\"spmaster\").alias(\"source\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    # 2. Affiliation change → produce (diasng_id, institution_key)\n",
    "    aff_keys = (\n",
    "        dp.readStream(\"affiliation_changes\")\n",
    "          .select(\n",
    "              col(\"sp_id\").alias(\"diasng_id\"),\n",
    "              col(\"institution_key\"),\n",
    "              lit(\"affiliation\").alias(\"source\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    # Union both streams — VALID streaming pattern\n",
    "    unified = sp_keys.unionByName(aff_keys)\n",
    "\n",
    "    # Deduplicate keys across microbatches\n",
    "    return unified.dropDuplicates([\"diasng_id\", \"institution_key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fddcf",
   "metadata": {},
   "source": [
    "##### 3.2 read CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270175f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark import pipelines as dp\n",
    "#from databricks import pipelines\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# ------------ CONFIG ------------------\n",
    "\n",
    "DB = \"ag_ra_search_analytics_data_dev.sandbox_v1_0\"\n",
    "DB1 = \"ag_content_ims_dev.acs_entity\"\n",
    "DB2= \"ag_content_ims_dev.acs_wos\"\n",
    "\n",
    "SPMASTER = f\"{DB}.d_spmaster\"\n",
    "AFFILIATION = f\"{DB}.d_daisng_ranked_affiliation\"\n",
    "\n",
    "START_VERSION = \"2\"   # Or load from pipeline checkpoint / control table\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 1. READ CDF CHANGES AS DP VIEWS\n",
    "# =======================================================================\n",
    "\n",
    "@dp.view\n",
    "def spmaster_changes():\n",
    "    return (\n",
    "         spark.read.format(\"delta\")\n",
    "            .option(\"readChangeData\", \"true\")\n",
    "            .option(\"startingVersion\", START_VERSION)\n",
    "            .table(SPMASTER)\n",
    "            .filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\", \"delete\"))\n",
    "            .selectExpr(\"diasng_id\", \"_change_type\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "@dp.view\n",
    "def affiliation_changes():\n",
    "    return (\n",
    "        spark.read.format(\"delta\")\n",
    "            .option(\"readChangeData\", \"true\")\n",
    "            .option(\"startingVersion\", START_VERSION)\n",
    "            .table(AFFILIATION)\n",
    "            .filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\", \"delete\"))\n",
    "            .selectExpr(\"sp_id\", \"institution_key\", \"_change_type\")\n",
    "    )\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 2. GENERATE DOWNSTREAM AFFECTED KEY PAIRS\n",
    "# =======================================================================\n",
    "\n",
    "@dp.table(\n",
    "    name=\"link_sp_affiliation\",\n",
    "    comment=\"Affected (diasng_id, institution_key) derived from CDF changes from d_spmaster and d_daisng_ranked_affiliation.\"\n",
    ")\n",
    "def link_sp_affiliation():\n",
    "    \n",
    "    # Extract distinct changed keys from each upstream table\n",
    "    sp = (\n",
    "        dp.read(\"spmaster_changes\")\n",
    "                 .select(\"diasng_id\")\n",
    "                 .distinct()\n",
    "    )\n",
    "\n",
    "    aff = (\n",
    "        dp.read(\"affiliation_changes\")\n",
    "                 .select(\"sp_id\", \"institution_key\")\n",
    "                 .distinct()\n",
    "    )\n",
    "\n",
    "    # FULL OUTER JOIN guarantees:\n",
    "    #  - If spmaster changes → produce pairs\n",
    "    #  - If affiliation changes → produce ALL diasng_id linked\n",
    "    #  - Handles deletes, inserts, updates\n",
    "    joined = (\n",
    "        sp.join(aff, sp.diasng_id == aff.sp_id, \"full_outer\")\n",
    "          .select(\n",
    "              col(\"diasng_id\"),\n",
    "              col(\"institution_key\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    # Affected downstream keys\n",
    "    return joined.filter(col(\"diasng_id\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da33311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_catalog = ag_ra_search_analytics_data\n",
       "target_environment = dev\n",
       "target_version = v1_0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "getDapSchemas: ()Seq[String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_ra_search_analytics_data_dev.dap_entity_wos_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_metrics_wos_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_entity_pprn_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_metrics_pprn_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_docs_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_reference_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_sort_ref_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_entity_enrich_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_grant_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_prod_core_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_ops_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_work_v1_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "v1_0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  \n",
    "  val target_catalog = \"ag_ra_search_analytics_data\"\n",
    "  val target_environment = \"dev\"\n",
    "  val target_version = \"v1_0\"\n",
    "\n",
    "\n",
    "  def getDapSchemas(): Seq[String] = {\n",
    "    val base = s\"${target_catalog}_$target_environment\"\n",
    "    val suffixes = Seq(\n",
    "      \"dap_entity_wos\",\n",
    "      \"dap_metrics_wos\",\n",
    "      \"dap_entity_pprn\",\n",
    "      \"dap_metrics_pprn\",\n",
    "      \"dap_docs\",\n",
    "      \"dap_reference\",\n",
    "      \"dap_sort_ref\",\n",
    "      \"dap_entity_enrich\",\n",
    "      \"dap_grant\",\n",
    "      \"dap_prod_core\",\n",
    "      \"dap_ops\",\n",
    "      \"dap_work\"\n",
    "    )\n",
    "    suffixes.map { suffix =>\n",
    "      s\"$base.${suffix}_$target_version\"\n",
    "    }\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "  getDapSchemas().foreach(println)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e60b0933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getSchemaMap: ()Map[String,String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dap_docs -> ag_ra_search_analytics_data_dev.dap_docsv1_0\n",
      "dap_entity_pprn -> ag_ra_search_analytics_data_dev.dap_entity_pprnv1_0\n",
      "pprn -> ag_content_ims_dev.gold_pprnv1_0_)\n",
      "dap_metrics_pprn -> ag_ra_search_analytics_data_dev.dap_metrics_pprnv1_0\n",
      "dap_entity_wos -> ag_ra_search_analytics_data_dev.dap_entity_wosv1_0\n",
      "dap_metrics_wos -> ag_ra_search_analytics_data_dev.dap_metrics_wosv1_0\n",
      "dap_entity_enrich -> ag_ra_search_analytics_data_dev.dap_entity_enrichv1_0\n",
      "wos -> ag_content_ims_dev.gold_wosv1_0_)\n",
      "dap -> ag_ra_search_analytics_data_dev.sandboxv1_0\n",
      "dap_reference -> ag_ra_search_analytics_data_dev.dap_referencev1_0\n",
      "entity -> ag_content_ims_dev.gold_entityv1_0_)\n",
      "dap_prod_core -> ag_ra_search_analytics_data_dev.dap_prod_corev1_0\n",
      "dap_ops -> ag_ra_search_analytics_data_dev.dap_opsv1_0\n",
      "dap_grant -> ag_ra_search_analytics_data_dev.dap_grantv1_0\n",
      "dap_work -> ag_ra_search_analytics_data_dev.dap_workv1_0\n",
      "dap_sort_ref -> ag_ra_search_analytics_data_dev.dap_sort_refv1_0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getSchemaMap(): Map[String, String] = {\n",
    "  val srcBase = \"ag_content_ims_dev\"\n",
    "  val tgtBase = \"ag_ra_search_analytics_data_dev\"\n",
    "  val versionSuffix = \"v1_0_)\"\n",
    "  val tgtVerSuffix = \"v1_0\"\n",
    "\n",
    "  // ACS schemas: key → base name (without version)\n",
    "  val acsSchemas = Seq(\n",
    "    \"entity\" -> s\"$srcBase.gold_entity$versionSuffix\",\n",
    "    \"wos\"    -> s\"$srcBase.gold_wos$versionSuffix\",\n",
    "    \"pprn\"   -> s\"$srcBase.gold_pprn$versionSuffix\"\n",
    "  )\n",
    "\n",
    "  // DAP schemas: key → base name (without target version suffix)\n",
    "  val dapKeys = Seq(\n",
    "    \"dap_entity_wos\",\n",
    "    \"dap_metrics_wos\",\n",
    "    \"dap_entity_pprn\",\n",
    "    \"dap_metrics_pprn\",\n",
    "    \"dap_docs\",\n",
    "    \"dap_reference\",\n",
    "    \"dap_sort_ref\",\n",
    "    \"dap_entity_enrich\",\n",
    "    \"dap_grant\",\n",
    "    \"dap_prod_core\",\n",
    "    \"dap_ops\",\n",
    "    \"dap_work\"\n",
    "  )\n",
    "\n",
    "  val dapSchemas: Seq[(String, String)] =\n",
    "    dapKeys.map { key =>\n",
    "      key -> s\"$tgtBase.$key$tgtVerSuffix\"\n",
    "    }\n",
    "\n",
    "  // Optional sandbox entry\n",
    "  val sandboxEntry = Seq(\n",
    "    \"dap\" -> s\"$tgtBase.sandbox$tgtVerSuffix\"\n",
    "  )\n",
    "\n",
    "  // Combine all\n",
    "  (acsSchemas ++ dapSchemas ++ sandboxEntry).toMap\n",
    "}\n",
    "\n",
    "  getSchemaMap().foreach{ case (k,v) => println(s\"$k -> $v\") }  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70efeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object SchemaResolver\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dap_docs -> ag_ra_search_analytics_data_dev.dap_docs_v1_0\n",
      "dap_entity_pprn -> ag_ra_search_analytics_data_dev.dap_entity_pprn_v1_0\n",
      "pprn -> ag_content_ims_acs_prod.gold_pprn\n",
      "dap_metrics_pprn -> ag_ra_search_analytics_data_dev.dap_metrics_pprn_v1_0\n",
      "dap_entity_wos -> ag_ra_search_analytics_data_dev.dap_entity_wos_v1_0\n",
      "dap_metrics_wos -> ag_ra_search_analytics_data_dev.dap_metrics_wos_v1_0\n",
      "dap_entity_enrich -> ag_ra_search_analytics_data_dev.dap_entity_enrich_v1_0\n",
      "wos -> ag_content_ims_acs_prod.gold_wos\n",
      "dap -> ag_ra_search_analytics_data_dev.sandbox_v1_0\n",
      "dap_reference -> ag_ra_search_analytics_data_dev.dap_reference_v1_0\n",
      "entity -> ag_content_ims_acs_prod.gold_entity\n",
      "dap_prod_core -> ag_ra_search_analytics_data_dev.dap_prod_core_v1_0\n",
      "dap_ops -> ag_ra_search_analytics_data_dev.dap_ops_v1_0\n",
      "dap_grant -> ag_ra_search_analytics_data_dev.dap_grant_v1_0\n",
      "dap_work -> ag_ra_search_analytics_data_dev.dap_work_v1_0\n",
      "dap_sort_ref -> ag_ra_search_analytics_data_dev.dap_sort_ref_v1_0\n",
      "-----\n",
      "ag_content_ims_acs_prod.gold_entity\n",
      "ag_content_ims_acs_prod.gold_wos\n",
      "ag_content_ims_acs_prod.gold_pprn\n",
      "ag_ra_search_analytics_data_dev.dap_entity_wos_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_metrics_wos_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_entity_pprn_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_metrics_pprn_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_docs_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_reference_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_sort_ref_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_entity_enrich_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_grant_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_prod_core_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_ops_v1_0\n",
      "ag_ra_search_analytics_data_dev.dap_work_v1_0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "object SchemaResolver {\n",
    "\n",
    "  private def getWidget(name: String, default: String): String = {\n",
    "    try {\n",
    "      val value = \"\" // dbutils.widgets.get(name)\n",
    "      if (value == null || value.isEmpty) default else value\n",
    "    } catch {\n",
    "      case _: Throwable => default\n",
    "    }\n",
    "  }\n",
    "\n",
    "  private val source_catalog = getWidget(\"source_catalog\", \"ag_content_ims_acs\")\n",
    "  private val source_environment = getWidget(\"source_environment\", \"prod\")\n",
    "  private val source_version = getWidget(\"source_version\", \"\")\n",
    "\n",
    "  private val target_catalog = getWidget(\"target_catalog\", \"ag_ra_search_analytics_data\")\n",
    "  private val target_environment = getWidget(\"target_environment\", \"dev\")\n",
    "  private val target_version = getWidget(\"target_version\", \"v1_0\")\n",
    "\n",
    "val dapSchemaBases = Seq(\n",
    "      \"dap_entity_wos\",\n",
    "      \"dap_metrics_wos\",\n",
    "      \"dap_entity_pprn\",\n",
    "      \"dap_metrics_pprn\",\n",
    "      \"dap_docs\",\n",
    "      \"dap_reference\",\n",
    "      \"dap_sort_ref\",\n",
    "      \"dap_entity_enrich\",\n",
    "      \"dap_grant\",\n",
    "      \"dap_prod_core\",\n",
    "      \"dap_ops\",\n",
    "      \"dap_work\"\n",
    "  )\n",
    "\n",
    "  private val acsSchemaBases= Seq(\n",
    "      \"gold_entity\",\n",
    "      \"gold_wos\",\n",
    "      \"gold_pprn\"\n",
    "  )\n",
    "\n",
    "  val SCHEMA_MAP: Map[String, String] = {\n",
    "    val srcBase = s\"${source_catalog}_${source_environment}\"\n",
    "    val tgtBase = s\"${target_catalog}_${target_environment}\"\n",
    "    val versionSuffix = if (source_version.isEmpty) \"\" else s\"_${source_version}\"\n",
    "    val tgtVerSuffix = s\"_${target_version}\"\n",
    "\n",
    "    // ACS schemas: key → base name (without version)\n",
    "    val acsSchemas = Seq(\n",
    "      \"entity\" -> s\"$srcBase.gold_entity$versionSuffix\",\n",
    "      \"wos\"    -> s\"$srcBase.gold_wos$versionSuffix\",\n",
    "      \"pprn\"   -> s\"$srcBase.gold_pprn$versionSuffix\"\n",
    "    )\n",
    "\n",
    "    // DAP schemas: key → base name (without target version suffix)\n",
    "    val dapSchemas: Seq[(String, String)] =\n",
    "      dapSchemaBases.map { key =>\n",
    "        key -> s\"$tgtBase.$key$tgtVerSuffix\"\n",
    "      }\n",
    "\n",
    "    // Optional sandbox entry\n",
    "    val sandboxEntry = Seq(\n",
    "      \"dap\" -> s\"$tgtBase.sandbox$tgtVerSuffix\"\n",
    "    )\n",
    "    // Combine all\n",
    "    (acsSchemas ++ dapSchemas ++ sandboxEntry).toMap\n",
    "  }\n",
    "\n",
    "  val SCHEMAS: Seq[String] = {\n",
    "    val srcBase = s\"${source_catalog}_${source_environment}\"\n",
    "    val tgtBase = s\"${target_catalog}_${target_environment}\"\n",
    "    val versionSuffix = if (source_version.isEmpty) \"\" else s\"_${source_version}\"\n",
    "    val tgtVerSuffix  = s\"_${target_version}\"\n",
    "\n",
    "    // ACS schema names\n",
    "    val acsSchemas = acsSchemaBases.map(name => s\"$srcBase.$name$versionSuffix\")\n",
    "\n",
    "    // DAP schema names\n",
    "    val dapSchemas = dapSchemaBases.map(baseName => s\"$tgtBase.$baseName$tgtVerSuffix\")\n",
    "\n",
    "    acsSchemas ++ dapSchemas\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SchemaResolver.SCHEMA_MAP.foreach{ case (k,v) => println(s\"$k -> $v\") }\n",
    "println(\"-----\")\n",
    "SchemaResolver.SCHEMAS.foreach{ schema => println(schema) }  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4800d304",
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:23: error: not found: value SchemaResolver\n       val pipelinesDF = SchemaResolver.dapSchemaBases.toDF()\n                         ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "\n",
    "\n",
    "val pipelinesDF = SchemaResolver.dapSchemaBases.toDF()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
