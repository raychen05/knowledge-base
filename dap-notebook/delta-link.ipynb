{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6033ae4a",
   "metadata": {},
   "source": [
    "#### 1. DeltaLinkGenerator.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f17d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// DeltaLinkGenerator.scala\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import io.delta.tables._\n",
    "import java.sql.Timestamp\n",
    "\n",
    "object DeltaLinkGenerator {\n",
    "\n",
    "  // -------------------------\n",
    "  // Config / Case classes\n",
    "  // -------------------------\n",
    "  case class Upstream(alias: String, tablePath: String, keyCol: String)\n",
    "  case class PipelineConfig(\n",
    "    pipelineName: String,\n",
    "    upstreams: Seq[Upstream],\n",
    "    // Optional join SQL: must produce columns: entity_key, last_commit_version, last_change_ts, upstream_tables (array<string>)\n",
    "    // The SQL may reference temp views with names equal to the upstream.alias (e.g., SELECT ... FROM a JOIN b ON ...)\n",
    "    // If joinSql is None, the engine will UNION distinct keys from each upstream (simple case).\n",
    "    joinSql: Option[String] = None,\n",
    "    // path where per-pipeline link table will be stored (delta)\n",
    "    linkTablePath: String,\n",
    "    // optional checkpoint path for streaming\n",
    "    checkpointPath: Option[String] = None\n",
    "  )\n",
    "\n",
    "  // Tracker table name (metastore) or path; here we use a Delta path for portability\n",
    "  val trackerPathDefault = \"/mnt/delta/delta_link_tracker\"\n",
    "\n",
    "  // -------------------------\n",
    "  // Utilities\n",
    "  // -------------------------\n",
    "  private def spark: SparkSession = SparkSession.builder().getOrCreate()\n",
    "\n",
    "  import spark.implicits._\n",
    "\n",
    "  // Initialize tracker table if not exists\n",
    "  def initTracker(trackerPath: String = trackerPathDefault): Unit = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    if (!io.delta.tables.DeltaTable.isDeltaTable(spark, trackerPath)) {\n",
    "      val empty = Seq.empty[(String, Long, Timestamp)].toDF(\"upstream_table\", \"last_processed_version\", \"last_processed_ts\")\n",
    "      empty.write.format(\"delta\").mode(\"overwrite\").save(trackerPath)\n",
    "      println(s\"Created tracker table at $trackerPath\")\n",
    "    } else {\n",
    "      println(s\"Tracker table already exists at $trackerPath\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // read last processed version for a table (0 if not present)\n",
    "  def getLastProcessedVersion(upstreamTable: String, trackerPath: String = trackerPathDefault): Long = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    if (!io.delta.tables.DeltaTable.isDeltaTable(spark, trackerPath)) {\n",
    "      return 0L\n",
    "    }\n",
    "    val df = spark.read.format(\"delta\").load(trackerPath)\n",
    "    val row = df.filter($\"upstream_table\" === upstreamTable).select(\"last_processed_version\").as[Long].collect()\n",
    "    if (row.isEmpty) 0L else row.head\n",
    "  }\n",
    "\n",
    "  // update tracker entries for multiple upstreams\n",
    "  def updateTracker(updates: Seq[(String, Long, java.sql.Timestamp)], trackerPath: String = trackerPathDefault): Unit = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    // ensure exists\n",
    "    initTracker(trackerPath)\n",
    "\n",
    "    val temp = updates.toDF(\"upstream_table\", \"last_processed_version\", \"last_processed_ts\")\n",
    "    val dt = DeltaTable.forPath(spark, trackerPath)\n",
    "\n",
    "    // Merge: update existing rows, insert new ones\n",
    "    dt.as(\"t\")\n",
    "      .merge(\n",
    "        temp.as(\"s\"),\n",
    "        \"t.upstream_table = s.upstream_table\"\n",
    "      )\n",
    "      .whenMatched()\n",
    "      .updateExpr(Map(\n",
    "        \"last_processed_version\" -> \"s.last_processed_version\",\n",
    "        \"last_processed_ts\" -> \"s.last_processed_ts\"\n",
    "      ))\n",
    "      .whenNotMatched()\n",
    "      .insertExpr(Map(\n",
    "        \"upstream_table\" -> \"s.upstream_table\",\n",
    "        \"last_processed_version\" -> \"s.last_processed_version\",\n",
    "        \"last_processed_ts\" -> \"s.last_processed_ts\"\n",
    "      ))\n",
    "      .execute()\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Batch: read change rows for an upstream from lastProcessedVersion+1 to latest\n",
    "  // returns DataFrame with its columns plus _commit_version and _commit_timestamp\n",
    "  // and renames the key column to the user-specified name for convenience (kept as original)\n",
    "  // -------------------------\n",
    "  def readChangesBatch(up: Upstream, fromVersionExclusive: Long): DataFrame = {\n",
    "    // readChangeData true supports .option(\"startingVersion\", v) or we can filter by _commit_version > v\n",
    "    // We'll read the change feed from beginning (or use starting version) and filter by > fromVersionExclusive\n",
    "    val dfRaw = spark.read\n",
    "      .format(\"delta\")\n",
    "      .option(\"readChangeData\", \"true\")\n",
    "      .load(up.tablePath)\n",
    "\n",
    "    // Some environments might require explicitly filtering _commit_version > fromVersionExclusive\n",
    "    val df = dfRaw\n",
    "      .filter(col(\"_change_type\").isin(\"insert\", \"update\", \"delete\"))\n",
    "      .filter(col(\"_commit_version\") > lit(fromVersionExclusive))\n",
    "      .withColumn(\"upstream_table\", lit(up.tablePath))\n",
    "      .withColumnRenamed(up.keyCol, up.keyCol) // keep original name but it's available in the view\n",
    "\n",
    "    df\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Aggregation & merge helper for link table\n",
    "  // Input: DataFrame expected columns -> entity_key, last_commit_version, last_change_ts, upstream_tables (array<string>)\n",
    "  // -------------------------\n",
    "  def writeLinkTableWithMerge(pipeline: PipelineConfig, df: DataFrame): Unit = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    // Ensure the output table exists; if not create with df.schema\n",
    "    if (!io.delta.tables.DeltaTable.isDeltaTable(spark, pipeline.linkTablePath)) {\n",
    "      df.write.format(\"delta\").mode(\"overwrite\").save(pipeline.linkTablePath)\n",
    "      println(s\"Created link table at ${pipeline.linkTablePath}\")\n",
    "      return\n",
    "    }\n",
    "\n",
    "    val deltaT = DeltaTable.forPath(spark, pipeline.linkTablePath)\n",
    "    // Merge by pipeline + entity_key; pipelineName stored in df or we add it\n",
    "    val src = df.withColumn(\"pipeline_name\", lit(pipeline.pipelineName)).alias(\"s\")\n",
    "    deltaT.as(\"t\")\n",
    "      .merge(\n",
    "        src,\n",
    "        \"t.pipeline_name = s.pipeline_name AND t.entity_key = s.entity_key\"\n",
    "      )\n",
    "      .whenMatched(\"s.last_commit_version > t.last_commit_version\")\n",
    "      .updateExpr(Map(\n",
    "        \"last_commit_version\" -> \"s.last_commit_version\",\n",
    "        \"last_change_ts\" -> \"s.last_change_ts\",\n",
    "        \"upstream_tables\" -> \"s.upstream_tables\",\n",
    "        \"updated_at\" -> \"current_timestamp()\"\n",
    "      ))\n",
    "      .whenNotMatched()\n",
    "      .insertExpr(Map(\n",
    "        \"pipeline_name\" -> \"s.pipeline_name\",\n",
    "        \"entity_key\" -> \"s.entity_key\",\n",
    "        \"last_commit_version\" -> \"s.last_commit_version\",\n",
    "        \"last_change_ts\" -> \"s.last_change_ts\",\n",
    "        \"upstream_tables\" -> \"s.upstream_tables\",\n",
    "        \"created_at\" -> \"current_timestamp()\",\n",
    "        \"updated_at\" -> \"current_timestamp()\"\n",
    "      ))\n",
    "      .execute()\n",
    "    println(s\"MERGE completed for pipeline ${pipeline.pipelineName} into ${pipeline.linkTablePath}\")\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Run batch processing for one pipeline (supports joinSql or union)\n",
    "  // -------------------------\n",
    "  def runPipelineBatch(pipeline: PipelineConfig, trackerPath: String = trackerPathDefault): Unit = {\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    // 1) Read last processed versions for each upstream\n",
    "    val lastVersions: Map[String, Long] = pipeline.upstreams.map { up =>\n",
    "      up.alias -> getLastProcessedVersion(up.tablePath, trackerPath)\n",
    "    }.toMap\n",
    "\n",
    "    // 2) Find latest versions for upstream tables (from table history) to decide upper bound\n",
    "    // We can find latest version via DeltaTable.forPath(...).history(1) or using snapshot metadata\n",
    "    val latestVersions: Map[String, Long] = pipeline.upstreams.map { up =>\n",
    "      val dt = io.delta.tables.DeltaTable.forPath(spark, up.tablePath)\n",
    "      val latest = dt.history(1).select(\"version\").as[Long].collect().headOption.getOrElse(0L)\n",
    "      up.alias -> latest\n",
    "    }.toMap\n",
    "\n",
    "    // 3) For each upstream, read changes > lastProcessedVersion\n",
    "    // Register as temp views with their alias name so joinSql can reference them.\n",
    "    pipeline.upstreams.foreach { up =>\n",
    "      val fromV = lastVersions(up.alias)\n",
    "      val df = readChangesBatch(up, fromV)\n",
    "      // Register temp view for SQL joins. The view keeps the original key column name.\n",
    "      df.createOrReplaceTempView(up.alias)\n",
    "    }\n",
    "\n",
    "    // 4) Build final DF\n",
    "    val finalDF: DataFrame = pipeline.joinSql match {\n",
    "      case Some(sql) =>\n",
    "        // User-provided SQL must return columns: entity_key, last_commit_version, last_change_ts, upstream_tables (array<string>)\n",
    "        // We run it directly.\n",
    "        val res = spark.sql(sql)\n",
    "        // Validation for expected columns (light)\n",
    "        val cols = res.columns.toSet\n",
    "        require(cols.contains(\"entity_key\") && cols.contains(\"last_commit_version\") && cols.contains(\"last_change_ts\"),\n",
    "          s\"joinSql must return entity_key, last_commit_version, last_change_ts. got: ${cols.mkString(\",\")}\")\n",
    "        // if upstream_tables is missing, try to create it from constants in SQL or fallback\n",
    "        val withUpstreams = if (cols.contains(\"upstream_tables\")) res\n",
    "                            else res.withColumn(\"upstream_tables\", array(lit(pipeline.upstreams.map(_.tablePath): _*)))\n",
    "        withUpstreams\n",
    "      case None =>\n",
    "        // Simple union of distinct keys: we expect each upstream temp view to expose the key column provided in Upstream.keyCol.\n",
    "        // We'll select each upstream's key as entity_key, commit_version, commit_ts, upstream_table then union.\n",
    "        val perUp = pipeline.upstreams.map { up =>\n",
    "          // select key as entity_key\n",
    "          val colExpr = col(up.keyCol).as(\"entity_key\")\n",
    "          spark.table(up.alias)\n",
    "            .select(colExpr, col(\"_commit_version\"), col(\"_commit_timestamp\"))\n",
    "            .withColumn(\"upstream_tables\", array(lit(up.tablePath)))\n",
    "        }\n",
    "        val unioned = perUp.reduce(_.unionByName(_, allowMissingColumns = true))\n",
    "        // collapse to latest commit per entity_key\n",
    "        unioned.groupBy(\"entity_key\")\n",
    "          .agg(\n",
    "            max(\"_commit_version\").as(\"last_commit_version\"),\n",
    "            max(\"_commit_timestamp\").as(\"last_change_ts\"),\n",
    "            flatten(collect_set(\"upstream_tables\")).as(\"upstream_tables\")\n",
    "          )\n",
    "    }\n",
    "\n",
    "    // Add pipeline_name if not present\n",
    "    val finalWithPipeline = if (finalDF.columns.contains(\"pipeline_name\")) finalDF else finalDF.withColumn(\"pipeline_name\", lit(pipeline.pipelineName))\n",
    "\n",
    "    // Normalize columns\n",
    "    val normalized = finalWithPipeline\n",
    "      .selectExpr(\"pipeline_name\", \"entity_key\", \"cast(last_commit_version as long) as last_commit_version\", \"cast(last_change_ts as timestamp) as last_change_ts\", \"upstream_tables\")\n",
    "      .withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "    // 5) Write/MERGE into link table\n",
    "    writeLinkTableWithMerge(pipeline, normalized)\n",
    "\n",
    "    // 6) Update tracker: set last_processed_version to latestVersions per upstream\n",
    "    val updates = pipeline.upstreams.map { up =>\n",
    "      val latestV = latestVersions(up.alias)\n",
    "      (up.tablePath, latestV, new java.sql.Timestamp(System.currentTimeMillis()))\n",
    "    }\n",
    "    updateTracker(updates, trackerPath)\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Run batch for many pipelines\n",
    "  // -------------------------\n",
    "  def runAllPipelinesBatch(pipelines: Seq[PipelineConfig], trackerPath: String = trackerPathDefault): Unit = {\n",
    "    // ensure tracker exists\n",
    "    initTracker(trackerPath)\n",
    "    pipelines.foreach { p =>\n",
    "      println(s\"===== Running batch for pipeline: ${p.pipelineName} =====\")\n",
    "      try {\n",
    "        runPipelineBatch(p, trackerPath)\n",
    "      } catch {\n",
    "        case e: Throwable =>\n",
    "          println(s\"ERROR running pipeline ${p.pipelineName}: ${e.getMessage}\")\n",
    "          e.printStackTrace()\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Streaming (continuous) - union-only mode\n",
    "  // Note: join-based streaming is non-trivial (stateful multi-stream join) and is not implemented here.\n",
    "  // If you need streaming joins, consider using Structured Streaming with event-time watermarks and careful\n",
    "  // windowing, or use DLT which simplifies streaming joins. For now streaming supports UNION mode for each upstream.\n",
    "  // -------------------------\n",
    "  def runPipelineStreamingUnion(pipeline: PipelineConfig): Unit = {\n",
    "    require(pipeline.joinSql.isEmpty, \"Streaming union runner supports only pipelines with no joinSql (simple union).\")\n",
    "\n",
    "    val spark = this.spark\n",
    "    import spark.implicits._\n",
    "\n",
    "    // Build streaming union of upstream change streams\n",
    "    val changeStreams = pipeline.upstreams.map { up =>\n",
    "      spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"readChangeData\", \"true\")\n",
    "        .load(up.tablePath)\n",
    "        .filter(col(\"_change_type\").isin(\"insert\", \"update\", \"delete\"))\n",
    "        .select(col(up.keyCol).as(\"entity_key\"), col(\"_commit_version\"), col(\"_commit_timestamp\"))\n",
    "        .withColumn(\"upstream_tables\", array(lit(up.tablePath)))\n",
    "    }\n",
    "\n",
    "    val unioned = changeStreams.reduce(_.unionByName(_, allowMissingColumns = true))\n",
    "\n",
    "    val deduped = unioned\n",
    "      .withWatermark(\"_commit_timestamp\", \"1 day\")\n",
    "      .groupBy(\"entity_key\")\n",
    "      .agg(\n",
    "        max(\"_commit_version\").as(\"last_commit_version\"),\n",
    "        max(\"_commit_timestamp\").as(\"last_change_ts\"),\n",
    "        flatten(collect_set(\"upstream_tables\")).as(\"upstream_tables\")\n",
    "      )\n",
    "      .withColumn(\"pipeline_name\", lit(pipeline.pipelineName))\n",
    "      .withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "    val checkpoint = pipeline.checkpointPath.getOrElse(s\"/mnt/checkpoints/delta_link_${pipeline.pipelineName}\")\n",
    "\n",
    "    val query = deduped.writeStream\n",
    "      .format(\"delta\")\n",
    "      .outputMode(\"update\")\n",
    "      .option(\"checkpointLocation\", checkpoint)\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .trigger(Trigger.ProcessingTime(\"5 minutes\"))\n",
    "      .start(pipeline.linkTablePath)\n",
    "\n",
    "    println(s\"Started streaming query for pipeline ${pipeline.pipelineName}; writing to ${pipeline.linkTablePath} with checkpoint $checkpoint\")\n",
    "    query.awaitTermination()\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Example: helper that constructs a simple JOIN SQL for two upstreams.\n",
    "  // If you prefer to supply custom SQL, skip this and provide joinSql in config.\n",
    "  // Example usage shown below in mainExample.\n",
    "  // -------------------------\n",
    "  def buildTwoTableJoinSql(aAlias: String, aKey: String, bAlias: String, bKey: String, downstreamKeyExpr: String): String = {\n",
    "    // downstreamKeyExpr is an expression using aliases a and b (e.g. \"coalesce(a.some_fk, b.other_fk) as entity_key\")\n",
    "    s\"\"\"\n",
    "      SELECT\n",
    "        ${downstreamKeyExpr},\n",
    "        greatest(a._commit_version, b._commit_version) as last_commit_version,\n",
    "        greatest(a._commit_timestamp, b._commit_timestamp) as last_change_ts,\n",
    "        array('${aAlias}', '${bAlias}') as upstream_tables\n",
    "      FROM $aAlias a\n",
    "      FULL OUTER JOIN $bAlias b\n",
    "        ON <PUT_JOIN_CONDITION_HERE>\n",
    "      -- NOTE: Replace <PUT_JOIN_CONDITION_HERE> with actual join condition, e.g., a.x = b.x\n",
    "    \"\"\"\n",
    "  }\n",
    "\n",
    "  // -------------------------\n",
    "  // Example main showing usage\n",
    "  // -------------------------\n",
    "  def mainExample(): Unit = {\n",
    "    // Example pipelines\n",
    "    val pipeline1 = PipelineConfig(\n",
    "      pipelineName = \"pipelineX\",\n",
    "      upstreams = Seq(\n",
    "        Upstream(\"a\", \"/mnt/delta/upstream_A\", \"a_id\"),\n",
    "        Upstream(\"b\", \"/mnt/delta/upstream_B\", \"b_id\")\n",
    "      ),\n",
    "      // Provide a joinSql that references temp views 'a' and 'b'\n",
    "      // The minimal required output columns are: entity_key, last_commit_version, last_change_ts, upstream_tables\n",
    "      joinSql = Some(\n",
    "        \"\"\"\n",
    "        -- Example: join where downstream entity_key is coalesced foreign key\n",
    "        SELECT\n",
    "          coalesce(a.some_fk, b.some_fk) as entity_key,\n",
    "          greatest(a._commit_version, b._commit_version) as last_commit_version,\n",
    "          greatest(a._commit_timestamp, b._commit_timestamp) as last_change_ts,\n",
    "          array(a.upstream_table, b.upstream_table) as upstream_tables\n",
    "        FROM a\n",
    "        FULL OUTER JOIN b\n",
    "          ON a.some_fk = b.some_fk\n",
    "        \"\"\"\n",
    "      ),\n",
    "      linkTablePath = \"/mnt/delta/delta_link_pipelineX\",\n",
    "      checkpointPath = Some(\"/mnt/checkpoints/delta_link_pipelineX\")\n",
    "    )\n",
    "\n",
    "    val pipeline2 = PipelineConfig(\n",
    "      pipelineName = \"pipelineY\",\n",
    "      upstreams = Seq(\n",
    "        Upstream(\"c\", \"/mnt/delta/upstream_C\", \"c_id\")\n",
    "      ),\n",
    "      joinSql = None,\n",
    "      linkTablePath = \"/mnt/delta/delta_link_pipelineY\",\n",
    "      checkpointPath = Some(\"/mnt/checkpoints/delta_link_pipelineY\")\n",
    "    )\n",
    "\n",
    "    // initialize tracker\n",
    "    initTracker()\n",
    "\n",
    "    // run batch for all pipelines\n",
    "    runAllPipelinesBatch(Seq(pipeline1, pipeline2))\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e931444",
   "metadata": {},
   "source": [
    "#### 2.  Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f60b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
