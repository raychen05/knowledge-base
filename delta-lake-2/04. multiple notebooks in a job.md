
### Running Multiple Notebooks in a Databricks Job with Dependencies


To efficiently orchestrate multiple Databricks notebooks within a single job, ensuring some run in parallel and others sequentially, follow these steps:

--- 

📌 1. Approach for Managing Workflow Sequence

-	Parallel Execution: Notebooks that don’t depend on each other can run in parallel.
-	Sequential Execution: If a notebook’s output feeds into another, it must be executed after the dependent notebook.
-	Triggering Logic: Use dbutils.notebook.run() for sequential runs & Jobs API for parallel execution.


---

📌 2. Configuring Job Workflow in Databricks UI


Databricks Workflows (Jobs) allow defining dependencies:
1.	Go to Workflows > Create Job.
2.	Add tasks (notebooks) and define dependencies.
3.	Set some tasks to run in parallel and others sequentially.


---

📌 3. Example: Databricks Job Workflow Configuration

```scala
    import com.databricks.workflow._
    val job = Workflow.create("DataPipeline")

    // Step 1: Load Data (Runs First)
    val loadData = job.addTask("Load_Data", "LoadDataNotebook")

    // Step 2: Process Data (Runs after Load)
    val processData = job.addTask("Process_Data", "ProcessDataNotebook")
    processData.dependsOn(loadData)

    // Step 3: Analytics & Report (Can Run in Parallel)
    val analyticsTask = job.addTask("Run_Analytics", "AnalyticsNotebook")
    val reportTask = job.addTask("Generate_Report", "ReportNotebook")
    analyticsTask.dependsOn(processData)
    reportTask.dependsOn(processData)

    // Step 4: Publish Results (Runs Last)
    val publishTask = job.addTask("Publish_Results", "PublishNotebook")
    publishTask.dependsOn(analyticsTask)
    publishTask.dependsOn(reportTask)

    // Submit job
    job.submit()
```

---

📌 4. Running Notebooks Sequentially in Scala

When one notebook’s output is needed in another, use dbutils.notebook.run():

Example: Notebook Chaining (Sequential Execution)

```scala
// Run Load Data Notebook
val loadDataOutput = dbutils.notebook.run("LoadDataNotebook", timeoutSeconds = 3600)

// Run Process Data Notebook using Load Data output
val processDataOutput = dbutils.notebook.run("ProcessDataNotebook", timeoutSeconds = 3600, Map("inputPath" -> loadDataOutput))

// Run Analytics Notebook using Process Data output
val analyticsOutput = dbutils.notebook.run("AnalyticsNotebook", timeoutSeconds = 3600, Map("inputPath" -> processDataOutput))

// Run Report Notebook using Process Data output
val reportOutput = dbutils.notebook.run("ReportNotebook", timeoutSeconds = 3600, Map("inputPath" -> processDataOutput))

// Run Publish Results Notebook after analytics & report
val publishOutput = dbutils.notebook.run("PublishNotebook", timeoutSeconds = 3600, Map("analyticsPath" -> analyticsOutput, "reportPath" -> reportOutput))

```


---

📌 5. Running Notebooks in Parallel

For parallel execution, use Databricks multitasking:

```scala
import scala.concurrent.Future
import scala.concurrent.ExecutionContext.Implicits.global

val analyticsFuture = Future {
  dbutils.notebook.run("AnalyticsNotebook", timeoutSeconds = 3600, Map("inputPath" -> processDataOutput))
}

val reportFuture = Future {
  dbutils.notebook.run("ReportNotebook", timeoutSeconds = 3600, Map("inputPath" -> processDataOutput))
}

// Wait for both parallel tasks to finish
val results = Seq(analyticsFuture, reportFuture).map(_.recover { case e => e.getMessage })
val allResults = Future.sequence(results)
allResults.foreach(println)

```

✅ Effect: AnalyticsNotebook and ReportNotebook run in parallel after ProcessDataNotebook.


---

📌 6. Reading & Writing Data from Delta Lake & S3


Reading from Delta Lake Catalog
```scala
val df = spark.read.format("delta").table("catalog.schema.table_name")
```

Writing to Delta Lake
```scala
df.write.format("delta").mode("overwrite").saveAsTable("catalog.schema.output_table")
```

Reading from S3
```scala
val df = spark.read.format("parquet").load("s3://your-bucket/path")
```

Writing to S3
```scala
df.write.format("parquet").mode("overwrite").save("s3://your-bucket/output-path")
```

---

📌 7. Summary


|Requirement	|Approach|
|-----------------------------------|---------------------------------------|
|Parallel Execution	|Use Databricks Job UI or Scala Futures|
|Sequential Execution	|Use dbutils.notebook.run()|
|Trigger Next Task	|Pass outputs as parameters|
|Read/Write from Delta	|spark.read.table() / df.write.saveAsTable()|
|Read/Write from S3	|spark.read.load() / df.write.save()|

🚀 This approach optimizes Databricks pipeline execution for performance & dependency management! 🚀


---

### Performnce of Parallel Execution of Multiple Notebooks in Databricks Cluster

When multiple notebooks are configured to run in parallel in a Databricks job, they will share the same Databricks cluster. However, whether they actually execute in parallel depends on cluster resource allocation and task scheduling.

---

📌 1. How Does Databricks Handle Parallel Notebook Execution?

**Multiple Notebooks Assigned to the Same Cluster:**
-	If enough available executors exist, Databricks truly runs them in parallel.
-	If not enough resources, tasks may be queued and executed sequentially due to scheduling limits.

**Parallelism Depends on Cluster Size:****
-	If the cluster has sufficient workers, the notebooks can execute at the same time.
-	If the cluster is overloaded, Spark will schedule jobs sequentially.


---
📌 2. How to Ensure Efficient Parallel Execution?

To maximize parallelism and efficient resource utilization, follow these steps:

✅ Step 1: Use a Cluster with Sufficient Resources

-	Choose a larger cluster with more nodes and cores.
-	Example optimal cluster for parallel execution:
	-	Worker Type: i3.4xlarge (16 vCPUs, 122GB RAM)
	-	Number of Workers: 8+ nodes
	-	Driver Type: Same as worker (i3.4xlarge)
- Databricks automatically distributes tasks across available executors.

---

✅ Step 2: Configure Notebooks to Run in Parallel in Databricks Job UI

1.	Go to Workflows → Jobs.
2.	Click Create Job.
3.	Add multiple tasks (notebooks) and remove dependencies so they run in parallel.
4.	Set the same shared cluster for all tasks.

✔️ This ensures tasks start at the same time, using shared cluster resources efficiently.


---

✅ Step 3: Manually Trigger Parallel Execution in Scala

You can trigger multiple notebooks in parallel using Futures in Scala:
```scala
import scala.concurrent.Future
import scala.concurrent.ExecutionContext.Implicits.global

// Run multiple notebooks in parallel
val future1 = Future { dbutils.notebook.run("Notebook_A", 3600) }
val future2 = Future { dbutils.notebook.run("Notebook_B", 3600) }
val future3 = Future { dbutils.notebook.run("Notebook_C", 3600) }

// Wait for all tasks to complete
val results = Seq(future1, future2, future3).map(_.recover { case e => e.getMessage })
val allResults = Future.sequence(results)
allResults.foreach(println)

```

✔️ Effect: The three notebooks (Notebook_A, Notebook_B, Notebook_C) execute truly in parallel if the cluster has enough resources.

---

✅ Step 4: Optimize Spark Configuration for Parallel Execution

Modify Spark settings in spark.conf to allow better parallelism:

```scala
spark.conf.set("spark.databricks.io.cache.enabled", "true")
spark.conf.set("spark.sql.shuffle.partitions", "200")  // Increase for parallelism
spark.conf.set("spark.default.parallelism", "200")
spark.conf.set("spark.task.cpus", "2") // Allows more tasks to run concurrently

```

---
📌 3. Key Takeaways

|Factor|	Impact on Parallel Execution|
|-----------------------------------|---------------------------------------|
|Cluster Size|	More nodes → More parallel execution|
|Executor Availability|	If full, tasks queue and run sequentially|
|Job UI Setup|	Removing dependencies allows parallel execution|
|Scala Futures|	Forces multiple notebooks to start at once|
|Spark Config Optimization|	Ensures better task distribution|


---

🚀 Conclusion

- ✅ Databricks allows true parallel execution if the cluster has sufficient resources.
- ✅ Optimizing cluster size, job dependencies, and Spark configurations ensures efficient parallel execution.
- ✅ If the cluster is overloaded, Spark automatically queues tasks, causing sequential execution even if the job is configured for parallelism.


🔹 Best Practice: If multiple notebooks need to run in parallel, use a large enough cluster and set Spark configurations correctly. 🚀