
### Databrick Scala Spark ES loader


Efficiently loading 80 million records from Databricks to a 40-node Elasticsearch cluster requires careful tuning of Spark configurations, Elasticsearch bulk ingestion settings, and Databricks cluster sizing. Below is an optimized Scala Spark solution with performance tuning.

---
#### 1. Optimized Scala Spark Code


```scala
import org.apache.spark.sql.SparkSession
import org.elasticsearch.spark.sql._

val spark = SparkSession.builder()
  .appName("DatabricksToElasticsearch")
  .config("spark.sql.shuffle.partitions", "400")  // Increase partitions for parallelism
  .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")  // Efficient serialization
  .config("es.batch.size.entries", "10000")  // Bulk indexing size
  .config("es.batch.size.bytes", "10mb")  // Limit request size per batch
  .config("es.batch.write.refresh", "false")  // Disable auto-refresh for performance
  .config("es.index.auto.create", "false")  // Avoid index auto-creation overhead
  .config("es.nodes.wan.only", "false")  // Use direct node communication
  .config("es.nodes", "es-cluster-endpoint")  // Replace with ES cluster
  .config("es.port", "9200")  // Elasticsearch port
  .getOrCreate()

// Load DataFrame (Example: JSON format in Data Lake)
val df = spark.read.json("s3://your-bucket/path-to-json")

// Optimize partitions for better parallelism
val numPartitions = 400  // Adjust based on ES cluster size and data volume
val partitionedDF = df.repartition(numPartitions)

// Define index and type
val esIndex = "your_index_name/_doc"

// Write data to Elasticsearch
partitionedDF.write
  .format("org.elasticsearch.spark.sql")
  .option("es.resource", esIndex)
  .option("es.nodes", "es-cluster-endpoint")
  .option("es.port", "9200")
  .option("es.batch.write.retry.count", "10")  // Retry failures
  .option("es.batch.write.retry.wait", "10s")  // Wait between retries
  .option("es.mapping.id", "document_id")  // Use a field as ID to avoid duplicates
  .save()

spark.stop()
```
---

#### 2.  Optimized Spark Configuration in Databricks

Modify these Spark configs in Databricks Cluster Settings:

```ini
spark.dynamicAllocation.enabled       true
spark.dynamicAllocation.minExecutors  20
spark.dynamicAllocation.maxExecutors  80
spark.executor.memory                 16g
spark.executor.cores                  4
spark.driver.memory                   8g
spark.driver.cores                    4
spark.sql.shuffle.partitions          400
spark.serializer                      org.apache.spark.serializer.KryoSerializer
```

---

#### 3. Recommended Databricks Cluster Sizing


| Cluster Type	| Recommended Specs| 
|-----------------------------------|-------------------------------------|
| Workers	| 40-60 nodes (depending on data size & indexing speed)| 
| Instance Type	| i3.4xlarge (16 vCPUs, 122GB RAM) (optimized for IO-heavy tasks)| 
| Executors	| At least 40-80 Executors (to match ES nodes)| 
| Cores/Executor| 	4| 
| Memory/Executor| 	16-32GB| 

---

#### 4.  Elasticsearch Index Tuning for High-Speed Ingestion


1.	Disable Refresh

```sh
PUT your_index_name/_settings
{
  "index": {
    "refresh_interval": "-1",
    "number_of_replicas": 0
  }
}
```

2.	Increase Bulk Thread Pool in ES (elasticsearch.yml)

```ini
    thread_pool.bulk.queue_size: 500
```

3.	Set Proper Sharding Strategy

```sh
PUT your_index_name
{
  "settings": {
    "number_of_shards": 40,
    "number_of_replicas": 1
  }
}
```

4.	Re-enable Refresh & Replicas After Ingestion

```sh
PUT your_index_name/_settings
{
  "index": {
    "refresh_interval": "1s",
    "number_of_replicas": 1
  }
}
```

**Expected Performance**

- Indexing Speed: 5k-15k records/sec per node
- Estimated Time: 80 million records ‚âà 1-2 hours


**Summary:**

- Use Databricks Dynamic Allocation to scale resources.
- Optimize Spark partitions & ES bulk indexing settings.
- Disable refresh & replicas during indexing for speed.
- Ensure Databricks cluster matches Elasticsearch node count for efficiency.


---

### Advanced Tuning for High-Speed Elasticsearch Bulk Ingestion

Since you‚Äôre dealing with 80 million records and a 40-node Elasticsearch cluster, further optimizations can help improve throughput. Below are additional performance tuning techniques for your Databricks-to-Elasticsearch pipeline.

---

#### 1. Advanced Spark Tuning in Databricks

Modify these Spark configs in Databricks Cluster Settings to maximize throughput:

```ini
spark.dynamicAllocation.enabled       true
spark.dynamicAllocation.minExecutors  40  # Match ES nodes
spark.dynamicAllocation.maxExecutors  80  # Scale up dynamically
spark.executor.instances              40  # Keep in sync with ES nodes
spark.executor.memory                 32g  # More memory per executor
spark.executor.cores                  4  # Keep parallelism manageable
spark.driver.memory                   16g
spark.driver.cores                    4
spark.sql.shuffle.partitions          800  # Increase partitions for parallelism
spark.serializer                      org.apache.spark.serializer.KryoSerializer
spark.rdd.compress                    true
spark.default.parallelism             400  # Keep consistent with partitions
```

**Why?**

- More executors = More parallel writes to Elasticsearch.
- Higher memory = Handles large JSON payloads efficiently.
- Shuffle partitions at 800 = Better parallel data distribution.

---

#### 2.  Adjusting Elasticsearch Bulk Indexing Settings

Modify these Elasticsearch cluster settings to improve bulk indexing:

---
1Ô∏è‚É£ Increase Bulk Thread Pool for Higher Write Throughput

Update Elasticsearch elasticsearch.yml config:

```ini
thread_pool.bulk.size: 40  # One thread per ES node
thread_pool.bulk.queue_size: 2000  # Increase queue size to handle more incoming bulk requests
indices.memory.index_buffer_size: 30%  # Increase buffer size for bulk indexing
```

**Why?**

- 40 bulk threads = Matches 40 ES nodes.
- Higher queue size = Prevents request rejection.
- Larger index buffer = More data is processed in memory before flush.

---
2Ô∏è‚É£ Increase Bulk Request Size for Efficient Writes

Modify Elasticsearch write configurations in Spark:

```ini
.config("es.batch.size.entries", "25000")  // Number of records per batch
.config("es.batch.size.bytes", "50mb")  // Bulk request size
.config("es.batch.write.retry.count", "15")  // Increase retries for robustness
.config("es.batch.write.retry.wait", "30s")  // Increase wait time between retries
.config("es.mapping.id", "doc_id")  // Use document ID for upserts
```

 **Why?**
- Larger batch size (25k records, 50MB) = Fewer bulk requests.
- More retries (15x, 30s delay) = Helps handle transient failures.

---
3Ô∏è‚É£ Use routing to Improve Indexing Efficiency

```ini
.config("es.mapping.routing", "routing_key")  // Route data efficiently
```

 **Why?**
- Ensures related documents go to the same shard = Faster lookups & writes.

---
#### 3. Elasticsearch Index Design for Speed

--
1Ô∏è‚É£ Optimize Number of Shards & Replicas

```sh
PUT your_index_name
{
  "settings": {
    "number_of_shards": 40,  # Match ES node count
    "number_of_replicas": 1,  # Keep low during ingestion
    "refresh_interval": "-1"  # Disable refresh for bulk inserts
  }
}
```

üìå Why?
- 40 shards = 1 shard per ES node for maximum parallel indexing.
- 0 replicas during ingestion = Avoids unnecessary duplication.

---
2Ô∏è‚É£ Use Faster Storage for High-Volume Writes

Ensure Elasticsearch cluster uses:
- NVMe SSDs (e.g., i3.4xlarge instances for best IO performance).
- Dedicated Master Nodes (to prevent ingestion overhead).

---
üîπ Best Databricks Cluster Sizing for 80M Records


|Cluster Component	| Recommended Specs|
|--------------------|-------------------|
|Worker Nodes	|40-60 nodes (to match ES cluster)|
|Instance Type	|i3.4xlarge (16 vCPUs, 122GB RAM, NVMe SSDs)|
|Executors per Node	| 1-2|
|Total Executors|	40-80|
|Memory per Executor|	16-32GB|
|Cores per Executor|	4|
|Parallelism	|400-800 partitions|


üìå Why?
- Match Databricks executors with ES nodes (40:40).
- Use i3.4xlarge for high disk IO speed.
- Keep memory per executor high (16-32GB).

---
üîπ Final Performance Estimates


|Optimization	| Estimated Impact|
|--------------------|-------------------|
|Parallel Writes (400-800 partitions) |	3x Faster Ingestion|
|Higher Bulk Size (25k docs, 50MB) |	2x Fewer Bulk Requests|
|Increased Thread Pool (40 threads)	|40x Faster Processing per Node|
|Zero Replicas During Ingestion|	30-50% Faster Indexing|
|NVMe SSD for Elasticsearch Nodes	|50-100% Faster Writes|


üöÄ Final Estimated Ingestion Time:
- 80 million records @ 15k docs/sec ‚Üí ~1.5 Hours

---

üîπ Post-Ingestion Cleanup

```sh
PUT your_index_name/_settings
{
  "index": {
    "refresh_interval": "1s",
    "number_of_replicas": 1
  }
}
```

üìå Why?
- Ensures fast search performance after ingestion.
- Enables replication for redundancy.


---

üìå Summary of Key Optimizations

‚úÖ Databricks Cluster: 40-60 Nodes, i3.4xlarge, 16GB+ Memory/Executor, 400+ Partitions

‚úÖ Elasticsearch Tuning: 40 shards, 40 bulk threads, NVMe SSD, no replicas during indexing

‚úÖ Bulk Ingestion Config: 25k records, 50MB per request, retry handling

‚úÖ Final Load Time: ~1.5 hours for 80M records


---
###  Optimizing Elasticsearch Bulk Indexing for a 40-Node Cluster


The number of indexing request threads per node in an Elasticsearch (ES) cluster depends on:

1.	CPU cores per node
2.	Elasticsearch bulk thread pool size
3.	Thread contention & merge pressure


üîπ Maximum Bulk Request Threads per Node


Your ES nodes are i3.4xlarge instances, which have:
- 16 vCPUs
- 122 GB RAM
- NVMe SSD storage (high throughput)


üîπ Default Bulk Thread Pool Per Node

By default, Elasticsearch uses this formula to determine the number of bulk threads per node:

bulk.thread_pool.size = min(32, vCPUs / 2)

For i3.4xlarge (16 vCPUs):

min(32, 16 / 2) = 8  threads

Thus, each node can process up to 8 concurrent bulk requests by default.


---

üîπ Optimizing Bulk Thread Pool for Maximum Throughput

Increase Bulk Thread Pool

You can manually increase the bulk thread pool up to 4 threads per vCPU:

```sh
PUT _cluster/settings
{
  "persistent": {
    "thread_pool.bulk.size": 32,    # 2x vCPUs (16*2)
    "thread_pool.bulk.queue_size": 8000  # Prevent request rejection
  }
}
```

üìå Effect:
- ‚úÖ Increases indexing concurrency per node
- ‚úÖ Allows up to 32 parallel indexing threads per node
- ‚úÖ Helps avoid bottlenecks when scaling ingestion

---

üîπ How Many Indexing Request Threads Can Be Processed in a 40-Node Cluster?


|Thread Pool Per Node	| Total Threads in 40 Nodes|
|--------------------|-------------------|
|8 (default)	| 320 threads |
16 (1 per vCPU)	|640 threads|
|32 (2 per vCPU)	|1280 threads|

---

üîπ Summary
-	Default max: 8 bulk request threads per node (320 for 40 nodes).
-	Optimized max: 16‚Äì32 bulk request threads per node (640‚Äì1280 for 40 nodes).
-	Start with 16 per node (640 total), increase if CPU is underutilized.


---

### Summary of ES Loader Optimization Appraoches


üìå 1. Recommended Indexing Request Threads Per Node

‚úÖ Optimized setting: Increase bulk thread pool to 16‚Äì32 per node.

üìå Recommended setting: Start with 16 threads per node (640 total) and scale up to 32 (1280 total) if CPU allows.

---

üìå 2. Bulk Request Configuration


```sh
PUT _cluster/settings
{
  "persistent": {
    "thread_pool.bulk.size": 32,          # Max 2x vCPUs per node
    "thread_pool.bulk.queue_size": 10000, # Prevent request rejection
    "indices.memory.index_buffer_size": "30%", # Increase memory for indexing
    "indices.store.throttle.type": "none"  # Remove store throttling
  }
}
```

‚úÖ Effect:
- ‚úî Increases indexing concurrency
- ‚úî Avoids bulk rejections
- ‚úî Reduces disk I/O throttling

---
üìå 3. Optimized Bulk Loading in Databricks

üî• Spark Configuration for ES
```python
spark.conf.set("es.batch.size.entries", "50000")  # Bulk size per request
spark.conf.set("es.batch.write.refresh", "false")  # Disable refresh during load
spark.conf.set("es.index.auto.create", "true")  # Auto-create index if needed
spark.conf.set("es.nodes", "your-es-cluster")  # Elasticsearch nodes
spark.conf.set("es.port", "9200")
spark.conf.set("es.nodes.wan.only", "true")  # Required for WAN mode
```

üöÄ Optimized Spark Write Code
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ES Loader").getOrCreate()

# Load DataFrame from Delta or Parquet
df = spark.read.format("delta").load("s3://your-bucket/path")

# Write to Elasticsearch efficiently
(df.write.format("org.elasticsearch.spark.sql")
   .option("es.resource", "your-index/_doc")
   .option("es.nodes", "your-es-cluster")
   .option("es.batch.size.bytes", "10mb")  # Bulk size
   .option("es.batch.size.entries", "50000")  # Docs per batch
   .option("es.write.operation", "index")
   .option("es.batch.write.retry.count", "5")  # Retry on failure
   .option("es.mapping.id", "your_id_column")  # Optional ID mapping
   .mode("append")
   .save()
)
```

‚úÖ Effect:
- ‚úî 50,000 docs per batch minimizes network overhead
- ‚úî 10MB batch size prevents memory issues
- ‚úî Disabled refresh speeds up bulk ingest

---
üìå 4. How Many Databricks Worker Nodes Are Needed?

|Databricks Worker Size	|vCPUs per Node	|Nodes Needed for 1-Hour Load (80M Docs)|
|--------------------|-------------------|-------------------|
|i3.4xlarge	|16 vCPUs	|~20 nodes|
|i3.8xlarge	|32 vCPUs	|~10 nodes|
|i3.16xlarge	|64 vCPUs|	|~5 nodes|

üìå Recommended:
-	10 i3.8xlarge nodes (~32 vCPUs each) to load 80M docs in ~1 hour.
-	Scale up/down based on CPU utilization.

---

üìå 5. Optimized Bulk Request Parallelism

To further maximize ES indexing throughput, split bulk requests across threads.

üîπ Increase Parallel Bulk Requests

Instead of 1 bulk request per node, run multiple per node:

```python
bulk_threads_per_node = 5  # 5 bulk requests per node
total_bulk_threads = 40 * bulk_threads_per_node  # 200 parallel bulk requests
```

üìå Result: 40 nodes √ó 5 threads = 200 parallel bulk requests.

---
üöÄ Summary


|Optimization Area	|Recommendation|
|--------------------|-------------------|
|Bulk Thread Pool	|16‚Äì32 per node (640‚Äì1280 total)|
|Bulk Size	|50,000 docs / 10MB per request|
|Queue Size	|10,000|
|Databricks Cluster	|10 i3.8xlarge (or 20 i3.4xlarge)|
|Bulk Parallelism	|5 bulk requests per node (200 total)|


üöÄ Following these optimizations, you can load 80M records in ~1 hour instead of 5! üöÄ