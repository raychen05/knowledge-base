### Delta Lake  View


Great question üëå ‚Äî yes, in Delta Lake with Spark (Scala), you can easily create views (either temporary or permanent) by joining / combining multiple Delta tables and then registering that combined DataFrame as a view.

Below I‚Äôll show you step-by-step Scala Spark examples for both temporary views (session-scoped) and permanent views (metastore / Hive-managed), using multiple Delta tables. üëá


---


### üß† Scenario

Suppose you have 2 Delta tables:
- 	/delta/customers
- 	/delta/orders

We want to create a view called customer_orders_view that joins customer info with their orders.

---

### 1Ô∏è‚É£ Load Multiple Delta Tables as DataFrames

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame

val spark = SparkSession.builder()
  .appName("Delta View Example")
  .getOrCreate()

// Load Delta tables as DataFrames
val customersDF: DataFrame = spark.read.format("delta").load("/delta/customers")
val ordersDF: DataFrame    = spark.read.format("delta").load("/delta/orders")
```

---

### 2Ô∏è‚É£ Transform / Join to Build Combined DataFrame

For example, join on customer_id:

```scala
import org.apache.spark.sql.functions._

val joinedDF = customersDF
  .join(ordersDF, Seq("customer_id"), "inner")
  .select(
    col("customer_id"),
    col("name"),
    col("email"),
    col("order_id"),
    col("order_date"),
    col("amount")
  )
```

---

### 3Ô∏è‚É£ Create a TEMPORARY View (Session-Scoped)

You can register the joined DataFrame as a temporary view, which is available only in the current Spark session:

```sql
joinedDF.createOrReplaceTempView("customer_orders_view")
```

‚úÖ Now you can query it like a table using SQL:

```scala
val result = spark.sql("""
  SELECT customer_id, name, SUM(amount) as total_spent
  FROM customer_orders_view
  GROUP BY customer_id, name
""")

result.show()

```

Temporary views are not stored in the metastore, and they disappear when the Spark session ends.


---

### 4Ô∏è‚É£ Create a GLOBAL TEMPORARY View (Cross-Session but Cluster-Scoped)

If you want the view to be accessible from multiple sessions in the same Spark application:

```scala
joinedDF.createOrReplaceGlobalTempView("customer_orders_view")
```

Query it using:

```scala
val globalResult = spark.sql("""
  SELECT * FROM global_temp.customer_orders_view
""")
```

Global temp views live in the global_temp database.

---

### 5Ô∏è‚É£ Create a PERMANENT View (Stored in Metastore / Hive)


If your Spark is configured with a Hive metastore (e.g., Databricks or external metastore), you can persist the view:

```scala
// First create a SQL view in the metastore
spark.sql("""
  CREATE OR REPLACE VIEW analytics.customer_orders_view AS
  SELECT c.customer_id, c.name, c.email, o.order_id, o.order_date, o.amount
  FROM delta.`/delta/customers` c
  JOIN delta.`/delta/orders` o
  ON c.customer_id = o.customer_id
""")

```

‚úÖ This creates a logical view in the analytics schema ‚Äî no data is copied.

The view definition references the underlying Delta tables, and you can query it just like a normal table:

```scala
val df = spark.sql("SELECT * FROM analytics.customer_orders_view WHERE amount > 1000")
df.show()
```

üí° You can also define views by first building the DataFrame in Scala and then calling .write.saveAsTable(viewName) with format("delta"), but that creates a physical table, not a view.


---

###  6Ô∏è‚É£ Example with More Tables (Join + Union)

You can also combine 3+ tables:

```scala
val customers = spark.read.format("delta").load("/delta/customers")
val orders = spark.read.format("delta").load("/delta/orders")
val payments = spark.read.format("delta").load("/delta/payments")

val customerOrderPaymentView = customers
  .join(orders, "customer_id")
  .join(payments, "order_id")
  .select("customer_id", "name", "order_id", "amount", "payment_status")

customerOrderPaymentView.createOrReplaceTempView("customer_order_payment_view")

```

Now you can query it:

```scala
spark.sql("""
  SELECT customer_id, name, COUNT(*) as total_orders, SUM(amount) as total_amount
  FROM customer_order_payment_view
  WHERE payment_status = 'Completed'
  GROUP BY customer_id, name
""").show()

```

---

üìå Summary Table



| View Type         | Scope            | Persisted in Metastore | Use Case                   |
|-------------------|------------------|------------------------|----------------------------|
| Temp View         | Current Session  | ‚ùå No                  | Quick ad hoc SQL           |
| Global Temp View  | Cluster-wide     | ‚ùå No                  | Shared within cluster      |
| Permanent View    | Metastore        | ‚úÖ Yes                 | Reusable, production-grade |


‚ö†Ô∏è Best Practices

- 	If the view will be reused in many jobs or tools (e.g., BI dashboards), use permanent metastore views.
- 	If it‚Äôs just intermediate processing within the job, temporary views are faster and simpler.
- 	Make sure your underlying Delta tables are stable ‚Äî permanent views are logical wrappers around them.


‚úÖ TL;DR

üëâ To create a view from multiple Delta tables in Scala Spark:

- 	Load multiple tables as DataFrames.
- 	Join or union as needed.
- 	Register the result as a view (temp, global_temp, or metastore permanent view).


---

##  Delta Lake Materialized View


Excellent üëå ‚Äî let‚Äôs now go through how to materialize a view built from multiple Delta tables into a new Delta table (i.e., a derived / summary table) using Scala + Spark.

This is a very common pattern for:
- 	building data marts / aggregate tables,
- 	speeding up downstream queries, and
- 	avoiding repeatedly computing heavy joins or aggregations.

---

### üß† Scenario

We have 3 source Delta tables:
- 	/delta/customers
- 	/delta/orders
- 	/delta/payments

We want to:
1.	Join them together.
2.	Compute some summary metrics.
3.	Materialize the result into a new Delta table: /delta/analytics/customer_summary.


---


###  1Ô∏è‚É£ Load Source Delta Tables


```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame

val spark = SparkSession.builder()
  .appName("Materialized View Example")
  .getOrCreate()

val customersDF: DataFrame = spark.read.format("delta").load("/delta/customers")
val ordersDF: DataFrame    = spark.read.format("delta").load("/delta/orders")
val paymentsDF: DataFrame  = spark.read.format("delta").load("/delta/payments")
```

---

###  2Ô∏è‚É£ Join & Aggregate

For example: get per-customer total orders and total payment amount.

```scala
import org.apache.spark.sql.functions._

val joinedDF = customersDF
  .join(ordersDF, Seq("customer_id"), "inner")
  .join(paymentsDF, Seq("order_id"), "inner")
  .groupBy("customer_id", "name", "email")
  .agg(
    countDistinct("order_id").as("total_orders"),
    sum("amount").as("total_amount"),
    max("last_payment_date").as("last_payment_date")
  )

```

This DataFrame represents your ‚Äúmaterialized view‚Äù result.

---

###  3Ô∏è‚É£ Write It as a New Delta Table

You can persist this derived dataset as a new Delta table (materialized view):

```scala
joinedDF.write
  .format("delta")
  .mode("overwrite")   // or "append" if incremental
  .option("overwriteSchema", "true")
  .save("/delta/analytics/customer_summary")
```

Optionally register it in the metastore:

```scala
spark.sql("""
  CREATE DATABASE IF NOT EXISTS analytics
""")

spark.sql("""
  DROP TABLE IF EXISTS analytics.customer_summary
""")

spark.sql("""
  CREATE TABLE analytics.customer_summary
  USING DELTA
  LOCATION '/delta/analytics/customer_summary'
""")
```

‚úÖ Now you have a real Delta table backed by physical Parquet files ‚Äî not just a logical SQL view.

It can be queried by other Spark jobs or BI tools efficiently:

```scala
val summaryDF = spark.table("analytics.customer_summary")
summaryDF.show(10)
```

---

### 4Ô∏è‚É£ Example with Incremental Updates (Optional)

If your sources update incrementally (e.g., daily), you can update this materialized view incrementally instead of full overwrite:

```scala
val incrementalUpdatesDF = ... // compute only new or changed customers

incrementalUpdatesDF.write
  .format("delta")
  .mode("overwrite")  // or MERGE for upsert
  .option("overwriteSchema", "true")
  .save("/delta/analytics/customer_summary")

```

Or use MERGE for fine-grained updates:

```scala

import io.delta.tables._

val targetTable = DeltaTable.forPath(spark, "/delta/analytics/customer_summary")

targetTable.as("t")
  .merge(
    joinedDF.as("s"),
    "t.customer_id = s.customer_id"
  )
  .whenMatchedUpdateAll()
  .whenNotMatchedInsertAll()
  .execute()

```

---

### üìå Materialized View vs Logical View


| Aspect                  | Logical View                        | Materialized Delta Table                |
|-------------------------|-------------------------------------|-----------------------------------------|
| **Storage**             | No extra storage                    | Stored as Delta table (Parquet + log)   |
| **Performance**         | Recomputed on each query            | Precomputed, fast to query              |
| **Maintenance**         | None (auto reflects changes)        | Needs refresh / overwrite               |
| **Use Case**            | Ad hoc / lightweight queries        | Heavy aggregations / stable schema      |
| **Time Travel / ACID**  | Same as base tables                 | Full Delta support                      |


‚ö° Best Practices
- 	Partition materialized tables by a suitable column (e.g., year or region) if queries filter on them.
- 	Z-ORDER by frequently queried keys to speed up downstream analytics:

```sql
OPTIMIZE analytics.customer_summary
ZORDER BY (customer_id)
```

Schedule periodic refresh jobs (e.g., daily with Databricks Workflows or Airflow).



‚úÖ TL;DR

üëâ To create a materialized view table from multiple Delta tables in Scala Spark:

1.	Read multiple Delta tables as DataFrames.
2.	Join / transform as needed.
3.	Write the result as a new Delta table (with .write.format("delta")).
4.	Optionally register in the metastore for easy querying.


---

## Regular Delta Table   vs. Materialized View



### üß± 1. Regular Delta Table

A Delta table is a physical dataset stored in Delta Lake format (on DBFS, S3, ADLS, etc.), with its own underlying Parquet files and transaction log (_delta_log).

‚úÖ Characteristics

- 	Physical storage: Data is stored in files; managed by Delta Lake.
- 	Supports full ACID (insert, update, delete, merge, time travel, schema evolution, etc.).
- 	Optimized for performance with Z-Ordering, OPTIMIZE, VACUUM, etc.
- 	Can be written to directly by Spark (batch or streaming).
- 	Ownership: You control how and when data is updated.

üìå Example

```python
spark.sql("""
  CREATE TABLE sales_delta
  USING delta
  LOCATION '/mnt/delta/sales'
  AS SELECT * FROM raw_sales
""")
```


- 	A Delta log and Parquet files are created under /mnt/delta/sales.
- 	When you query sales_delta, it scans those files directly.

---


### üß† 2. Materialized View

A materialized view in Databricks is a derived table that stores precomputed results of a SQL query, typically to improve performance.
Unlike a regular SQL view, a materialized view maintains its own Delta table underneath, but it‚Äôs automatically refreshed by Databricks.


‚úÖ Characteristics

- 	Also stored as a Delta table under the hood.
- 	You define it with a query (e.g., SELECT ... FROM ...), and Databricks manages refresh when upstream tables change.
- 	Best for aggregations, joins, expensive transformations that you don‚Äôt want to recompute every query.
- 	Read-only for users; you don‚Äôt insert/update it directly.
- 	Supports incremental refresh (Databricks SQL compute handles this efficiently).


üìå Example

```sql
CREATE MATERIALIZED VIEW monthly_sales_mv
AS
SELECT customer_id, month(order_date) as order_month, SUM(amount) as total_amount
FROM sales_delta
GROUP BY customer_id, month(order_date);
```

Databricks creates:

- 	A backing Delta table (managed by the system).
- 	A refresh plan so that when sales_delta changes, the materialized view is incrementally updated.

You can query it just like a table:

```sql
SELECT * FROM monthly_sales_mv WHERE order_month = 7;
```

But you don‚Äôt manually insert into it.


---

### üìä 3. Regular SQL View vs Materialized View vs Delta Table


| Feature         | SQL View üìù                 | Materialized View üß†                | Delta Table üì¶                  |
|-----------------|----------------------------|-------------------------------------|---------------------------------|
| **Storage**     | ‚ùå No storage (virtual)     | ‚úÖ Precomputed, stored as Delta     | ‚úÖ Physical Delta storage        |
| **Performance** | Slow for complex queries    | Fast ‚Äî query results cached/stored  | Fast ‚Äî direct data access       |
| **Refresh**     | N/A (computed on every query) | Automatic / incremental on source change | Manual (you control updates)    |
| **Writable**    | ‚ùå No                       | ‚ùå No                              | ‚úÖ Yes ‚Äî full CRUD support       |
| **Maintenance** | None                       | Managed by Databricks              | You maintain                    |
| **Use case**    | Lightweight query abstraction | Fast dashboard queries, repeated aggregations | Core datasets, raw or curated layers |



üìù In Practice (Databricks & Delta Lake)

- 	Use Delta tables for your bronze / silver / gold data layers ‚Äî i.e., actual data pipelines, raw ingestion, curated datasets.
- 	Use materialized views on top of Delta tables for performance optimization in Databricks SQL / BI dashboards, e.g., pre-aggregated metrics.
- 	Use regular views when you just need a logical abstraction with no performance/storage needs.


‚ö†Ô∏è Important
- 	Materialized views require Databricks SQL (Pro/Serverless) ‚Äî they‚Äôre not the same as manually creating a Delta table with CREATE TABLE AS SELECT (CTAS).
- 	You can simulate a materialized view by creating a Delta table and scheduling a job to refresh it ‚Äî useful if you‚Äôre not on Databricks SQL Pro.



---

### üß™ Quick Summary


| Feature                | Delta Table üì¶                       | Materialized View üß†                      |
|------------------------|--------------------------------------|------------------------------------------|
| Data stored            | ‚úÖ Yes                                | ‚úÖ Yes (precomputed derived results)      |
| User manages refresh   | ‚úÖ Yes                                | ‚ùå Databricks manages                    |
| Writable               | ‚úÖ Yes                                | ‚ùå No                                    |
| Best for               | Core datasets, ETL, transformations  | Speeding up queries on aggregates        |



---


##  Simulate a Materialized View using Delta table + a scheduled Databricks job 


Perfect ‚úÖ ‚Äî below is a clean and practical pattern to simulate a materialized view using a Delta table + a scheduled Databricks job (or Workflow).
This approach works in Databricks Community, Standard, or Premium ‚Äî you don‚Äôt need Databricks SQL Pro.


---

### üß≠ Scenario

You have a raw Delta table:

```sql
CREATE TABLE sales_delta
USING delta
LOCATION '/mnt/delta/sales';
```

You want a ‚Äúmaterialized view‚Äù that stores monthly aggregated sales per customer, refreshed daily (or incrementally).

---

### üõ†Ô∏è Step 1: Create the Target Delta Table

This table will hold the precomputed result, similar to a materialized view‚Äôs backing table.

```sql
CREATE TABLE IF NOT EXISTS monthly_sales_mv
(
  customer_id STRING,
  order_month INT,
  total_amount DOUBLE
)
USING delta
LOCATION '/mnt/delta/materialized_views/monthly_sales_mv';
```

üìå This acts like the materialized view table, but you manage its refresh.

---

### üß† Step 2: Write a Refresh Query (Scala / PySpark)


Here‚Äôs a Scala Spark job you can put in a Databricks notebook and schedule as a job:

```scala
import org.apache.spark.sql.functions._

// 1. Load source Delta table
val salesDF = spark.read.format("delta").load("/mnt/delta/sales")

// 2. Compute the derived result (e.g., monthly aggregation)
val aggDF = salesDF
  .withColumn("order_month", month(col("order_date")))
  .groupBy("customer_id", "order_month")
  .agg(sum("amount").alias("total_amount"))

// 3. Overwrite the target table atomically
aggDF.write
  .format("delta")
  .mode("overwrite")
  .option("overwriteSchema", "true")
  .save("/mnt/delta/materialized_views/monthly_sales_mv")

```

‚úÖ This simulates rebuilding the materialized view.

If the data volume is large, you can:

- 	Use MERGE INTO to incrementally update only changed partitions.
- 	Filter source table by watermark / modified date.


---

### üß† Optional: Incremental Refresh using MERGE

If you track updates using, say, a last_updated timestamp, you can make the refresh job incremental:

```scala
import org.apache.spark.sql.functions._
import org.apache.spark.sql.DataFrame

val lastRefreshTime = spark.sql("SELECT MAX(refresh_time) FROM metadata.mv_refresh_log")
  .first().getTimestamp(0)

val changedSalesDF = spark.read.format("delta")
  .load("/mnt/delta/sales")
  .filter(col("last_updated") > lit(lastRefreshTime))

val aggChangesDF = changedSalesDF
  .withColumn("order_month", month(col("order_date")))
  .groupBy("customer_id", "order_month")
  .agg(sum("amount").alias("total_amount"))

aggChangesDF.createOrReplaceTempView("changes")

spark.sql("""
MERGE INTO monthly_sales_mv AS target
USING changes AS source
ON target.customer_id = source.customer_id AND target.order_month = source.order_month
WHEN MATCHED THEN UPDATE SET target.total_amount = source.total_amount
WHEN NOT MATCHED THEN INSERT *
""")
```

Then update mv_refresh_log table with the new refresh timestamp.

---

### ‚è∞ Step 3: Schedule a Databricks Job / Workflow


1.	Go to Workflows ‚Üí Jobs in Databricks UI.
2.	Create a new job.
3.	Select the notebook with the refresh logic.
4.	Choose a cluster (can be a small job cluster or existing all-purpose).
5.	Set a schedule (e.g., daily at midnight or every hour).
6.	Add optional email / Slack alerts.


---

### üìù Step 4: Query the Materialized View Table

Your users / dashboards can now query the table as if it were a materialized view:

```sql
SELECT *
FROM monthly_sales_mv
WHERE order_month = 7
ORDER BY total_amount DESC;
```

Performance is fast because:
- 	It reads pre-aggregated Delta files
- 	You control refresh frequency and partitioning

---

### üöÄ Advantages of This Pattern


| Feature             | Simulated MV (Job + Delta Table)      |
|---------------------|---------------------------------------|
| Works on all tiers  | ‚úÖ Yes                                 |
| Refresh control     | ‚úÖ Full control (overwrite / merge)    |
| Query performance   | ‚úÖ Fast, stored data                   |
| Complex transforms  | ‚úÖ Handled in Spark                    |
| Auto refresh        | ‚úÖ Via scheduled job                   |


---

üìå Pro Tips

- 	Partition the MV table by month or other time column to optimize refresh performance.
- 	Add ZORDER on frequently filtered columns:

```sql
OPTIMIZE monthly_sales_mv ZORDER BY (customer_id);
```

Track refresh metadata (timestamps, row counts, duration) in a small log table for auditing.

---

‚úÖ Summary


| Feature                | Real Materialized View (Databricks SQL) | Simulated via Job + Delta Table      |
|------------------------|-----------------------------------------|--------------------------------------|
| Management             | Managed by Databricks SQL               | You manage via jobs/notebooks        |
| Refresh                | Automatic                               | Scheduled job (manual or workflow)   |
| Control                | Less flexible                           | Full control over logic and timing   |
| Cost                   | Requires SQL Pro                        | Works on any Databricks tier         |


---


##  Auto Loader + Change Data Feed (CDF)


Excellent üëå ‚Äî using Auto Loader + Change Data Feed (CDF) is the most efficient way to refresh a ‚Äúmaterialized view‚Äù in Delta Lake for very large tables.

This approach avoids full table scans by:

1.	Detecting incremental changes as new files land (Auto Loader).
2.	Reading only changed rows from the Delta source using Change Data Feed (table_changes).
3.	Applying incremental updates to your precomputed Delta ‚Äúmaterialized view‚Äù table using MERGE.

Let‚Äôs walk through this step by step with a Scala example üëá


---

### üß≠ Scenario

- 	Source table: sales_delta (80M+ rows).
- 	MV target: monthly_sales_mv (aggregated monthly totals).
- 	New or updated source data lands incrementally in cloud storage (e.g., S3 or ADLS).
- 	You want to incrementally refresh the MV without scanning the entire source every time.


---

### üß± 1. Enable Change Data Feed (CDF) on the Source Table

When creating or altering the source Delta table:

```sql
ALTER TABLE sales_delta SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
```

‚úÖ This ensures all inserts, updates, and deletes are tracked in the Delta log.

You can then use table_changes to get only the modified rows.

---

### ü™Ñ 2. Set Up the Target MV Table

```sql
CREATE TABLE IF NOT EXISTS monthly_sales_mv (
  customer_id STRING,
  order_month INT,
  total_amount DOUBLE
)
USING delta
LOCATION '/mnt/delta/materialized_views/monthly_sales_mv';
```

(Optional) Partition by month for performance:

```sql
ALTER TABLE monthly_sales_mv SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true');
```

---

### üöÄ 3. Auto Loader to Detect New Files

In Databricks, Auto Loader (cloudFiles) can continuously load new data and trigger downstream processing.

```scala
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.Trigger

val rawStreamDF = spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "parquet")  // or "json", "csv" depending on your data
  .option("cloudFiles.schemaLocation", "/mnt/checkpoints/sales_schema")
  .load("/mnt/raw/sales_incremental")   // incoming data folder

// Write new data into the source Delta table incrementally
rawStreamDF.writeStream
  .format("delta")
  .option("checkpointLocation", "/mnt/checkpoints/sales_delta_ingest")
  .outputMode("append")
  .toTable("sales_delta")

```

‚úÖ Now, whenever new files land in /mnt/raw/sales_incremental, Auto Loader ingests them into sales_delta.


---

### üß† 4. Incremental Refresh Job Using CDF

Create a separate Databricks job/notebook to process CDF and merge into the MV table.

```scala
import org.apache.spark.sql.functions._

// Get the last refresh version from a metadata table
val lastProcessedVersion = spark.sql("""
  SELECT COALESCE(MAX(source_version), 0) FROM metadata.mv_refresh_log
""").first().getLong(0)

// Read only changes since the last processed version
val changesDF = spark.read
  .format("delta")
  .option("readChangeFeed", "true")
  .option("startingVersion", lastProcessedVersion + 1)
  .table("sales_delta")

// Compute monthly aggregates from changed records
val aggChangesDF = changesDF
  .filter(col("_change_type") =!= "delete")  // handle deletes separately if needed
  .withColumn("order_month", month(col("order_date")))
  .groupBy("customer_id", "order_month")
  .agg(sum("amount").alias("total_amount"))

aggChangesDF.createOrReplaceTempView("agg_changes")

// Merge incremental aggregates into the MV table
spark.sql("""
MERGE INTO monthly_sales_mv AS target
USING agg_changes AS source
ON target.customer_id = source.customer_id AND target.order_month = source.order_month
WHEN MATCHED THEN UPDATE SET target.total_amount = source.total_amount
WHEN NOT MATCHED THEN INSERT *
""")

// Record the latest processed version for next run
val latestVersion = spark.sql("DESCRIBE HISTORY sales_delta").selectExpr("max(version)").first().getLong(0)

spark.sql(s"""
INSERT INTO metadata.mv_refresh_log (mv_name, source_version, refresh_time)
VALUES ('monthly_sales_mv', $latestVersion, current_timestamp())
""")
```

üìù Notes:

- 	readChangeFeed lets you read only changed rows, not the full 80M dataset.
- 	You can handle deletes by subtracting values or recomputing affected groups.
- 	mv_refresh_log is a simple table you maintain with columns: (mv_name STRING, source_version BIGINT, refresh_time TIMESTAMP).

---


### ‚è∞ 5. Schedule the Refresh Job

Create a Databricks Workflow (Job) that runs this notebook:

- 	Trigger: Every 15 min / hourly / after Auto Loader finishes.
- 	Cluster: Use a lightweight job cluster.
- 	Alerts: Add failure notifications.

Alternatively, if you want near-real-time, you can even make this streaming.

---

### üß™ Optional: Streaming MV Refresh

You can run a streaming job on CDF for real-time refresh:

```scala
val cdfStream = spark.readStream
  .format("delta")
  .option("readChangeFeed", "true")
  .option("startingVersion", 0)
  .table("sales_delta")

val aggStream = cdfStream
  .filter(col("_change_type") =!= "delete")
  .withColumn("order_month", month(col("order_date")))
  .groupBy("customer_id", "order_month")
  .agg(sum("amount").alias("total_amount"))

aggStream.writeStream
  .format("delta")
  .option("checkpointLocation", "/mnt/checkpoints/monthly_sales_mv")
  .foreachBatch { (batchDF, batchId) =>
    batchDF.createOrReplaceTempView("batch_changes")
    spark.sql("""
      MERGE INTO monthly_sales_mv AS target
      USING batch_changes AS source
      ON target.customer_id = source.customer_id AND target.order_month = source.order_month
      WHEN MATCHED THEN UPDATE SET target.total_amount = source.total_amount
      WHEN NOT MATCHED THEN INSERT *
    """)
  }
  .start()

```

This gives you near real-time ‚Äúmaterialized view‚Äù refresh üöÄ.

---

### üìù Summary Table


| Feature      | Full Scan Recompute         | CDF + Auto Loader                |
|--------------|----------------------------|----------------------------------|
| Reads whole table | ‚úÖ Yes                 | ‚ùå No (only changes)              |
| Cost         | High                       | Low                              |
| Latency      | Batch                      | Near real-time                   |
| Complexity   | Low                        | Moderate                         |
| Best for     | Small tables or daily refresh | Large tables, frequent updates |


---

‚úÖ Key Takeaways

- 	‚úÖ Enable CDF on source tables.
- 	‚úÖ Use Auto Loader to ingest data incrementally.
- 	‚úÖ Use CDF + MERGE to refresh MV tables efficiently.
- 	‚úÖ Schedule or stream the refresh job for real-time updates.
- 	‚úÖ This pattern scales to hundreds of millions of rows without full scans.


