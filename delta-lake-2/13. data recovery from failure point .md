### A. Data recovery from any failure point


To implement data recovery from any failure point in the Databricks pipeline project, follow these key strategies:


1ï¸âƒ£ Use Task-Level Retry Logic

Each task in a Databricks job can be configured to retry upon failure. Modify the pipeline configuration to specify the max_retries parameter.

Updated Example databricks.yml
```yaml
pipelines:
  - name: DataProcessingPipeline
    target: dev
    clusters:
      - name: shared_cluster
        spark_version: 12.2.x-scala2.12
        node_type_id: i3.xlarge
        autoscale:
          min_workers: 2
          max_workers: 8

    jobs:
      - name: Data Ingestion
        tasks:
          - task_key: extract_data
            notebook_task:
              notebook_path: /Repos/data_team/extract_data
            cluster_key: shared_cluster
            max_retries: 3
            min_retry_interval_millis: 30000  # 30 sec delay before retry

      - name: Data Transformation
        tasks:
          - task_key: transform_data
            notebook_task:
              notebook_path: /Repos/data_team/transform_data
            cluster_key: shared_cluster
            depends_on:
              - task_key: extract_data
            max_retries: 3

      - name: Data Loading
        tasks:
          - task_key: load_data
            notebook_task:
              notebook_path: /Repos/data_team/load_data
            cluster_key: shared_cluster
            depends_on:
              - task_key: transform_data
            max_retries: 3
```

âœ… This ensures automatic retries in case of transient failures (e.g., network issues, API timeouts).


---

2ï¸âƒ£ Implement Checkpointing for Stateful Recovery

To ensure recovery from failure points, write checkpoint files after completing each step.

Example: Using Delta Lake Checkpoints

Modify each notebook (extract_data, transform_data, load_data) to track progress using Delta Lake:

```python
from delta.tables import DeltaTable
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DataPipeline").getOrCreate()

# Check if the checkpoint exists
checkpoint_path = "dbfs:/pipelines/checkpoints/transform_data"
if DeltaTable.isDeltaTable(spark, checkpoint_path):
    print("Checkpoint found, resuming from last state...")
    df = spark.read.format("delta").load(checkpoint_path)
else:
    print("No checkpoint found, starting fresh...")
    df = spark.read.format("parquet").load("dbfs:/raw-data/source.parquet")

# Process data...
df_transformed = df.withColumn("processed_timestamp", current_timestamp())

# Save checkpoint
df_transformed.write.format("delta").mode("overwrite").save(checkpoint_path)
```

âœ… If a failure occurs, the next run will resume from the last checkpoint instead of reprocessing everything.

---

3ï¸âƒ£ Use Job Cloning for Selective Re-Runs

If a specific job fails, restart only that job instead of the entire pipeline.

CLI Command to Restart a Failed Job

```sh
databricks jobs run-now --job-id <job-id>
```

âœ… This ensures only failed components are retried, reducing processing time.

---

4ï¸âƒ£ Store Execution State in a Metadata Table

Maintain a log table in Delta Lake or an external metadata table (e.g., MySQL, PostgreSQL) to track each jobâ€™s execution status.

Example: Tracking Job Status in Delta Table

```python
from datetime import datetime
from pyspark.sql.functions import lit

log_table = "dbfs:/pipelines/metadata/job_status"

df_log = spark.createDataFrame(
    [(datetime.now(), "extract_data", "SUCCESS")],
    ["timestamp", "task_name", "status"]
)

df_log.write.format("delta").mode("append").save(log_table)
```

âœ… When a failure occurs, read this table to determine where to resume processing.

---

5ï¸âƒ£ Alerting and Monitoring (Databricks Alerts & Webhooks)

Set up failure notifications to monitor job status using Databricks Alerts or Webhooks.

Example: Slack Notification on Failure
-	Configure Databricks Jobs to send failure alerts via Slack, PagerDuty, or Email.

```sh
databricks jobs update --json '{
  "email_notifications": {
    "on_failure": ["your_email@example.com"]
  }
}'
```

---
ğŸ”¹ Summary of Best Practices for Failure Recovery


---

### B. Pipeline DAG with Failure Recovery


Hereâ€™s a DAG (Directed Acyclic Graph) visualization of the failure recovery workflow for your Databricks pipeline.

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Data Ingestionâ”‚
        â”‚ (extract_data)â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚  
      Retry on Failure (max_retries=3)  
               â”‚  
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚ Data Transformation â”‚
        â”‚ (transform_data)    â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚     â”‚  
    Retry on Failure   Checkpointing (Delta Lake)  
              â”‚     â”‚  
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚ Data Loading    â”‚
        â”‚ (load_data)     â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚  
    Retry on Failure  
              â”‚  
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Metadata Logging â”‚
        â”‚ (job_status)     â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚  
        Alert on Failure  
              â”‚  
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Notifications  â”‚
        â”‚ (Slack/Email)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Key Features in the DAG:

1.	Task-Level Retries: Each job (extract_data, transform_data, load_data) is set to retry up to 3 times on failure.
2.	Checkpointing with Delta Lake: Before moving to the next stage, data is stored in a checkpoint table (dbfs:/pipelines/checkpoints).
3.	Metadata Logging: Execution status is recorded in a Delta table (job_status) for tracking.
4.	Selective Job Re-Runs: If a job fails, you can restart it without rerunning the entire pipeline.
5.	Alerts & Monitoring: Slack/email notifications trigger on failures for quick debugging.

---

Hereâ€™s a Databricks pipeline YAML configuration that defines multiple jobs, implements failure recovery using retries, checkpoints, and alerts, and supports incremental reprocessing by tracking progress in Delta tables.


Databricks Pipeline YAML with Failure Recovery

```yaml
pipelines:
  name: data_processing_pipeline
  target: prod
  libraries:
    - pypi:
        package: delta-spark
    - pypi:
        package: databricks-api

  clusters:
    - label: default
      spark_version: 12.2.x-scala2.12
      num_workers: 4
      node_type_id: Standard_D3_v2

  jobs:
    - name: extract_data
      task: 
        notebook_task:
          notebook_path: "/Repos/data_pipeline/extract_data"
        retry_policy:
          max_retries: 3
          min_retry_interval_millis: 60000
        depends_on: []

    - name: transform_data
      task: 
        notebook_task:
          notebook_path: "/Repos/data_pipeline/transform_data"
        retry_policy:
          max_retries: 3
          min_retry_interval_millis: 60000
        depends_on:
          - extract_data
      libraries:
        - pypi:
            package: pandas

    - name: load_data
      task: 
        notebook_task:
          notebook_path: "/Repos/data_pipeline/load_data"
        retry_policy:
          max_retries: 3
          min_retry_interval_millis: 60000
        depends_on:
          - transform_data

    - name: job_status
      task:
        notebook_task:
          notebook_path: "/Repos/data_pipeline/logging"
        depends_on:
          - extract_data
          - transform_data
          - load_data

    - name: notifications
      task:
        notebook_task:
          notebook_path: "/Repos/data_pipeline/alerting"
        depends_on:
          - job_status
```

How This Works

- âœ… Modular Jobs: Each step (extract_data, transform_data, load_data) runs separately and retries on failure (up to 3 times).
- âœ… Dependency Management: transform_data runs only after extract_data succeeds.
- âœ… Incremental Processing: Uses checkpoints in Delta Lake to resume from the last successful step.
- âœ… Failure Alerts: notifications job sends alerts via Slack or email when any job fails.


---

 ### C. Python Notebook Example for Failure Recovery

Hereâ€™s a Python-based failure recovery implementation for your Databricks pipeline using Delta Lake checkpoints and job retries.


---
1ï¸âƒ£ Extract Data (extract_data.py)

Extracts data from an S3 source and writes it to a Delta table with a checkpoint.

```python
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

spark = SparkSession.builder.appName("ExtractData").getOrCreate()

# Source configuration
S3_SOURCE_PATH = "s3://your-bucket/raw-data/"
CHECKPOINT_PATH = "dbfs:/mnt/checkpoints/extract_data"
DELTA_PATH = "dbfs:/mnt/delta/extracted_data"

# Read data from S3
df = spark.read.format("csv").option("header", "true").load(S3_SOURCE_PATH)

# Write to Delta with checkpointing
df.write.format("delta").mode("overwrite").option("checkpointLocation", CHECKPOINT_PATH).save(DELTA_PATH)

print("âœ… Extract Data Completed")
```

---
2ï¸âƒ£ Transform Data (transform_data.py)

Reads from the Delta table, applies transformations, and writes the cleaned data back.

```python
from pyspark.sql.functions import col

# Paths
DELTA_INPUT = "dbfs:/mnt/delta/extracted_data"
DELTA_OUTPUT = "dbfs:/mnt/delta/transformed_data"
CHECKPOINT_PATH = "dbfs:/mnt/checkpoints/transform_data"

df = spark.read.format("delta").load(DELTA_INPUT)

# Transformation: Convert column to uppercase
df_transformed = df.withColumn("name", col("name").upper())

# Write back with checkpoint
df_transformed.write.format("delta").mode("overwrite").option("checkpointLocation", CHECKPOINT_PATH).save(DELTA_OUTPUT)

print("âœ… Transform Data Completed")
```

---

3ï¸âƒ£ Load Data (load_data.py)

Loads transformed data into a final table in Databricks.

```python
DELTA_INPUT = "dbfs:/mnt/delta/transformed_data"
FINAL_TABLE = "default.final_table"
CHECKPOINT_PATH = "dbfs:/mnt/checkpoints/load_data"

df = spark.read.format("delta").load(DELTA_INPUT)

# Write to final table
df.write.format("delta").mode("overwrite").option("checkpointLocation", CHECKPOINT_PATH).saveAsTable(FINAL_TABLE)

print("âœ… Load Data Completed")

```

---

4ï¸âƒ£ Error Handling & Retrying (utils.py)

Retries a job 3 times in case of failure.

```python
import time

def run_with_retries(job, max_retries=3):
    attempt = 0
    while attempt < max_retries:
        try:
            job()
            return
        except Exception as e:
            print(f"âš ï¸ Attempt {attempt+1} failed: {str(e)}")
            attempt += 1
            time.sleep(30)  # Wait before retrying
    print("âŒ Job failed after multiple attempts")
```

---

5ï¸âƒ£ Alerting (alerting.py)

Sends alerts via email or Slack.

```python
import requests

SLACK_WEBHOOK = "https://hooks.slack.com/services/YOUR/WEBHOOK"

def send_alert(message):
    requests.post(SLACK_WEBHOOK, json={"text": message})

send_alert("ğŸš¨ Job Failed: Extract Data")
```
---

â© Recovery Features

âœ… Delta Checkpoints â†’ Resume from last successful step
âœ… Retry Mechanism â†’ Retries failed jobs up to 3 times
âœ… Slack Alerts â†’ Sends alerts on failure


---
### D. Run from Failure


Yes! Databricks has a built-in feature called â€œRun from Failureâ€ that automatically detects which tasks have succeeded and only re-runs the failed ones. You donâ€™t need to implement custom recovery logic in most cases.

âœ… How â€œRun from Failureâ€ Works

-	When a Databricks Job fails, you can re-run the job, and it will automatically skip completed tasks and only restart the failed ones.
-	It uses task dependencies to ensure that all dependent tasks are executed in the correct order.

---

ğŸš€ How to Enable â€œRun from Failureâ€

1ï¸âƒ£ Manually Trigger Recovery (Databricks UI)

-	Open Databricks Workflows â†’ Job Runs
-	Find the failed job run
-	Click â€œRun from Failureâ€

2ï¸âƒ£ Automatically Restart from Failure (Databricks API)

-	You can use the Databricks Jobs API to programmatically restart a failed job.
```bash
curl -X POST https://<databricks-instance>/api/2.1/jobs/runs/repair \
  -H "Authorization: Bearer <your-token>" \
  -H "Content-Type: application/json" \
  -d '{
    "run_id": <failed_run_id>,
    "rerun_tasks": "ALL"
  }'
```
---

ğŸ›  Best Practice: Automatic Job Restart in YAML

If you want Databricks to automatically retry failed tasks, configure retries in your job.yml file:

```yaml
jobs:
  - job:
      name: "Data_Pipeline_With_Recovery"
      tasks:
        - task:
            name: "Step_1_Extract"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_1_Extract"
            max_retries: 3  # Retries failed tasks automatically

        - task:
            name: "Step_2_Transform"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_2_Transform"
            depends_on:
              - Step_1_Extract
            max_retries: 3

        - task:
            name: "Step_3_Load"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_3_Load"
            depends_on:
              - Step_2_Transform
            max_retries: 3
```

---


ğŸ”¥ Final Answer:

Yes! Just re-run the job, and Databricks will automatically skip successful tasks and restart only the failed ones.

-	If you want full automation, use job retries (max_retries) or the â€œrepair jobâ€ API.
-	This is the best and simplest approach for failure recovery in Databricks Jobs. ğŸš€


---

### E. ğŸ”¥ Implementing Automatic Recovery in Databricks Jobs (Scala)

Since your data processing pipeline is divided into multiple notebooks, you need a programmatic recovery mechanism to ensure that:

- âœ… Only failed tasks are retried (no reprocessing of successful tasks).
- âœ… Each task runs in a separate Databricks notebook.
- âœ… State tracking is managed outside the notebook execution.

---

ğŸ”¹ Approach: Using Databricks Job API for Failure Recovery

Since each task is in a separate notebook, the Databricks Jobs API can be used to track failed tasks and restart only those tasks.

ğŸ”¹ 1ï¸âƒ£ Setup: Define Databricks Job YAML (job.yml)

Define a Databricks Job where each task runs a separate notebook.

```yaml
jobs:
  - job:
      name: "Data_Pipeline_With_Recovery"
      tasks:
        - task:
            name: "Step_1_Extract"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_1_Extract"
            depends_on: []
            max_retries: 3

        - task:
            name: "Step_2_Transform"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_2_Transform"
            depends_on:
              - Step_1_Extract
            max_retries: 3

        - task:
            name: "Step_3_Load"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_3_Load"
            depends_on:
              - Step_2_Transform
            max_retries: 3
```

---

ğŸ”¹ 2ï¸âƒ£ Implement Recovery Logic in a Controller Notebook

We create a controller notebook (/Workspace/ETL/Controller) to:
- âœ… Check job status
- âœ… Restart only failed tasks
- âœ… Log recovery status


```scala
import com.databricks.api.jobs._
import org.json4s._
import org.json4s.jackson.JsonMethods._
import scalaj.http._

// Databricks API credentials
val databricksInstance = "https://<your-databricks-instance>"
val databricksToken = "your-databricks-token"
val jobId = "123456" // Replace with actual job ID

// Function to check job run status
def getJobRunStatus(runId: String): String = {
  val url = s"$databricksInstance/api/2.1/jobs/runs/get?run_id=$runId"
  val response = Http(url)
    .header("Authorization", s"Bearer $databricksToken")
    .asString
  val json = parse(response.body)
  (json \ "state" \ "life_cycle_state").extract[String]
}

// Function to restart failed tasks
def restartFailedTasks(): Unit = {
  val url = s"$databricksInstance/api/2.1/jobs/runs/list"
  val response = Http(url)
    .header("Authorization", s"Bearer $databricksToken")
    .asString
  val json = parse(response.body)
  val runs = (json \ "runs").extract[List[JValue]]

  for (run <- runs) {
    val runId = (run \ "run_id").extract[Long].toString
    val state = getJobRunStatus(runId)

    if (state == "FAILED" || state == "TIMED_OUT") {
      println(s"Restarting failed task: Run ID $runId")
      val restartUrl = s"$databricksInstance/api/2.1/jobs/runs/run-now"
      val restartPayload = s"""{"job_id": $jobId, "notebook_params": {"mode": "resume"}}"""
      
      val restartResponse = Http(restartUrl)
        .header("Authorization", s"Bearer $databricksToken")
        .header("Content-Type", "application/json")
        .postData(restartPayload)
        .asString

      println(s"Restart response: ${restartResponse.body}")
    }
  }
}

// Execute the recovery function
restartFailedTasks()

```

---

ğŸ”¹ 3ï¸âƒ£ How This Works

1.	Each task runs in a separate notebook (Step_1_Extract, Step_2_Transform, Step_3_Load).
2.	If a task fails, Databricks records its status.
3.	The Controller Notebook (Controller):
  -	Checks Databricks Jobs API for failed tasks.
  -	Restarts only the failed tasks, bypassing successful ones.
4.	Automatic retries ensure tasks complete successfully.


ğŸ”¥ Why This Approach?

| Feature                           | Benefit                                    |
|-----------------------------------|--------------------------------------------|
| No reprocessing of successful tasks | Saves compute cost & time                  |
| Automatic failure recovery        | No manual intervention needed              |
| Uses Databricks Jobs API          | Scalable & reliable                        |
| Logs status & failures            | Easy debugging & monitoring                |


---

### F. 