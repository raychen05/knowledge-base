### Updating a column or multiple columns efficiently in Delta lake

In Delta Lake, updating a column or multiple columns efficiently can be achieved using the DeltaTable APIs in PySpark or Scala. Here’s an example of how to perform updates efficiently.


Setup: Example Data

```scala
    import org.apache.spark.sql.SparkSession
    import io.delta.tables._

    val spark = SparkSession.builder()
    .appName("Delta Lake Update Example")
    .master("local[*]")
    .getOrCreate()

    // Example data
    val data = Seq(
        (1, "John", 28, "NY"),
        (2, "Jane", 35, "CA"),
        (3, "Smith", 40, "TX")
    ).toDF("id", "name", "age", "state")

    // Save the data as a Delta table
    data.write.format("delta").mode("overwrite").save("/tmp/delta-table")

```

1. Update a Single Column

For example, let’s increase everyone’s age by 1:

```scala
    val deltaTable = DeltaTable.forPath(spark, "/tmp/delta-table")

    deltaTable.update(
        condition = expr("true"), // Apply to all rows
        set = Map("age" -> expr("age + 1"))
    )
```

2. Update Multiple Columns

Let’s update the state to “UpdatedState” for rows where age > 30 and change the name to UpdatedName:

```scala
    deltaTable.update(
        condition = expr("age > 30"),
        set = Map(
            "state" -> lit("UpdatedState"),
            "name" -> lit("UpdatedName")
        )
    )
```

3. Updating Using a Join

You can update columns based on another DataFrame:

```scala
    val updates = Seq(
        (2, "NewName", "FL"), // id 2: new name and state
        (3, "AnotherName", "NV") // id 3: new name and state
    ).toDF("id", "name", "state")

    deltaTable.as("target")
    .merge(
        updates.as("source"),
        "target.id = source.id"
    )
    .whenMatched()
    .updateExpr(Map(
        "name" -> "source.name",
        "state" -> "source.state"
    ))
    .execute()
  ```

#### Efficient Practices for Updates

	1.	Avoid Frequent Small Updates: Perform batch updates to minimize overhead.
	2.	Partition Pruning: Use Delta Lake’s partitioning to limit the scope of updates.
        •	Example: Update only rows in a specific partition (partition_column = value).
	3.	Z-Ordering: If frequent updates target specific rows, optimize file layout with OPTIMIZE and ZORDER.
    ```scala
    spark.sql("OPTIMIZE '/tmp/delta-table' ZORDER BY id")
    ```
	4.	Indexing and Caching: Delta Lake uses metadata caching for efficient updates, so regularly compact files if the table has small files.

This ensures your updates are applied efficiently and allows you to manage large-scale updates in Delta Lake with ease.


### Updating huge DataFrame source into a Delta Lake

To efficiently update a huge DataFrame source into a Delta Lake table while inserting new records and updating existing ones, you can use the MERGE operation. The MERGE statement allows you to perform an upsert (update + insert) efficiently in a transactional manner.


**Step-by-Step Implementation**

1. Prepare the Delta Table

Ensure your Delta Lake table is already created and properly partitioned for efficient querying and updating.

```scala
    import org.apache.spark.sql.SparkSession
    import io.delta.tables._

    // Initialize Spark session
    val spark = SparkSession.builder()
    .appName("Delta Upsert Example")
    .getOrCreate()

    // Path to Delta table
    val deltaTablePath = "/tmp/delta-table"

    // Load Delta table
    val deltaTable = DeltaTable.forPath(spark, deltaTablePath)
```

2. Prepare the Source DataFrame

Create or load a new source DataFrame with records that need to be upserted.

```scala
    // Example new data
    val newData = Seq(
    (1, "John Updated", 29, "NY"),  // Update
    (4, "New User", 22, "FL")       // Insert
    ).toDF("id", "name", "age", "state")
```

3. Perform Upsert with MERGE

Use the merge method to match records on a unique key (e.g., id) and define actions for WHEN MATCHED (update) and WHEN NOT MATCHED (insert).

```scala
    deltaTable.as("target")
    .merge(
        newData.as("source"),
        "target.id = source.id"  // Matching condition (primary key)
    )
    .whenMatched()
    .updateExpr(Map(
        "name" -> "source.name",
        "age" -> "source.age",
        "state" -> "source.state"
    ))
    .whenNotMatched()
    .insertExpr(Map(
        "id" -> "source.id",
        "name" -> "source.name",
        "age" -> "source.age",
        "state" -> "source.state"
    ))
    .execute()
  ```

4. Optimize for Performance

For large-scale data, you need to ensure performance optimizations:

	1.	Partition the Delta Table
    Partition the Delta table based on a logical key (e.g., state or date) to reduce the scope of updates.

    ```scala
        data.write.format("delta").partitionBy("state").save(deltaTablePath)
    ```

	2.	Z-Order Indexing
    Optimize the Delta table with Z-order indexing on frequently queried or updated columns.

    ```scala
        spark.sql(s"OPTIMIZE '$deltaTablePath' ZORDER BY id")
    ```

    3.	Small File Compaction
    Periodically compact small files generated by incremental updates using Delta Lake’s OPTIMIZE.

    ```scala
        spark.sql(s"OPTIMIZE '$deltaTablePath'")
    ```

    4.	Data Skipping
    Delta Lake automatically supports data skipping via min/max column statistics. Ensure your table is partitioned correctly for this feature to be effective.

    5.	Incremental Loads
    If your source data can be ingested incrementally (e.g., via timestamps), use filters to load only new or updated data:

    ```scala
        val incrementalData = sourceDf.filter("last_updated > '2024-01-01'")
    ```

    Additional Tips

	•	Schema Evolution: Use mergeSchema if the schema might evolve.

    ```scala
        deltaTable.merge(
            sourceData,
            condition = "target.id = source.id"
        ).option("mergeSchema", "true")
    ```

    •	Monitor Performance: Use Delta Lake’s DESCRIBE HISTORY to monitor and optimize table performance.

    This approach is scalable for large datasets and integrates well with Delta Lake’s ACID properties to ensure data consistency.