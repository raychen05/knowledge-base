
 ## Batch Data Processing for Large-Scale CSV Files on S3


 üîπ Overview

The project involves processing a large dataset (~80 million records) stored as CSV files in an S3 bucket. The data processing workflow follows a weekly batch schedule and requires multiple processing phases, intermediate data transformations, and controlled execution sequencing. The final output consists of multiple Delta tables.


---
üîπ Key Requirements

1.	Batch Data Processing: The system should efficiently handle large-scale CSV data ingestion, transformation, and storage.
2.	Multi-Stage Processing: Data should go through multiple sequential processing phases, ensuring each phase is executed in the correct order.
3.	Intermediate Data Handling: Intermediate results should be persisted to optimize processing and enable incremental updates.
4.	Automation: The entire workflow should be automated, minimizing manual intervention.
5.	Failure Recovery: If a processing step fails, the pipeline should automatically restart from the last successful step instead of starting over from scratch.
6.	Scalability & Maintainability: The system should be designed to scale efficiently while remaining easy to maintain.

---

üîπ Best Approach: Databricks Jobs vs. Databricks Pipelines



| Feature                            | Databricks Jobs                           | Databricks Pipelines (Delta Live Tables) |
|------------------------------------|-------------------------------------------|------------------------------------------|
| **Best For**                       | General-purpose ETL workflows             | Incremental, real-time, or live table processing |
| **Processing Type**                | Batch-oriented                            | Streaming and batch                      |
| **Supports Delta Lake?**           | ‚úÖ Yes                                    | ‚úÖ Yes                                   |
| **Complex Data Dependencies?**    | ‚úÖ Yes                                    | üö´ Limited                               |
| **Automated Failure Recovery?**    | ‚úÖ Yes                                    | ‚úÖ Yes                                   |
| **Best Use Case for This Project?**| ‚úÖ Recommended                            | üö´ Not Ideal                             |



üí° Recommendation: Use Databricks Jobs because:

-	The project involves batch processing rather than real-time/live updates, which is what Delta Live Tables (DLT) Pipelines are optimized for.
-	Databricks Jobs provide better control over execution flow, intermediate outputs, and automatic recovery from failures at any step.

Based on your requirements, Databricks Jobs are the best choice because:

-	‚úÖ Batch processing is required (not streaming or real-time updates).
-	‚úÖ Complex processing logic with multiple phases and intermediate data.
-	‚úÖ Automatic failure recovery is needed, allowing jobs to restart from the last successful phase.
-	‚úÖ Multiple output Delta tables must be generated.
-	‚úÖ Data processing sequence control is critical.


---

üîπ How to Implement This in Databricks Jobs

1Ô∏è‚É£ Define a Databricks Workflow (YAML-based Job Definition)

-	Each processing phase should be a separate task in a Databricks Job.
-	Task dependencies should be used to enforce execution order.
-	Set up automatic retries and failure recovery.

Example job.yml:
```yaml
jobs:
  - job:
      name: "Weekly_Batch_Processing"
      schedule: "0 3 * * 1"  # Runs every Monday at 3 AM UTC
      tasks:
        - task:
            name: "Step_1_Ingest"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_1_Ingest"
            libraries:
              - pypi: { package: "pandas" }
              - maven: { coordinates: "org.apache.spark:spark-sql_2.12:3.3.0" }
            max_retries: 3

        - task:
            name: "Step_2_Transform"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_2_Transform"
            depends_on:
              - Step_1_Ingest
            max_retries: 3

        - task:
            name: "Step_3_Aggregate"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_3_Aggregate"
            depends_on:
              - Step_2_Transform
            max_retries: 3

        - task:
            name: "Step_4_Load"
            notebook_task:
              notebook_path: "/Workspace/ETL/Step_4_Load"
            depends_on:
              - Step_3_Aggregate
            max_retries: 3
```


2Ô∏è‚É£ Enable Automatic Failure Recovery

-	Databricks ‚ÄúRun from Failure‚Äù ensures that when a job fails, it only re-runs the failed tasks instead of restarting the entire pipeline.
-	If needed, restart failed jobs manually via Databricks UI or programmatically using the Databricks API.

3Ô∏è‚É£ Optimize Processing with Delta Lake

-	Store intermediate and final results in Delta tables for efficient storage, ACID compliance, and better performance.
-	Example SQL to write final results:

```sql
CREATE OR REPLACE TABLE processed_data
USING DELTA
AS
SELECT * FROM raw_data WHERE processed_flag = true;
```

---

üîπ Final Recommendation: Databricks Jobs

‚úÖ Databricks Jobs is the best choice for this use case because:

-	It allows precise control over batch processing and task dependencies.
-	Supports automatic failure recovery at any phase.
-	Enables automation with YAML-based job configurations.
-	Works well with Delta Lake for efficient data storage.


üí° If real-time ingestion or continuous updates are needed in the future, consider Delta Live Tables (Pipelines). But for now, Databricks Jobs is the best fit. üöÄ


---

##  Recommended Approach: Databricks Jobs with Multiple Tasks & Checkpointing

1.	Define a Job in job.yml
2.	Break the process into multiple tasks (ETL phases).
3.	Use Delta Lake Checkpointing for failure recovery.
4.	Leverage Task Dependencies to control the execution order.
5.	Use Job Clusters to optimize costs (instead of all-purpose clusters).

Example Databricks Job (job.yml)
```yaml
jobs:
  - job:
      name: "Weekly_Batch_Processing"
      schedule:
        quartz_cron_expression: "0 0 2 ? * MON *"  # Runs every Monday at 2 AM UTC
        timezone_id: "UTC"
      tasks:
        - task:
            name: "Ingest_CSV_from_S3"
            new_cluster:
              spark_version: "13.3.x-scala2.12"
              node_type_id: "i3.xlarge"
              num_workers: 4
            notebook_task:
              notebook_path: "/Workspace/ETL/Ingest"
            libraries:
              - pypi:
                  package: "boto3"
            outputs:
              - dataset: "raw_data"
              
        - task:
            name: "Data_Cleansing"
            depends_on: ["Ingest_CSV_from_S3"]
            notebook_task:
              notebook_path: "/Workspace/ETL/Cleansing"
            outputs:
              - dataset: "clean_data"
              
        - task:
            name: "Transformations"
            depends_on: ["Data_Cleansing"]
            notebook_task:
              notebook_path: "/Workspace/ETL/Transform"
            outputs:
              - dataset: "transformed_data"
              
        - task:
            name: "Generate_Delta_Tables"
            depends_on: ["Transformations"]
            notebook_task:
              notebook_path: "/Workspace/ETL/Save_To_Delta"
            outputs:
              - dataset: "final_output"

      email_notifications:
        on_failure:
          - "your_email@example.com"
```

---
Why This Approach?


| Requirement                    | Why Databricks Jobs?                                      |
|---------------------------------|-----------------------------------------------------------|
| Batch Processing                | Jobs are optimized for batch workloads.                   |
| Large Data Volume (80M records) | Uses job clusters for better scalability & cost control.  |
| Complex Multi-Step Processing   | Tasks with dependencies ensure sequential execution.      |
| Automatic Failure Recovery      | Checkpoints & task retries enable resuming from failure points. |
| Intermediate Data Handling      | Intermediate outputs (raw_data, clean_data, transformed_data) allow staged processing. |
| Multiple Delta Table Outputs    | Final task writes processed data to multiple Delta tables. |

---

Failure Recovery Strategy

1.	Use Delta Lake Checkpoints
-	Each step writes output to Delta tables with _checkpoint metadata.
-	If a job fails, restart from the last completed stage.

2.	Enable Automatic Task Retries
-	Set up retries in the Databricks UI for each task.

3.	Use depends_on for Task Sequencing
-	Ensures proper execution order & avoids re-processing.

4.	Monitor with Job Alerts & Logging
-	Email notifications for failures.
-	Log status updates to a monitoring table.

---

Alternative: Why Not a Databricks Pipeline (pipelines.yml)?


| Limitation               | Issue with Pipelines                                                                 |
|--------------------------|--------------------------------------------------------------------------------------|
| Not Ideal for Batch Jobs | Pipelines are designed for streaming & incremental data (Delta Live Tables).       |
| More Overhead            | Requires setting up continuous processing, even if running weekly.                  |
| Less Control Over Execution | Harder to control task sequence & handle failures gracefully.                       |

---

Conclusion

For your use case (weekly batch processing, complex ETL with multiple phases, intermediate outputs, and failure recovery), Databricks Jobs with multiple tasks & checkpoints is the best approach. üöÄ

---

##  Best Solution for Automatic Recovery from a Failed Task in Databricks Jobs


Approach: Use Task Retry + Checkpoints + Job Re-run from Failure Point

When a task fails in the middle of the workflow, the entire job should not restart from the beginning. Instead, it should resume from the failed task while preserving previously completed results.

Solution Overview

1.	Enable Automatic Task Retries ‚Üí If the failure is temporary, retries can resolve it.
2.	Use Delta Lake Checkpoints ‚Üí Ensures processed data isn‚Äôt lost.
3.	Leverage Databricks Job run from failure Feature ‚Üí If retries fail, allow manual or automatic resumption from the failed task.

---

Implementation Details

1Ô∏è‚É£ Enable Automatic Task Retries in job.yml
```yaml
jobs:
  - job:
      name: "Weekly_Batch_Processing"
      schedule:
        quartz_cron_expression: "0 0 2 ? * MON *"  # Runs every Monday at 2 AM UTC
        timezone_id: "UTC"
      tasks:
        - task:
            name: "Ingest_CSV_from_S3"
            new_cluster:
              spark_version: "13.3.x-scala2.12"
              node_type_id: "i3.xlarge"
              num_workers: 4
            notebook_task:
              notebook_path: "/Workspace/ETL/Ingest"
            libraries:
              - pypi:
                  package: "boto3"
            outputs:
              - dataset: "raw_data"
            max_retries: 3
            min_retry_interval_millis: 300000  # Wait 5 mins before retry

        - task:
            name: "Data_Cleansing"
            depends_on: ["Ingest_CSV_from_S3"]
            notebook_task:
              notebook_path: "/Workspace/ETL/Cleansing"
            outputs:
              - dataset: "clean_data"
            max_retries: 3
            min_retry_interval_millis: 300000

        - task:
            name: "Transformations"
            depends_on: ["Data_Cleansing"]
            notebook_task:
              notebook_path: "/Workspace/ETL/Transform"
            outputs:
              - dataset: "transformed_data"
            max_retries: 3

        - task:
            name: "Generate_Delta_Tables"
            depends_on: ["Transformations"]
            notebook_task:
              notebook_path: "/Workspace/ETL/Save_To_Delta"
            outputs:
              - dataset: "final_output"
            max_retries: 3

      email_notifications:
        on_failure:
          - "your_email@example.com"
```

---

2Ô∏è‚É£ Use Delta Lake Checkpoints for Data Consistency

Each step writes intermediate results to Delta tables with checkpointing, so completed tasks don‚Äôt need to rerun.

- ‚úÖ If ‚ÄúTransformations‚Äù task fails, previous steps (‚ÄúData_Cleansing‚Äù & ‚ÄúIngest_CSV_from_S3‚Äù) don‚Äôt need to rerun.
= ‚úÖ When the job resumes, it reads the last successful checkpoint and continues processing.

Example Checkpoint Code for Each Task
```python
from delta.tables import DeltaTable
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ETL").getOrCreate()

# Read previous checkpoint if exists
checkpoint_path = "s3://your-bucket/checkpoints/data_cleansing_checkpoint"

if DeltaTable.isDeltaTable(spark, checkpoint_path):
    df = spark.read.format("delta").load(checkpoint_path)
else:
    df = spark.read.format("csv").option("header", True).load("s3://your-bucket/raw_data.csv")

# Perform cleansing
df_cleaned = df.filter("some_filter_condition")

# Save progress as checkpoint
df_cleaned.write.format("delta").mode("overwrite").save(checkpoint_path)
```

---
3Ô∏è‚É£ Enable ‚ÄúRun from Failure‚Äù for Automatic Restart

Databricks Jobs UI allows you to resume failed jobs from the last failed task, without re-executing completed tasks.

How to Enable It?

-	In the Databricks Jobs UI, go to the Job Runs tab.
-	Click on the failed job instance.
-	Click ‚ÄúRun from Failure‚Äù, and Databricks will only re-run the failed task(s).

---

4Ô∏è‚É£ Additional Enhancements

‚úÖ Implement Task Timeout Handling

If a task hangs indefinitely, set a timeout to prevent blocking:
```yaml
timeout_seconds: 3600  # Fails task if it exceeds 1 hour
```

‚úÖ Use Job Parameterization for Dynamic Recovery

If you need to restart only failed tasks dynamically:
```yaml
parameters:
  - name: "start_from_task"
    default: "None"
```

Then modify your notebooks to skip tasks that were already completed.

---

üìå Final Recommendation


| Solution                           | Purpose                                                       |
|------------------------------------|---------------------------------------------------------------|
| Task Retries (max_retries)         | Automatically retries transient failures                       |
| Delta Lake Checkpointing           | Ensures no data is lost & avoids redundant processing          |
| ‚ÄúRun from Failure‚Äù Feature         | Allows job recovery from the exact failure point              |
| Task Timeout (timeout_seconds)     | Prevents long-running stuck jobs                              |
| Job Parameterization (start_from_task) | Enables selective task re-execution                           |


üîπ Best Choice ‚Üí Databricks Jobs with Checkpoints + ‚ÄúRun from Failure‚Äù Feature + Task Retries ‚úÖ
This approach ensures automation, efficiency, and fault tolerance without unnecessary reprocessing. üöÄ

---

## Implementing ‚ÄúRun from Failure‚Äù in Databricks Jobs with Scala

Databricks provides a ‚ÄúRun from Failure‚Äù feature in Jobs UI, but to implement automatic recovery programmatically, we need to:

‚úÖ Track Job Status: Store checkpoint metadata (successful tasks).
‚úÖ Skip Completed Tasks: When re-running, load the last successful state and execute only the failed tasks.


--- 
1Ô∏è‚É£ Config-Based Approach (Databricks Jobs YAML)

If using Databricks Jobs, you can configure automatic retry and resume execution from the failed task:

```yaml
jobs:
  - job:
      name: "Data_Pipeline"
      tasks:
        - task:
            name: "Ingest_CSV"
            notebook_task:
              notebook_path: "/Workspace/ETL/Ingest"
            max_retries: 3
            timeout_seconds: 3600
            outputs:
              - dataset: "raw_data"

        - task:
            name: "Transform"
            depends_on: ["Ingest_CSV"]
            notebook_task:
              notebook_path: "/Workspace/ETL/Transform"
            max_retries: 3
            outputs:
              - dataset: "transformed_data"

        - task:
            name: "Save_To_Delta"
            depends_on: ["Transform"]
            notebook_task:
              notebook_path: "/Workspace/ETL/Save"
            max_retries: 3
            outputs:
              - dataset: "final_output"
```

- üëâ If a task fails, only that task is retried, not the entire pipeline.
- üëâ You can manually click ‚ÄúRun from Failure‚Äù in Databricks Jobs UI to resume execution from the failed task.

---
2Ô∏è‚É£ Programmatic Approach (Scala)

For more granular control, store checkpoint logs and dynamically resume only failed tasks.

üìå Steps
1.	Store each successful task execution in Delta Table (Checkpoint).
2.	When a job starts, check the checkpoint log to determine which tasks to execute.
3.	Skip completed tasks and execute only failed ones.

üîπ Scala Example
```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.DataFrame

val spark = SparkSession.builder.appName("Fault-Tolerant Pipeline").getOrCreate()

// Path for checkpointing
val checkpointPath = "s3://your-bucket/checkpoints/job_status"

// Read last saved state
def getCheckpoint(): DataFrame = {
    try {
        spark.read.format("delta").load(checkpointPath)
    } catch {
        case _: Exception => spark.createDataFrame(Seq.empty[(String, String)]).toDF("task_name", "status")
    }
}

// Update checkpoint when a task is successfully completed
def updateCheckpoint(task: String): Unit = {
    val df = Seq((task, "success")).toDF("task_name", "status")
    df.write.format("delta").mode("append").save(checkpointPath)
}

// Load checkpoint
val checkpointDF = getCheckpoint()
val completedTasks = checkpointDF.select("task_name").as[String].collect().toSet

// Task 1: Ingest CSV
if (!completedTasks.contains("Ingest_CSV")) {
    println("Executing: Ingest_CSV")
    val df = spark.read.option("header", true).csv("s3://your-bucket/raw_data.csv")
    df.write.format("delta").mode("overwrite").save("s3://your-bucket/processed_data")
    updateCheckpoint("Ingest_CSV") // Save progress
}

// Task 2: Transform
if (!completedTasks.contains("Transform")) {
    println("Executing: Transform")
    val df = spark.read.format("delta").load("s3://your-bucket/processed_data")
    val transformedDF = df.withColumn("new_col", lit("processed"))
    transformedDF.write.format("delta").mode("overwrite").save("s3://your-bucket/transformed_data")
    updateCheckpoint("Transform") // Save progress
}

// Task 3: Save to Delta
if (!completedTasks.contains("Save_To_Delta")) {
    println("Executing: Save_To_Delta")
    val df = spark.read.format("delta").load("s3://your-bucket/transformed_data")
    df.write.format("delta").mode("overwrite").save("s3://your-bucket/final_output")
    updateCheckpoint("Save_To_Delta") // Save progress
}

println("Pipeline Completed Successfully!")
```

---

üìå Final Thoughts


| Approach                                      | How It Works                                                      | Best For                                        |
|-----------------------------------------------|-------------------------------------------------------------------|-------------------------------------------------|
| Databricks Jobs ‚ÄúRun from Failure‚Äù (UI Feature) | Resumes execution from the failed task                            | Simple workflows with minimal custom logic      |
| Databricks Jobs YAML (max_retries + depends_on) | Each task retries automatically, and dependencies ensure correct sequencing | Moderate complexity workflows                  |
| Scala-Based Checkpointing (Custom Logic)      | Skips completed tasks dynamically and only reprocesses failed ones | Complex workflows with strict failure recovery requirements |

üöÄ Recommendation

-	For basic automation ‚Üí Use Databricks Jobs UI (‚ÄúRun from Failure‚Äù).
-	For structured jobs ‚Üí Use Databricks Job YAML (max_retries).
-	For complex fault-tolerant pipelines ‚Üí Use Scala-based checkpointing.

---

## Enhancing Fault-Tolerant Recovery in Databricks Jobs


To improve flexibility and automation, let‚Äôs enhance the Scala-based checkpointing approach with:

-	‚úÖ Dynamic Task Execution ‚Äì Reads task status dynamically from Delta Table.
-	‚úÖ Failure Logging & Retry Mechanism ‚Äì Automatically retries failed tasks.
-	‚úÖ Parameterization ‚Äì Allows configurable execution for debugging or partial reprocessing.

---

1Ô∏è‚É£ Configurable Checkpointing with Parameters

We introduce a configurable ‚Äúmode‚Äù parameter, so users can:

-	Run all tasks (mode = "full")
-	Resume from failure (mode = "resume")
-	Reprocess specific tasks (mode = "reprocess", taskList = ["task1", "task2"])

üîπ Scala Implementation
```scala
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import scala.util.Try

val spark = SparkSession.builder.appName("Fault-Tolerant Pipeline").getOrCreate()

// Configurable Parameters
val checkpointPath = "s3://your-bucket/checkpoints/job_status"
val mode = dbutils.widgets.get("mode") // "full", "resume", or "reprocess"
val taskList = dbutils.widgets.get("taskList").split(",").map(_.trim).toSet

// Function to read checkpoint data
def getCheckpoint(): DataFrame = {
    Try(spark.read.format("delta").load(checkpointPath))
      .getOrElse(spark.createDataFrame(Seq.empty[(String, String)]).toDF("task_name", "status"))
}

// Function to update checkpoint after task completion
def updateCheckpoint(task: String): Unit = {
    val df = Seq((task, "success")).toDF("task_name", "status")
    df.write.format("delta").mode("append").save(checkpointPath)
}

// Load existing checkpoint data
val checkpointDF = getCheckpoint()
val completedTasks = checkpointDF.select("task_name").as[String].collect().toSet

// Function to check if a task should run based on mode
def shouldRun(task: String): Boolean = {
    mode match {
        case "full" => true  // Run all tasks
        case "resume" => !completedTasks.contains(task)  // Skip completed tasks
        case "reprocess" => taskList.contains(task)  // Run only specified tasks
        case _ => throw new IllegalArgumentException(s"Unknown mode: $mode")
    }
}

// Task 1: Ingest Data
if (shouldRun("Ingest_CSV")) {
    println("Executing: Ingest_CSV")
    val df = spark.read.option("header", true).csv("s3://your-bucket/raw_data.csv")
    df.write.format("delta").mode("overwrite").save("s3://your-bucket/processed_data")
    updateCheckpoint("Ingest_CSV")
}

// Task 2: Transform Data
if (shouldRun("Transform")) {
    println("Executing: Transform")
    val df = spark.read.format("delta").load("s3://your-bucket/processed_data")
    val transformedDF = df.withColumn("new_col", lit("processed"))
    transformedDF.write.format("delta").mode("overwrite").save("s3://your-bucket/transformed_data")
    updateCheckpoint("Transform")
}

// Task 3: Save to Delta
if (shouldRun("Save_To_Delta")) {
    println("Executing: Save_To_Delta")
    val df = spark.read.format("delta").load("s3://your-bucket/transformed_data")
    df.write.format("delta").mode("overwrite").save("s3://your-bucket/final_output")
    updateCheckpoint("Save_To_Delta")
}

println("Pipeline Completed Successfully!")
```

---

2Ô∏è‚É£ Configuring Job Parameters in Databricks

To enable dynamic execution control, define Databricks Job parameters:
```yaml
jobs:
  - job:
      name: "Fault_Tolerant_Pipeline"
      tasks:
        - task:
            name: "ETL_Pipeline"
            notebook_task:
              notebook_path: "/Workspace/ETL/Pipeline"
            parameters:
              - mode: "resume"   # Options: full, resume, reprocess
              - taskList: ""  # Example: "Transform,Save_To_Delta"
            max_retries: 3
            timeout_seconds: 3600
```

---

3Ô∏è‚É£ How to Run the Pipeline


 1Ô∏è‚É£ Run the Full Pipeline (All Tasks)

```shell
databricks jobs run-now --job-id <job_id> --notebook-params '{"mode": "full"}'
```

  2Ô∏è‚É£ Resume from the Last Failed Task
```shell
databricks jobs run-now --job-id <job_id> --notebook-params '{"mode": "resume"}'
```

 3Ô∏è‚É£ Reprocess Only Specific Tasks

```shell
databricks jobs run-now --job-id <job_id> --notebook-params '{"mode": "reprocess", "taskList": "Transform,Save_To_Delta"}'
```

---

üöÄ Final Enhancements & Recommendations


| Enhancement                                   | Benefit                                           |
|-----------------------------------------------|---------------------------------------------------|
| Parameterizing Task Execution (mode, taskList) | Allows flexible execution control                |
| Skipping Successful Tasks Automatically (resume mode) | Saves time & costs by re-executing only failed tasks |
| Logging Checkpoints in Delta Table           | Enables fault-tolerant execution                 |
| Using Databricks jobs run-now with Parameters | Supports automation & debugging                   |

This method ensures automatic recovery without reprocessing successful tasks and allows targeted reprocessing when needed. üöÄ


---

##  Enhancing Monitoring & Logging for Databricks Fault-Tolerant Pipeline


To improve observability and real-time monitoring, let‚Äôs integrate:
- ‚úÖ Enhanced Logging ‚Äì Track execution status in logs.
- ‚úÖ Error Handling & Alerts ‚Äì Send failure alerts to Slack or email.
- ‚úÖ Datadog / CloudWatch Integration ‚Äì Push logs to external monitoring tools.


---
1Ô∏è‚É£ Enhanced Logging with Databricks Notebook Logs

Modify the pipeline to log execution status and capture errors.

üîπ Updated Scala Implementation with Logging & Error Handling
```scala
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import scala.util.{Try, Failure, Success}

// Initialize Spark Session
val spark = SparkSession.builder.appName("Fault-Tolerant Pipeline").getOrCreate()

// Configuration
val checkpointPath = "s3://your-bucket/checkpoints/job_status"
val mode = dbutils.widgets.get("mode") // "full", "resume", "reprocess"
val taskList = dbutils.widgets.get("taskList").split(",").map(_.trim).toSet

// Logging Function
def logStatus(task: String, status: String, errorMsg: Option[String] = None): Unit = {
    val logDF = Seq((task, status, errorMsg.getOrElse("None"), current_timestamp()))
      .toDF("task_name", "status", "error_message", "timestamp")
    logDF.write.format("delta").mode("append").save(checkpointPath)

    // Send alert on failure
    if (status == "failed") {
        dbutils.notebook.exit(s"Task $task failed: ${errorMsg.getOrElse("Unknown error")}")
    }
}

// Get existing checkpoint data
def getCheckpoint(): DataFrame = {
    Try(spark.read.format("delta").load(checkpointPath))
      .getOrElse(spark.createDataFrame(Seq.empty[(String, String, String)]).toDF("task_name", "status", "error_message"))
}

// Determine which tasks to run
val checkpointDF = getCheckpoint()
val completedTasks = checkpointDF.filter("status = 'success'").select("task_name").as[String].collect().toSet

def shouldRun(task: String): Boolean = mode match {
    case "full" => true
    case "resume" => !completedTasks.contains(task)
    case "reprocess" => taskList.contains(task)
    case _ => throw new IllegalArgumentException(s"Unknown mode: $mode")
}

// Execute tasks with error handling
def runTask(taskName: String, taskLogic: => Unit): Unit = {
    if (shouldRun(taskName)) {
        println(s"Executing: $taskName")
        Try(taskLogic) match {
            case Success(_) => logStatus(taskName, "success")
            case Failure(ex) => logStatus(taskName, "failed", Some(ex.getMessage))
        }
    } else {
        println(s"Skipping: $taskName (Already completed)")
    }
}

// Task Execution
runTask("Ingest_CSV", {
    val df = spark.read.option("header", true).csv("s3://your-bucket/raw_data.csv")
    df.write.format("delta").mode("overwrite").save("s3://your-bucket/processed_data")
})

runTask("Transform", {
    val df = spark.read.format("delta").load("s3://your-bucket/processed_data")
    val transformedDF = df.withColumn("new_col", lit("processed"))
    transformedDF.write.format("delta").mode("overwrite").save("s3://your-bucket/transformed_data")
})

runTask("Save_To_Delta", {
    val df = spark.read.format("delta").load("s3://your-bucket/transformed_data")
    df.write.format("delta").mode("overwrite").save("s3://your-bucket/final_output")
})

println("Pipeline Completed Successfully!")
```

---

2Ô∏è‚É£ Sending Alerts on Failure (Slack / Email)

We can send alerts using Slack Webhooks or Databricks Jobs Alerts.

üîπ Databricks Alerts (Easier Approach)

1.	Go to Databricks UI ‚Üí Click on your Job
2.	Navigate to ‚ÄúAlerts‚Äù ‚Üí ‚ÄúAdd Alert‚Äù
3.	Set conditions:
-	Trigger on failure
-	Send to email or Slack Webhook

---

3Ô∏è‚É£ Integrating Logs with Datadog or CloudWatch

To track execution in Datadog or AWS CloudWatch, we can push logs.

üîπ Push Logs to Datadog
```scala
import scalaj.http.Http

def sendToDatadog(logMessage: String): Unit = {
    val apiKey = "your-datadog-api-key"
    val url = s"https://http-intake.logs.datadoghq.com/api/v2/logs?api_key=$apiKey"
    
    val payload = s"""{
      "ddsource": "databricks",
      "service": "etl-pipeline",
      "message": "$logMessage",
      "status": "info"
    }"""

    Http(url).postData(payload).header("Content-Type", "application/json").asString
}

// Example usage
sendToDatadog("Pipeline execution started")
```
  
---

4Ô∏è‚É£ Running the Job with Failure Recovery

Databricks YAML Configuration

```yaml
jobs:
  - job:
      name: "Fault_Tolerant_Pipeline"
      tasks:
        - task:
            name: "ETL_Pipeline"
            notebook_task:
              notebook_path: "/Workspace/ETL/Pipeline"
            parameters:
              - mode: "resume"
              - taskList: ""
            max_retries: 3
            timeout_seconds: 3600
            email_notifications:
              on_failure:
                - your_email@example.com
```

üöÄ Key Benefits of This Approach

| Feature                           | Benefit                                         |
|-----------------------------------|-------------------------------------------------|
| Checkpointing with Delta Table    | Enables automatic failure recovery             |
| Skip Successful Tasks (resume Mode) | Saves time & compute costs                     |
| Slack/Email Alerts on Failure     | Immediate failure notifications                |
| Datadog/CloudWatch Integration    | Centralized logging & monitoring               |
| Error Handling with Try-Catch     | Prevents silent failures                       |

This setup ensures a fully automated, fault-tolerant, and monitored ETL pipeline in Databricks.

---

## Automating Databricks Pipeline Trigger on S3 File Arrival

To automate the pipeline when new data arrives in S3, we can use:
- ‚úÖ AWS S3 Event Notifications ‚Üí Detect new files.
- ‚úÖ AWS Lambda Function ‚Üí Trigger Databricks job.
- ‚úÖ AWS EventBridge Scheduler ‚Üí (Optional) Fallback scheduler.

---

1Ô∏è‚É£ Enable S3 Event Notifications for File Arrival

S3 can send an event when new files are uploaded.

üîπ Steps to Configure S3 Event Notifications

1.	Go to AWS S3 Console
2.	Select your S3 bucket (e.g., s3://your-bucket/raw-data/)
3.	Navigate to Properties ‚Üí Event Notifications
4.	Click ‚ÄúCreate Event Notification‚Äù
-	Name: NewFileArrival
-	Event Type: PUT (Object Created)
-	Prefix: raw-data/
-	Destination: AWS Lambda

---

2Ô∏è‚É£ Create an AWS Lambda Function to Trigger Databricks Job

The Lambda function will start the Databricks workflow job whenever new data arrives.

üîπ Python Lambda Function (S3 ‚Üí Databricks)
```python
import json
import boto3
import requests

# Databricks Configurations
DATABRICKS_INSTANCE = "https://<your-databricks-instance>"
DATABRICKS_TOKEN = "your-databricks-token"
JOB_ID = "123456"  # Replace with actual job ID

def lambda_handler(event, context):
    try:
        # Extract the file details
        records = event.get("Records", [])
        if not records:
            print("No new files detected.")
            return {"statusCode": 200, "body": "No new files detected."}

        for record in records:
            s3_bucket = record["s3"]["bucket"]["name"]
            file_path = record["s3"]["object"]["key"]
            print(f"New file detected: {s3_bucket}/{file_path}")

            # Trigger Databricks Job
            job_url = f"{DATABRICKS_INSTANCE}/api/2.1/jobs/run-now"
            headers = {"Authorization": f"Bearer {DATABRICKS_TOKEN}", "Content-Type": "application/json"}
            payload = {"job_id": JOB_ID, "notebook_params": {"mode": "resume"}}

            response = requests.post(job_url, headers=headers, data=json.dumps(payload))
            if response.status_code == 200:
                print("Databricks job triggered successfully.")
                return {"statusCode": 200, "body": "Job triggered successfully"}
            else:
                print(f"Error triggering job: {response.text}")
                return {"statusCode": 500, "body": f"Job trigger failed: {response.text}"}

    except Exception as e:
        print(f"Error: {str(e)}")
        return {"statusCode": 500, "body": f"Lambda execution failed: {str(e)}"}
```

---


3Ô∏è‚É£ (Optional) Fallback: Schedule a Databricks Job via AWS EventBridge

If S3 event notifications fail, AWS EventBridge can trigger the Databricks job daily/hourly.

üîπ Steps to Configure AWS EventBridge Scheduler

1.	Go to AWS EventBridge Console
2.	Click ‚ÄúCreate Rule‚Äù
3.	Name: Trigger-Databricks-Job
4.	Event Source: Schedule (Rate or Cron Expression)
-	Example: cron(0 6 * * ? *) ‚Üí Runs daily at 6 AM
5.	Target: AWS Lambda (Select the Lambda function created above)


---

4Ô∏è‚É£ Databricks Job Configuration (job.yml)

Modify the Databricks job to process only new data and resume from failure.

```yaml
jobs:
  - job:
      name: "Automated_S3_Triggered_Pipeline"
      tasks:
        - task:
            name: "ETL_Pipeline"
            notebook_task:
              notebook_path: "/Workspace/ETL/Pipeline"
            parameters:
              - mode: "resume"
            max_retries: 3
            timeout_seconds: 3600
            email_notifications:
              on_failure:
                - your_email@example.com
```


üî• Benefits of This Setup


| Feature                         | Benefit                                              |
|---------------------------------|------------------------------------------------------|
| Event-Driven Execution          | No manual triggers, runs automatically on new data arrival |
| Resume from Failure             | Saves compute cost, only runs failed tasks           |
| AWS EventBridge Fallback        | Ensures job runs even if S3 events fail              |
| Automatic Notifications         | Alerts via Slack/Email if failure occurs            |


This fully automates your Databricks pipeline using S3 event triggers, Lambda, and Databricks Jobs. üöÄ

---

## 