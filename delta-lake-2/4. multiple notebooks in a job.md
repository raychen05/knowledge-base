
### Running Multiple Notebooks in a Databricks Job with Dependencies


To efficiently orchestrate multiple Databricks notebooks within a single job, ensuring some run in parallel and others sequentially, follow these steps:

--- 

ğŸ“Œ 1. Approach for Managing Workflow Sequence

-	Parallel Execution: Notebooks that donâ€™t depend on each other can run in parallel.
-	Sequential Execution: If a notebookâ€™s output feeds into another, it must be executed after the dependent notebook.
-	Triggering Logic: Use dbutils.notebook.run() for sequential runs & Jobs API for parallel execution.


---

ğŸ“Œ 2. Configuring Job Workflow in Databricks UI


Databricks Workflows (Jobs) allow defining dependencies:
1.	Go to Workflows > Create Job.
2.	Add tasks (notebooks) and define dependencies.
3.	Set some tasks to run in parallel and others sequentially.


---

ğŸ“Œ 3. Example: Databricks Job Workflow Configuration

```scala
    import com.databricks.workflow._
    val job = Workflow.create("DataPipeline")

    // Step 1: Load Data (Runs First)
    val loadData = job.addTask("Load_Data", "LoadDataNotebook")

    // Step 2: Process Data (Runs after Load)
    val processData = job.addTask("Process_Data", "ProcessDataNotebook")
    processData.dependsOn(loadData)

    // Step 3: Analytics & Report (Can Run in Parallel)
    val analyticsTask = job.addTask("Run_Analytics", "AnalyticsNotebook")
    val reportTask = job.addTask("Generate_Report", "ReportNotebook")
    analyticsTask.dependsOn(processData)
    reportTask.dependsOn(processData)

    // Step 4: Publish Results (Runs Last)
    val publishTask = job.addTask("Publish_Results", "PublishNotebook")
    publishTask.dependsOn(analyticsTask)
    publishTask.dependsOn(reportTask)

    // Submit job
    job.submit()
```

---

ğŸ“Œ 4. Running Notebooks Sequentially in Scala

When one notebookâ€™s output is needed in another, use dbutils.notebook.run():

Example: Notebook Chaining (Sequential Execution)

```scala
// Run Load Data Notebook
val loadDataOutput = dbutils.notebook.run("LoadDataNotebook", timeoutSeconds = 3600)

// Run Process Data Notebook using Load Data output
val processDataOutput = dbutils.notebook.run("ProcessDataNotebook", timeoutSeconds = 3600, Map("inputPath" -> loadDataOutput))

// Run Analytics Notebook using Process Data output
val analyticsOutput = dbutils.notebook.run("AnalyticsNotebook", timeoutSeconds = 3600, Map("inputPath" -> processDataOutput))

// Run Report Notebook using Process Data output
val reportOutput = dbutils.notebook.run("ReportNotebook", timeoutSeconds = 3600, Map("inputPath" -> processDataOutput))

// Run Publish Results Notebook after analytics & report
val publishOutput = dbutils.notebook.run("PublishNotebook", timeoutSeconds = 3600, Map("analyticsPath" -> analyticsOutput, "reportPath" -> reportOutput))

```


---

ğŸ“Œ 5. Running Notebooks in Parallel

For parallel execution, use Databricks multitasking:

```scala
import scala.concurrent.Future
import scala.concurrent.ExecutionContext.Implicits.global

val analyticsFuture = Future {
  dbutils.notebook.run("AnalyticsNotebook", timeoutSeconds = 3600, Map("inputPath" -> processDataOutput))
}

val reportFuture = Future {
  dbutils.notebook.run("ReportNotebook", timeoutSeconds = 3600, Map("inputPath" -> processDataOutput))
}

// Wait for both parallel tasks to finish
val results = Seq(analyticsFuture, reportFuture).map(_.recover { case e => e.getMessage })
val allResults = Future.sequence(results)
allResults.foreach(println)

```

âœ… Effect: AnalyticsNotebook and ReportNotebook run in parallel after ProcessDataNotebook.


---

ğŸ“Œ 6. Reading & Writing Data from Delta Lake & S3


Reading from Delta Lake Catalog
```scala
val df = spark.read.format("delta").table("catalog.schema.table_name")
```

Writing to Delta Lake
```scala
df.write.format("delta").mode("overwrite").saveAsTable("catalog.schema.output_table")
```

Reading from S3
```scala
val df = spark.read.format("parquet").load("s3://your-bucket/path")
```

Writing to S3
```scala
df.write.format("parquet").mode("overwrite").save("s3://your-bucket/output-path")
```

---

ğŸ“Œ 7. Summary


|Requirement	|Approach|
|-----------------------------------|---------------------------------------|
|Parallel Execution	|Use Databricks Job UI or Scala Futures|
|Sequential Execution	|Use dbutils.notebook.run()|
|Trigger Next Task	|Pass outputs as parameters|
|Read/Write from Delta	|spark.read.table() / df.write.saveAsTable()|
|Read/Write from S3	|spark.read.load() / df.write.save()|

ğŸš€ This approach optimizes Databricks pipeline execution for performance & dependency management! ğŸš€


---

### Performnce of Parallel Execution of Multiple Notebooks in Databricks Cluster

When multiple notebooks are configured to run in parallel in a Databricks job, they will share the same Databricks cluster. However, whether they actually execute in parallel depends on cluster resource allocation and task scheduling.

---

ğŸ“Œ 1. How Does Databricks Handle Parallel Notebook Execution?

**Multiple Notebooks Assigned to the Same Cluster:**
-	If enough available executors exist, Databricks truly runs them in parallel.
-	If not enough resources, tasks may be queued and executed sequentially due to scheduling limits.

**Parallelism Depends on Cluster Size:****
-	If the cluster has sufficient workers, the notebooks can execute at the same time.
-	If the cluster is overloaded, Spark will schedule jobs sequentially.


---
ğŸ“Œ 2. How to Ensure Efficient Parallel Execution?

To maximize parallelism and efficient resource utilization, follow these steps:

âœ… Step 1: Use a Cluster with Sufficient Resources

-	Choose a larger cluster with more nodes and cores.
-	Example optimal cluster for parallel execution:
	-	Worker Type: i3.4xlarge (16 vCPUs, 122GB RAM)
	-	Number of Workers: 8+ nodes
	-	Driver Type: Same as worker (i3.4xlarge)
- Databricks automatically distributes tasks across available executors.

---

âœ… Step 2: Configure Notebooks to Run in Parallel in Databricks Job UI

1.	Go to Workflows â†’ Jobs.
2.	Click Create Job.
3.	Add multiple tasks (notebooks) and remove dependencies so they run in parallel.
4.	Set the same shared cluster for all tasks.

âœ”ï¸ This ensures tasks start at the same time, using shared cluster resources efficiently.


---

âœ… Step 3: Manually Trigger Parallel Execution in Scala

You can trigger multiple notebooks in parallel using Futures in Scala:
```scala
import scala.concurrent.Future
import scala.concurrent.ExecutionContext.Implicits.global

// Run multiple notebooks in parallel
val future1 = Future { dbutils.notebook.run("Notebook_A", 3600) }
val future2 = Future { dbutils.notebook.run("Notebook_B", 3600) }
val future3 = Future { dbutils.notebook.run("Notebook_C", 3600) }

// Wait for all tasks to complete
val results = Seq(future1, future2, future3).map(_.recover { case e => e.getMessage })
val allResults = Future.sequence(results)
allResults.foreach(println)

```

âœ”ï¸ Effect: The three notebooks (Notebook_A, Notebook_B, Notebook_C) execute truly in parallel if the cluster has enough resources.

---

âœ… Step 4: Optimize Spark Configuration for Parallel Execution

Modify Spark settings in spark.conf to allow better parallelism:

```scala
spark.conf.set("spark.databricks.io.cache.enabled", "true")
spark.conf.set("spark.sql.shuffle.partitions", "200")  // Increase for parallelism
spark.conf.set("spark.default.parallelism", "200")
spark.conf.set("spark.task.cpus", "2") // Allows more tasks to run concurrently

```

---
ğŸ“Œ 3. Key Takeaways

|Factor|	Impact on Parallel Execution|
|-----------------------------------|---------------------------------------|
|Cluster Size|	More nodes â†’ More parallel execution|
|Executor Availability|	If full, tasks queue and run sequentially|
|Job UI Setup|	Removing dependencies allows parallel execution|
|Scala Futures|	Forces multiple notebooks to start at once|
|Spark Config Optimization|	Ensures better task distribution|


---

ğŸš€ Conclusion

- âœ… Databricks allows true parallel execution if the cluster has sufficient resources.
- âœ… Optimizing cluster size, job dependencies, and Spark configurations ensures efficient parallel execution.
- âœ… If the cluster is overloaded, Spark automatically queues tasks, causing sequential execution even if the job is configured for parallelism.


ğŸ”¹ Best Practice: If multiple notebooks need to run in parallel, use a large enough cluster and set Spark configurations correctly. ğŸš€