## How Delta Lake Type-1 SCD works


Below is a clear, practical explanation of how Delta Lake SCD Type-1 handles INSERT, UPDATE, and DELETE operations, including:

- What data looks like before vs after each operation
- MERGE logic
- Delta/CDF behavior if you need to detect changes
- How Type-1 differs from Type-2

---

### ğŸ¯ What is SCD Type-1?

SCD Type-1 keeps only the latest version of each row.

So Type-1 overwrites old data:
- Insert â†’ add the row
- Update â†’ overwrite existing row
- Delete â†’ remove the row

- No history is kept.
- No effective_start, effective_end, or is_current.

---

### ğŸ“¦ Typical Type-1 Table Schema

| Column | Meaning |
|--------|---------|
| `sp_id` | Primary key |
| `name` | attribute |
| `city` | attribute |
| `age` | attribute |

(any other attributesâ€¦)	

This is a simple, current-state table.

---

### ğŸŸ¦ Example Source Change Events

Letâ€™s say the business key is:

```css
sp_id
```

Initial data:

```yaml
sp_id | name  | city      | age
------+------+-----------+-----
101   | Alice | New York  | 29
102   | Bob   | Boston    | 31
```

---

### ğŸ”· 1. INSERT Example

New record arrives:
```ini
sp_id = 103
```

Source event:

```yaml
sp_id | name    | city | age | _change_type
------+---------+------+-----+-------------
103   | Charlie | LA   | 40  | insert
```

Type-1 result (table after operation):

```yaml
101 | Alice   | New York | 29
102 | Bob     | Boston   | 31
103 | Charlie | LA       | 40   <-- newly inserted
```

---

### ğŸ”· 2. UPDATE Example

Employee 101 moves from New York â†’ Chicago.

Source event:

```yaml
sp_id | name  | city    | age | _change_type
------+-------+---------+-----+--------------
101   | Alice | Chicago | 29  | update_postimage
```

Type-1 result after update:

```yaml
sp_id | name  | city     | age
------+-------+----------+----
101   | Alice | Chicago  | 29   <-- overwritten
102   | Bob   | Boston   | 31
103   | Charlie | LA     | 40
```

Old value (â€œNew York") is gone.

---

### ğŸ”· 3. DELETE Example

Record 102 is deleted.

Source event:

```yaml
sp_id | _change_type
------+--------------
102   | delete
```

Type-1 result:

```yaml
sp_id | name    | city     | age
------+---------+----------+----
101   | Alice   | Chicago  | 29
103   | Charlie | LA       | 40
```

Row has been physically removed.

---

### ğŸ’¡ Summary of Type-1 Behavior
| Change | Type-1 Action | History? |
|--------|---------------|----------|
| INSERT | Add new row | âŒ No |
| UPDATE | Overwrite old record | âŒ No |
| DELETE | Remove record | âŒ No |

---

### ğŸ§¨ How do we detect insert/update/delete if upstream is a Delta table?

You have two choices:

#### âœ” Option A â€” If you DO NOT use CDF

You CANNOT detect updates or deletes.

You can only:
- read the entire table each day
- MERGE against your target

This is simple but not incremental.


#### âœ” Option B â€” If you DO use CDF (Recommended)

CDF tells you:

| `_change_type` | Meaning |
|----------------|---------|
| `insert` | new row |
| `update_postimage` | latest updated row |
| `delete` | logical delete |

Example CDF output:

```yaml
sp_id | name  | city     | age | _change_type
------+-------+----------+-----+--------------
101   | Alice | New York | 29  | update_preimage
101   | Alice | Chicago  | 29  | update_postimage
102   | Bob   | Boston   | 31  | delete
103   | Charlie | LA     | 40  | insert
```

This is perfect for Type-1 MERGE logic.

---

### ğŸš€ SCD Type-1 MERGE Example (Delta Lake)

This is the standard pattern:

```sql
MERGE INTO target t
USING source s
ON t.sp_id = s.sp_id
WHEN MATCHED AND s._change_type = 'delete' THEN DELETE
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *
```

Equivalent PySpark code:

```python
source = spark.readStream.format("delta") \
    .option("readChangeData", "true") \
    .table("catalog.schema.source_table")

merge_condition = "t.sp_id = s.sp_id"

(
    target.alias("t")
        .merge(source.alias("s"), merge_condition)
        .whenMatchedDelete(condition="s._change_type = 'delete'")
        .whenMatchedUpdateAll(condition="s._change_type IN ('update_postimage')")
        .whenNotMatchedInsertAll(condition="s._change_type = 'insert'")
        .execute()
)
```

---

### ğŸ¯ Type-1 vs Type-2 (Important)

| Feature | Type-1 | Type-2 |
|---------|--------|--------|
| Keeps history | âŒ No | âœ” Yes |
| Update | Overwrite | New row + close old row |
| Detect deletes? | Only via CDF | via CDF |
| Table size | small | grows over time |
| Query complexity | simple | more complex |


If you want â€”
I can generate:
    - âœ” A complete SCD Type-1 DLT pipeline
(using Unity Catalog + CDF)
    - âœ” Or a non-CDF full-refresh daily Type-1 pipeline

Just tell me:
- source table name
- target table name
- primary key

I will produce a ready-to-run DLT pipeline file.



---



## How Delta Lake Type-2 SCD works


Below is a clear, practical, end-to-end explanation of how Delta Lake Type-2 SCD works, including:

1. How inserts, updates, and deletes are represented in the table
2. A realistic schema
3. Sample data before and after each operation
4. How the DLT pipeline (or plain Delta Lake) should process eachâ€¨â€” with and without CDF

This is the cleanest explanation you will find.

---

### ğŸ”· 1. Typical SCD Type-2 Table Schema

A Type-2 table keeps full history of changes by creating new versions of rows.

**Common columns:**

| Column | Purpose |
|--------|---------|
| `business_key` | Natural key (e.g., `sp_id`) |
| `attr1`/`attr2`/... | Source data attributes |
| `effective_start` | When this version becomes valid |
| `effective_end` | When this version stops being valid |
| `is_current` | Whether row is the latest version (true/false) |
| `op` (optional) | Insert / Update / Delete label if you want extra clarity |

ğŸ“Œ Logical meaning:
- Insert â†’ Add a new row with is_current = true, no old row.
- Update â†’
    - Close the old row (is_current = false, set effective_end)
    - Add a new row (is_current = true)
- Delete â†’
    - Close the old row (is_current = false)
    - Optionally add a "tombstone row" if required.

---

### ğŸ”· 2. How the data looks (example)

ğŸ”¹ Initial table (before any change)

sp_id | name   | city      | effective_start     | effective_end | is_current
------+--------+-----------+---------------------+---------------+-----------
101   | Alice  | New York  | 2024-01-01 00:00:00 | null          | true
102   | Bob    | Boston    | 2024-01-01 00:00:00 | null          | true

---

### ğŸ”· 3. INSERT example

New employee:
```ini
sp_id = 103, name = Charlie
```

We add a single new row:

sp_id | name     | city | effective_start         | effective_end | is_current
------+----------+------+--------------------------+---------------+-----------
103   | Charlie  | LA   | 2024-02-01 00:00:00      | null          | true

---

### ğŸ”· 4. UPDATE example (Type-2 change)

Employee 101 moves from New York â†’ Chicago

#### Step 1 â€” Close old version

```yaml
101 | Alice | New York | 2024-01-01 | 2024-03-01 00:00:00 | false
```

#### Step 2 â€” Insert new version

```yaml
101 | Alice | Chicago | 2024-03-01 | null | true
```


Resulting table:

```yaml
sp_id | name  | city      | effective_start | effective_end       | is_current
------+-------+-----------+-----------------+----------------------+-----------
101   | Alice | New York  | 2024-01-01      | 2024-03-01 00:00:00 | false
101   | Alice | Chicago   | 2024-03-01      | null                | true
102   | Bob   | Boston    | 2024-01-01      | null                | true
103   | Charlie | LA      | 2024-02-01      | null                | true
```

---

### ğŸ”· 5. DELETE example

Employee 103 is removed (deleted).

#### Step 1 â€” Close current record

```yaml
103 | Charlie | LA | 2024-02-01 | 2024-04-15 00:00:00 | false
```

#### (Optional) Step 2 â€” Add tombstone

Some business rules require this; some don't.
```yaml
103 | null | null | 2024-04-15 | null | true
```

---

### ğŸ”· 6. How to detect Insert / Update / Delete


#### Option A â€” Using CDF (Recommended)

CDF automatically gives you:
| `_change_type` | Meaning |
|----------------|---------|
| `insert` | new row |
| `update_preimage` | old row before update |
| `update_postimage` | new row after update |
| `delete` | deleted row |


Sample CDF data


```yaml
sp_id | name  | city    | _change_type        | _commit_timestamp
------+-------+----------+---------------------+---------------------
101   | Alice | New York | update_preimage     | 2024-03-01 00:00:00
101   | Alice | Chicago  | update_postimage    | 2024-03-01 00:00:00
103   | Charlie | LA     | delete              | 2024-04-15 00:00:00
102   | Bob   | Boston   | insert              | 2024-03-05 00:00:00

```


This tells you exactly:
- which rows were updated (and what changed)
- which were deleted
- which were inserted

---

### ğŸ”· 7. DLT SCD Type-2 logic using CDF

#### Step 1 â€” Read CDF

```python
@dlt.view
def spmaster_changes():
    return (
        spark.read.format("delta")
        .option("readChangeData", "true")
        .option("startingVersion", "latest")
        .load("/path/to/upstream/table")
    )
```


#### Step 2 â€” Handle SCD2

```python
@dlt.table
def spmaster_scd2():
    source = dlt.read_stream("spmaster_changes")

    inserts = source.filter("_change_type = 'insert'")
    updates_new = source.filter("_change_type = 'update_postimage'")
    deletes = source.filter("_change_type = 'delete'")

    # Then MERGE using SCD2 logic
```

---

### ğŸ”· 8. If you DO NOT use CDF

- You cannot see updates or deletes.
- You only see new files, not row-level changes.

That means SCD2 cannot be built from streaming input without CDF.

---

### ğŸ”¥ Summary (Important)

âœ” If you want a Type-2 SCD table
    - â†’ You MUST use Change Data Feed (CDF).
    - Otherwise you cannot detect update or delete.

âœ” SCD2 representation
  - update â†’ close old row, insert new row
  - delete â†’ close old row
  - insert â†’ insert a new row

âœ” CDF tells you exactly what changed
(insert, update_preimage, update_postimage, delete)

#### Want a complete ready-to-run DLT SCD2 pipeline?

If you give me your table name + primary key, I can generate the full working DLT code:
- CDF reader
- SCD2 merge logic
- schema management
- daily scheduling

Just send:
- source table name
- primary key column(s)
- attribute columns to track.


---


â— You CANNOT point DLT directly to the physical S3 location of a Unity Catalogâ€“managed table.

- Unity Catalog protects managed table storage.
- DLT is not allowed to read the internal UC path with .load().

This is why the error mentions:

__unitystorage/catalogs/<catalog_id>/tables/<table_id>

That path is UCâ€™s internal storage layout â†’ off limits for .load().


### âœ… HERE IS THE CORRECT SOLUTION

ğŸ”¥ When your table is a Unity Catalog managed table, you MUST read it by table name, not physical S3 path.
But you still need CDF, so you must use:

```python
spark.readStream.table("catalog.schema.table")
# not .load().
```

Because CDF works with streaming table reads in Unity Catalog.

---

### ğŸš€ Correct DLT pattern for UC managed tables + CDF

âœ” Enable CDF:
```sql
ALTER TABLE ag_content_ims_dev.acs_entity.d_spmaster
SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
```

âœ” DLT CDF reader (this is correct for UC tables)

```python
@dlt.view
def spmaster_changes():
    return (
        spark.readStream
             .format("delta")
             .option("readChangeData", "true")
             .table("ag_content_ims_dev.acs_entity.d_spmaster")
    )
```


â—DO NOT use .load("s3://.../__unitystorage/...")

Unity Catalog will block it.

---

### â­ Why this works

Unity Catalog allows:
- table name access,
- CDF streaming,
- incremental reading,

but blocks direct physical path reads for managed tables.

---

### ğŸŸ¥ Summary (Important)


| Approach | Allowed? | Why |
|----------|----------|-----|
| `.load("s3://â€¦/__unitystorage/.../tables/...")` | âŒ No | UC internal path, protected |
| `.load("dbfs:/mnt/...")` | âŒ No | UC disallows DBFS-mount access |
| `spark.read.table("catalog.schema.table")` | âœ” Yes | Uses UC API |
| `spark.readStream.table("catalog.schema.table")` | âœ” Yes | Full CDF support |
| `spark.read.format("delta").table(...)` with CDF options | âœ” Yes | Supported path |

---

### ğŸš€ Fully correct DLT CDF view for UC-managed table

```python
@dlt.view
def spmaster_changes():
    return (
        spark.readStream
            .format("delta")
            .option("readChangeData", "true")
            .table("ag_content_ims_dev.acs_entity.d_spmaster")
    )
```


DLT will process:
- inserts
- update_preimage
- update_postimage
- deletes

### ğŸ If you want

I can now generate your complete Type-2 SCD DLT pipeline using this correct UC-compatible reader.
Would you like:
- SCD2 (close previous records)?
- SCD1 (overwrite)?
- Tombstone delete rows?

Tell me, and Iâ€™ll generate the full pipeline.


---