## Spark Performance Optimization


---

### 📋 Spark Optimization Checklist


1. Input Stage: Efficient Data Read
	-	Use .select() to load only the columns you need
	-	Disable .option("mergeSchema", "false") if not needed
	-	Use .filter() as early as possible when you can

⸻

2. Join Stage: Broadcast Smartly
	-	Use broadcast() for any small table (<2GB)
	-	Always select only the join key + important fields before join
	-	Avoid joining two big tables without partition tuning

⸻

3. Shuffle Minimization
	-	Remove .distinct() unless truly necessary
	-	Prefer collect_list(), collect_set() only after groupBy
	-	Use .repartitionByRange() instead of .repartition() for sorted key distribution

⸻

4. Memory Management
	-	Control partition sizes (spark.sql.shuffle.partitions)
	-	Prefer narrow transformations (select, withColumn) over wide (join, groupBy) when possible
	-	Cache intermediate DataFrames only if reused more than once

⸻

5. Output Stage: Efficient Write
	-	Use .option("maxRecordsPerFile", 1000000) to control output file size
	-	Prefer .coalesce() to merge small partitions before writing
	-	Write in overwrite mode carefully to avoid concurrent writers if possible


---

###  🎯 Simple Performance Rules to Remember



*Situation and Action*

| **Situation**                                   | **Action**                                                           |
|-------------------------------------------------|---------------------------------------------------------------------|
| Joining big table + small table                | Always broadcast the small one                                      |
| Only need 3 columns from a table               | .select() before processing                                          |
| Too many tiny output files                     | .coalesce() or .repartition() smartly before .write                  |
| Seeing random huge shuffles?                   | Check .distinct(), .groupBy(), bad joins                             |
| Writes taking forever?                         | Control .option("maxRecordsPerFile"), repartition output             |


---

### 🔑 Key Config Adjustments:

1.	Adaptive Query Execution (AQE):
Helps automatically adjust shuffle partition sizes based on data volume. This minimizes excessive shuffling.
2.	Shuffle Partitions:
Controls the number of partitions Spark uses during shuffle. A higher value can reduce the risk of overloading a single partition (good for larger datasets).
3.	Broadcast Join Threshold:
Ensures smaller tables are broadcasted to workers, preventing the need for shuffling.
4.	Max Partition Bytes:
Defines the maximum size of a partition. Tuning it properly can improve parallelism and speed up IO-bound tasks.
5.	Output File Size:
Adjust maxRecordsPerFile to limit the number of records per output file. This avoids many small files in your output.


---

### 🎯 Additional Tips for Shuffling & Caching:


✨ Quick Visual of Narrow vs Wide
   - Narrow Transformations = Fast, local

select, withColumn, filter
   - Wide Transformations = Slow, shuffle

join, groupBy, distinct
   - We want to push Wide late, and do most work Narrow early!


---


###  Major Performance Issues with Spark Code


| **Issue**                                          | **Impact**                                                                                                                                                      |
|----------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **1. Excessive .distinct() Calls**                 | Leads to expensive shuffles, particularly in large datasets, increasing data transfer between nodes and consuming more memory.                                  |
| **2. Multiple Dense Rank Calculations**           | Triggers full shuffles, causing memory pressure and inefficient execution, especially when applied across large datasets.                                       |
| **3. Redundant Parquet Reads/Writes**              | Inefficient as it requires re-reading Parquet files immediately after writing them back to disk; data can be kept in memory for further processing.            |
| **4. Exploding Large Arrays Early**                | Causes massive data skew, leading to unbalanced workloads across the cluster when large arrays are exploded before filtering or joins.                          |
| **5. Unoptimized Joins**                           | Large shuffle sizes and costly operations when joining on multiple keys without partitioning or co-partitioning.                                               |
| **6. Lack of Partitioning During Writes**          | Slows down read performance later and increases disk I/O when large datasets are written to Parquet without partitioning.                                        |
| **7. No Caching for Reused DataFrames**            | Increases computation time as the same data is recomputed multiple times when intermediate DataFrames are reused but not cached or persisted.                    |
| **8. Unnecessary GroupBy Operations**              | Increases shuffle size and reduces efficiency when groupBy is used early or with unnecessary columns. It should only be done after filtering out irrelevant data. |
| **9. Unoptimized Use of .select()**                | Increases the amount of data shuffled and stored in memory when unnecessary columns are selected early in the pipeline.                                           |


---


###  General and Most Efficient Optimization Considerations for Spark with Large Datasets


| **Optimization Tip**                               | **Explanation/Impact**                                                                                                                                                                |
|---------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **1. Push Projections Early**                      | Limit the number of columns selected before transformations or joins to reduce shuffle size and memory usage. Avoid selecting unnecessary columns early in the pipeline.              |
| **2. Filter Early, Join Late**                     | Apply .filter() early to reduce the number of rows before transformations like joins. This reduces the amount of data that Spark needs to shuffle.                                     |
| **3. Broadcast Small Tables**                      | Use .broadcast() for small dimension tables during joins to avoid shuffling large datasets. Ensures the small table is sent to all executors instead of performing a shuffle.         |
| **4. Cache Strategically**                         | Cache or persist DataFrames that are reused multiple times to avoid recomputation and save time for repeated accesses, especially if expensive transformations are required.            |
| **5. Avoid Using .distinct() After Joins**         | Using .distinct() after joins causes unnecessary shuffling. Instead, use .dropDuplicates() before the join to ensure that only unique rows are involved.                             |
| **6. Repartition Before Joining**                  | Repartition data on the join keys (e.g., .repartition($"pguid")) to ensure that data is co-located, reducing shuffle size and improving join performance.                           |
| **7. Group Aggregations Outside Join Chains**     | Perform aggregations before joining where possible. Grouping data within a join chain increases shuffle size and memory pressure.                                                     |
| **8. Explode Arrays After Joins**                  | Perform the explode() operation on arrays after filtering or joining to avoid large intermediate datasets that could lead to inefficient execution.                                   |
| **9. Use Coalesce Before Writing Data**            | Use .coalesce(n) before writing data to Parquet to reduce the number of output files, minimizing overhead during the write phase and improving file management.                        |
| **10. Partition Your Output**                      | Partition data based on a column with small cardinality (e.g., year or article_key prefixes) to improve read performance when writing large datasets to Parquet.                      |
| **11. Materialize Small Lookup Tables**            | Cache or persist small lookup tables after reading them initially to avoid multiple accesses to the same data, thus reducing I/O costs.                                              |
| **12. Skew Handling**                              | If join keys are highly skewed, use techniques like salting to distribute data more evenly across partitions and improve parallelism during the join process.                         |
| **13. Avoid Repeated Parquet Reads**               | Reuse Parquet data by storing results in memory rather than reading the same files multiple times in the pipeline. This minimizes I/O costs.                                          |
| **14. Use .explain() and Spark UI for Profiling**  | Regularly use .explain() to inspect execution plans and identify expensive operations like shuffles or skewed joins. Leverage Spark UI for deeper profiling of query performance.    |
| **15. Efficient Use of Window Functions**          | Avoid applying multiple window functions like dense_rank() on the entire dataset. Combine them into a single select statement to reduce shuffling and improve performance.            |
| **16. Optimize Large Aggregations with Skewed Data**| Use techniques like salting or more distributed approaches for aggregations when keys (e.g., pguid) are skewed to balance data distribution and improve performance.                 |


---

###  Summary of Efficient Spark Coding Practices


| **Optimization Tip**                             | **Explanation/Impact**                                                                                                                                                          |
|-------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **1. Avoid Shuffles**                           | Minimize shuffle operations, especially in joins and groupBy operations. Consider using broadcast joins for small tables and partitioning the data appropriately to reduce shuffling. |
| **2. Optimize Memory Usage**                    | Use .cache() or .persist() only when necessary. Avoid caching datasets that are read or processed only once to save memory.                                                        |
| **3. Efficient Joins**                          | Use broadcast joins for small datasets to avoid large shuffles. For large datasets, repartition them by the join keys to reduce shuffle size and improve performance.               |
| **4. Preprocess Data**                          | Deduplicate and select relevant columns before performing heavy transformations or joins to reduce I/O and shuffle costs, optimizing pipeline performance.                        |
| **5. Optimize Writes**                          | Partition and coalesce the output data before writing to Parquet to ensure efficient storage and fast read performance later.                                                     |


---

###  ⚡ Summary of Advanced Tricks


*Optimization Techniques*

| **Technique**      | **When to Use**                                | **Benefit**                  |
|--------------------|------------------------------------------------|------------------------------|
| **Precompute tables** | Static or rarely changing datasets            | Faster startup               |
| **Manual broadcast**  | Small tables <200MB                           | Eliminate shuffle            |
| **Z-Ordering**        | Delta tables, heavy read patterns             | 5–10× read speedup           |
| **Bucketing**         | Very large fact tables, frequent joins        | 2–4× join speedup            |


| Recommendation               | Description                                                                                     |
|-----------------------------|-------------------------------------------------------------------------------------------------|
| Cache Intermediate DataFrames | Only cache DataFrames if reused multiple times to speed up read operations (but monitor memory usage). |
| Avoid Shuffling Large Tables | Minimize shuffle steps by using `select` early, reducing the number of columns to avoid large shuffles. |
| Partitioning                 | When writing out data, use partitioning to improve parallel reads and writes, e.g., partition by relevant columns. |



---

📈 If you combine all of these:

You could make your end-to-end job 5×–10× faster than the original!!


By following these principles, you can significantly reduce memory pressure, shuffle sizes, and I/O overhead while improving CPU efficiency when processing large-scale datasets in Spark.


---

## Tips of Optimization techonolgies


---


### 1. ⚙️ Spark Tuning Config for Optimal Performance


```scala
// Enable Adaptive Query Execution for optimized shuffle sizes
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.shuffle.targetPostShuffleInputSize", "64MB") // Target shuffle size per partition

// Adjust shuffle partitions to avoid too many small partitions during shuffle
spark.conf.set("spark.sql.shuffle.partitions", "200") // Adjust based on job size, default is 200 (increase for larger clusters)

// Optimize Broadcast Join Threshold
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "50MB") // For tables that are small enough to be broadcasted

// Partition size tuning to balance the job
spark.conf.set("spark.sql.files.maxPartitionBytes", "128MB") // Partition size, lower for better parallelism
spark.conf.set("spark.sql.files.openCostInBytes", "134217728") // Size to read file blocks, tune for better IO balance

// Cache intermediate DataFrames only if reused often
spark.conf.set("spark.sql.cache.serializer", "org.apache.spark.storage.BlockManagerBasedStore")
// Enable cache if needed during repetitive access
// spark.conf.set("spark.sql.cache", "true") 

// Control the size of the file output during writes to avoid many small files
spark.conf.set("spark.sql.parquet.compression.codec", "snappy") // Snappy is a good compression choice
spark.conf.set("spark.sql.files.maxRecordsPerFile", "1000000") // Max records per file to reduce file fragmentation

// Optional: Enable a more aggressive shuffle merging
spark.conf.set("spark.shuffle.compress", "true") // Compress shuffle data to save memory
spark.conf.set("spark.shuffle.spill.compress", "true") // Compress shuffle spill data

```

---

###  2. Broadcast Join 



🔧 How Broadcast Join Works in Spark:

1. Broadcasting the Small Table:
- Spark will copy the smaller table to all worker nodes (executors) in the cluster.
- This eliminates the need to shuffle the smaller table across the network.

2. Local Join Execution:
- Once broadcasted, the join is done locally on each partition of the larger table using the in-memory small table.
- This drastically reduces network I/O and shuffle, which is often the biggest performance bottleneck.

3. Broadcast Trigger:
- Spark automatically decides to broadcast a table if its size is smaller than the config threshold:
```pgsql
spark.sql.autoBroadcastJoinThreshold  (default: 10MB)
```
- You can increase this if your cluster has more memory:
```scala
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 50 * 1024 * 1024) // 50MB

```

Here, threshold is not the number of rows — it's the maximum size of the table in bytes that Spark will consider for broadcast join.

💡 How to Check Size of a DataFrame?

```scala
//option-1
df.queryExecution.optimizedPlan.stats.sizeInBytes
// option-2
df.cache()
df.count()  // Materialize it
spark.sessionState.executePlan(df.queryExecution.logical).optimizedPlan.stats.sizeInBytes

```



4. Manual Broadcast (if needed):
- You can explicitly mark a DataFrame to broadcast:
```scala
import org.apache.spark.sql.functions.broadcast

val joined = largeDF.join(broadcast(smallDF), "join_key")
```

✅ When to Use Broadcast Join:

- One table is small (< 10–50 MB typically).
- The join key is not skewed.
- You want to minimize shuffles.
- You are joining with a dimension table (common in star schemas).

⚠️ Caution:

- Don’t broadcast large tables — it can crash executors due to OOM (out-of-memory).
- If both tables are large, Spark will fall back to sort-merge or shuffle hash join.


---

###  3.



Great question — and you’re absolutely right to consider performance implications when refactoring for modularity. Here’s the honest trade-off analysis:

⸻

🔍 Short Answer:

No, modularizing your code into functions does not inherently use more memory or degrade performance significantly, as long as:
	•	You don’t unnecessarily cache or persist intermediate DataFrames,
	•	You avoid redundant computations (like re-reading the same table twice),
	•	You don’t trigger actions (count(), collect(), etc.) inside functions unless necessary.

But there are caveats — see below.


📌 What Happens Internally in Spark
	•	Spark uses lazy evaluation: each DataFrame transformation builds a logical query plan. Nothing is executed until an action is called (e.g., .write, .count, .collect).
	•	Intermediate variables like df1, df2 in a function chain just extend the plan; they don’t execute until the end.
	•	So even though you see “many intermediate DFs,” Spark internally combines them into one optimized job unless you explicitly cache or trigger them.

⸻

🚫 Potential Performance Pitfalls

Here are things that could degrade performance:
	1.	Redundant Table Reads
If each function re-reads the same source table (e.g., spark.table(...)), you’re wasting IO and memory.
✔️ Solution: Pass already-loaded DataFrames as arguments (which the refactor above already does).
	2.	Multiple Wide Transformations Without Repartitioning
Operations like groupBy, pivot, or joins across huge keys can blow up shuffle partitions.
✔️ Solution: Monitor shuffles and add .repartition() or .coalesce() only when truly needed.
	3.	Caching Too Early
Caching intermediate DataFrames in every step might balloon memory usage.
✔️ Solution: Only cache after expensive and reused steps.
	4.	Long Lineage Chains in Complex DAGs
If your function chain gets too long (dozens of transformations), Spark’s execution plan can get complex and costlier to optimize.
✔️ Solution: Use .checkpoint() if lineage grows unmanageable (very rare unless doing recursive logic).


✅ Best Practices for Function-Based Modular Spark Pipelines

Practice
Recommended?
Notes
Break logic into functions
âœ… Yes
Improves readability, testability
Avoid table reads in functions
âœ… Yes
Pass DataFrames instead
Avoid unnecessary .cache()
âœ… Yes
Cache only reused or expensive stages
Chain transformations lazily
âœ… Yes
Spark will optimize whole DAG at the end
Trigger actions only once
âœ… Yes
Actions = .write, .count, .collect
Use logging or explain plans
âœ… Yes
Helps verify whatâ€™s materialized and when



🧪 Debug Tip: Use df.explain(true)

Before triggering the final action (e.g., write), run:

dfSizeNorm.explain(true)

This shows you the full optimized plan, and you can verify:
	•	If Spark reuses stages,
	•	If multiple scans are occurring,
	•	If joins/shuffles are problematic.



✅ Summary

Concern
Answer
More memory?
âŒ Not if lazy evaluation and caching are used wisely
Slower performance?
âŒ Not unless you introduce redundancy or misplace cache
Recommended pattern?
âœ… Yes for complex pipelines with multiple conditional steps

---

## Spark Configuration


JIRA https://jira.clarivate.io/browse/SES-4638

Databrick Default Config

``` scala
sparkSession.conf.set("spark.hadoop.fs.s3a.acl", "bucket-owner-full-control")
sparkSession.conf.set("spark.sql.adaptive.enabled", "true")
sparkSession.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
sparkSession.conf.set("spark.sql.adaptive.skewJoin.enabled", "true") 
```

EMR  Config

```scala
sparkSession.conf.set("spark.hadoop.mapreduce.output.committer.class", "org.apache.hadoop.mapred.DirectFileOutputCommitter")
sparkSession.conf.set("spark.hadoop.mapreduce.use.directfileoutputcommitter", "true")
sparkSession.conf.set("spark.sql.parquet.output.committer.class", "org.apache.spark.sql.parquet.DirectParquetOutputCommitter")

//  sc.hadoopConfiguration.set("mapreduce.output.committer.class", "org.apache.hadoop.mapred.DirectFileOutputCommitter")
//  sc.hadoopConfiguration.set("mapreduce.use.directfileoutputcommitter", "true")
```