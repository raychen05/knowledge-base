Pipeline Structure



Below is a clean, production-grade extension of your PipelineRunner that:
	â€¢	Reads once using a single watermark
	â€¢	Runs multiple transform tasks on the same extracted DataFrame
	â€¢	Initializes the checkpoint once
	â€¢	Marks completion once (or failure once)
	â€¢	Allows per-task target tables
	â€¢	Supports dry-run
	â€¢	Is safe for partial failure handling

This pattern fits very well with your existing Delta + watermark + checkpoint architecture.

â¸»

1ï¸âƒ£ Define a Task Abstraction

First, define a small task model so each transform is explicit and traceable.


```scala
case class PipelineTask(
    taskName: String,
    transformFunc: DataFrame => DataFrame,
    targetTable: String
)
```

2ï¸âƒ£ Extended PipelineRunner (Multi-Task, Single Watermark)


```scala
object PipelineRunner {

  def runMultiple(
      pipelineName: String,
      queryConfigPath: String,
      tasks: Seq[PipelineTask],
      dryRun: Boolean = false
  ): Unit = {

    // 1. Start watermark + checkpoint ONCE
    Checkpoints.markRunStarted(pipelineName)

    try {
      // 2. Extract ONCE (single watermark read)
      val rawDF = DeltaReader.readDeltaWithWatermark(queryConfigPath)

      if (rawDF.isEmpty) {
        println(s"No data found for $pipelineName")
        Checkpoints.markCheckpointSkipped(pipelineName)
        return
      }

      // Optional but recommended to avoid re-reads
      val cachedRawDF = rawDF.persist()

      // 3. Run each task
      tasks.foreach { task =>
        println(s"Running task: ${task.taskName}")

        val transformedDF = task.transformFunc(cachedRawDF)

        if (!dryRun) {
          DeltaWriter.writeDelta(transformedDF, task.targetTable)
        }
      }

      // 4. Mark completion ONCE
      Checkpoints.markRunCompleted(pipelineName)

      cachedRawDF.unpersist()

    } catch {
      case e: Throwable =>
        // 5. Mark failure ONCE
        Checkpoints.markRunFailed(pipelineName, Some(e.getMessage))
        throw e
    }
  }
}
```


3ï¸âƒ£ Example Usage


```scala
PipelineRunner.runMultiple(
  pipelineName = "article_metrics_pipeline",
  queryConfigPath = "/configs/wos/article_metrics.json",
  tasks = Seq(
    PipelineTask(
      taskName = "citations_metrics",
      transformFunc = ArticleCitationsTransform.run,
      targetTable = "dap_wos.article_citations"
    ),
    PipelineTask(
      taskName = "open_access_metrics",
      transformFunc = ArticleOpenAccessTransform.run,
      targetTable = "dap_wos.article_open_access"
    ),
    PipelineTask(
      taskName = "journal_metrics",
      transformFunc = JournalMetricsTransform.run,
      targetTable = "dap_wos.journal_metrics"
    )
  )
)
```



4ï¸âƒ£ Why This Design Works Well

âœ… Single Watermark Semantics
	â€¢	Watermark read once
	â€¢	All derived tables are logically consistent
	â€¢	No risk of partial watermark advancement

âœ… Atomic Pipeline Semantics
	â€¢	Either all tasks succeed â†’ watermark advances
	â€¢	Or any task fails â†’ watermark stays intact

âœ… Performance-Friendly
	â€¢	One Delta read
	â€¢	Cached DataFrame reused across tasks

âœ… Operationally Clean
	â€¢	One checkpoint record per pipeline run
	â€¢	Easy monitoring and replay

â¸»

5ï¸âƒ£ Optional Enhancements (Recommended for You)

Given your larger DLT / Workflow system, you may want:

ğŸ”¹ Task-level Metrics


```scala
Checkpoints.markTaskStarted(pipelineName, task.taskName)
Checkpoints.markTaskCompleted(pipelineName, task.taskName)
```

ğŸ”¹ Parallel Task Execution (Careful)

Only safe if:
	â€¢	No shared temp views
	â€¢	No conflicting writes


```scala
tasks.par.foreach { task => ... }
```


ğŸ”¹ Partial-Failure Strategy
	â€¢	Allow non-critical tasks to fail
	â€¢	Fail pipeline only for critical ones

â¸»

6ï¸âƒ£ Summary

This extension gives you:

âœ” Single watermark read
âœ” Multiple transforms
âœ” Clean checkpoint lifecycle
âœ” Strong consistency guarantees
âœ” Scales naturally with your Databricks + Delta architecture

If you want, I can next:
	â€¢	Add task-level checkpoint schema
	â€¢	Show parallel execution with failure isolation
	â€¢	Align this with Databricks Workflows task graph
	â€¢	Integrate CDF-based downstream triggers



This is a very good question, and it goes straight to operational-grade pipeline design â€” especially aligned with how youâ€™re already thinking about checkpoints, watermarks, and Databricks Workflows.

Below is a clean, recoverable, YAML-driven design that:

âœ” Defines tasks in job YAML
âœ” Runs them through your existing PipelineRunner framework
âœ” Uses task-level state so successful tasks are NOT rerun
âœ” Advances the watermark only when all tasks succeed
âœ” Supports resume-after-failure

â¸»

1ï¸âƒ£ Job YAML: Define Tasks Declaratively

Example: article_metrics_job.yaml


```scala
pipeline:
  name: article_metrics_pipeline
  queryConfigPath: /configs/wos/article_metrics.json

  tasks:
    - taskName: citations_metrics
      transformClass: com.company.transforms.ArticleCitationsTransform
      targetTable: dap_wos.article_citations
      critical: true

    - taskName: open_access_metrics
      transformClass: com.company.transforms.ArticleOpenAccessTransform
      targetTable: dap_wos.article_open_access
      critical: true

    - taskName: journal_metrics
      transformClass: com.company.transforms.JournalMetricsTransform
      targetTable: dap_wos.journal_metrics
      critical: false
```


Key points:
	â€¢	transformClass â†’ loaded dynamically
	â€¢	critical â†’ controls failure behavior
	â€¢	YAML becomes the source of truth

â¸»

2ï¸âƒ£ Task State Table (Core for Recovery)

You need a task-level checkpoint table in Delta.

Example: dap_meta.pipeline_task_runs


```sql
pipeline_name STRING
run_id STRING
task_name STRING
status STRING        -- PENDING | RUNNING | SUCCESS | FAILED
error_message STRING
started_at TIMESTAMP
completed_at TIMESTAMP
```


This is separate from the watermark table.

â¸»

3ï¸âƒ£ Pipeline Execution Flow (High Level)


```script
mark pipeline RUNNING
read watermark ONCE
for each task:
    if task already SUCCESS â†’ SKIP
    else run task
if all critical tasks SUCCESS:
    advance watermark
    mark pipeline SUCCESS
else:
    mark pipeline FAILED
```


4ï¸âƒ£ Load YAML â†’ Runtime Task Model

```scala
case class TaskConfig(
    taskName: String,
    transformClass: String,
    targetTable: String,
    critical: Boolean
)

case class PipelineConfig(
    name: String,
    queryConfigPath: String,
    tasks: Seq[TaskConfig]
)

```â€™


YAML loader â†’ PipelineConfig

â¸»

5ï¸âƒ£ Dynamic Transform Loader

Each transform implements a common trait:

```scala
trait PipelineTransform {
  def run(df: DataFrame): DataFrame
}
```

Example:

```scala
object ArticleCitationsTransform extends PipelineTransform {
  override def run(df: DataFrame): DataFrame = {
    // transform logic
  }
}
```


Loader:


```scala
object TransformLoader {
  def load(className: String): PipelineTransform = {
    Class.forName(className)
      .getField("MODULE$")
      .get(null)
      .asInstanceOf[PipelineTransform]
  }
}

```


6ï¸âƒ£ Enhanced PipelineRunner (Recovery-Aware)


```scala

object PipelineRunner {

  def runFromYaml(config: PipelineConfig, dryRun: Boolean = false): Unit = {

    val pipelineName = config.name
    val runId = Checkpoints.startPipelineRun(pipelineName)

    try {
      // 1. Read watermark ONCE
      val rawDF = DeltaReader.readDeltaWithWatermark(config.queryConfigPath)

      if (rawDF.isEmpty) {
        Checkpoints.markPipelineSkipped(pipelineName, runId)
        return
      }

      val cachedDF = rawDF.persist()

      // 2. Execute tasks with recovery
      config.tasks.foreach { task =>
        if (Checkpoints.isTaskSuccessful(pipelineName, runId, task.taskName)) {
          println(s"Skipping successful task: ${task.taskName}")
        } else {
          runTask(pipelineName, runId, cachedDF, task, dryRun)
        }
      }

      // 3. Validate critical tasks
      if (Checkpoints.allCriticalTasksSucceeded(pipelineName, runId)) {
        Checkpoints.markPipelineCompleted(pipelineName, runId)
      } else {
        throw new RuntimeException("Critical task failure detected")
      }

      cachedDF.unpersist()

    } catch {
      case e: Throwable =>
        Checkpoints.markPipelineFailed(pipelineName, runId, e.getMessage)
        throw e
    }
  }

  private def runTask(
      pipelineName: String,
      runId: String,
      df: DataFrame,
      task: TaskConfig,
      dryRun: Boolean
  ): Unit = {

    Checkpoints.markTaskStarted(pipelineName, runId, task.taskName)

    try {
      val transform = TransformLoader.load(task.transformClass)
      val resultDF = transform.run(df)

      if (!dryRun) {
        DeltaWriter.writeDelta(resultDF, task.targetTable)
      }

      Checkpoints.markTaskSuccess(pipelineName, runId, task.taskName)

    } catch {
      case e: Throwable =>
        Checkpoints.markTaskFailed(pipelineName, runId, task.taskName, e.getMessage)

        if (task.critical) throw e
    }
  }
}
```


7ï¸âƒ£ Failure Recovery: What Happens in Practice

Scenario
	â€¢	Task A â†’ SUCCESS
	â€¢	Task B â†’ FAILED
	â€¢	Task C â†’ NOT RUN

Restart Job

âœ” Watermark is unchanged
âœ” Task A â†’ SKIPPED
âœ” Task B â†’ RETRIED
âœ” Task C â†’ RUNS

This is exactly what you want.

â¸»

8ï¸âƒ£ Why This Beats Databricks â€œRetry Jobâ€


Approach	Problem
Databricks retry	Re-runs everything
Framework retry	Resumes at failed task
Watermark-first	Data loss risk
Task-first + watermark-last	Safe


9ï¸âƒ£ Optional Enhancements (Very Relevant for You)

ğŸ”¹ Parallel Non-Conflicting Tasks

```scala
config.tasks.par.foreach(...)
```

ğŸ”¹ Idempotent Write Guard


```scala
DeltaWriter.writeIfNotExists(runId, taskName)
```


ğŸ”¹ Task Dependency Graph


```scala
dependsOn: [citations_metrics]
```


ğŸ”¹ Retry Policy per Task


```scala
retries: 3
retryBackoffSeconds: 60
```

ğŸ”Ÿ Final Recommendation (Best Practice)

For your DAP / Delta / Workflow system:

âœ” YAML defines structure
âœ” Framework controls execution
âœ” Task-level checkpoints enable recovery
âœ” Watermark advances last
âœ” Works perfectly with Databricks Workflows

â¸»

If you want, next I can:
	â€¢	Design the checkpoint table schemas
	â€¢	Add task DAG dependencies
	â€¢	Align this with Databricks Jobs YAML
	â€¢	Show parallel execution safety rules
	â€¢	Integrate CDF downstream triggers






