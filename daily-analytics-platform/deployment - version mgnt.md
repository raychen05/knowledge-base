## Deployment Strategies & Versioning


Great question â€” youâ€™re weighing two valid deployment strategies. Letâ€™s break it down clearly and evaluate both options, then Iâ€™ll recommend the better approach for your use case.


### Option 1: Single Jenkins Job / Unified Deployment (One parent project)

**Description**:
- All 20+ Databricks pipelines are managed and deployed together under the same release version. A single Jenkins job handles deployment for all pipelines, even if only one changes.

âœ… **Pros**:
- Ensures version consistency across all pipelines (everything runs under one release tag).
- Easier to track releases â€” one tag, one deployment process.
- Simplifies dependency management if pipelines depend on shared components.

âš ï¸ **Cons**:
- Inefficient during development â€” redeploys all pipelines when only one is updated.
- Slower CI/CD cycle, higher compute and time costs.
- Increases risk: one pipeline failure can block the entire deployment.

---

### Option 2: Separate Repositories + Central DAP Config Repository

**Description**:
- Each pipeline is developed and versioned independently. A central DAP deployment repository stores metadata (Git URL, version tags, etc.) for all pipelines. Jenkins reads this configuration to deploy the correct version of each pipeline.
- 
âœ… **Pros**:
- Modular and flexible â€” you can update or redeploy only the pipelines that changed.
- Easier collaboration: different teams can own different pipelines.
- Scales well as the number of pipelines grows.
- Still allows coordinated releases by managing version tags in the DAP config repo.

âš ï¸ **Cons**:
- Slightly more complex setup â€” requires config management and some scripting logic in Jenkins.
- Requires clear governance to ensure all pipelines are version-tagged properly before release.

---

### âœ… Recommendation

#### Go with Option 2 â€” Separate Repos + Central DAP Config Repository.

It offers a better balance between flexibility and control:
- You avoid unnecessary redeployments.
- You still maintain release consistency through version config tagging.
- It scales well as your number of pipelines grows beyond 20.

You can later enhance this by:
- Automating the version lookup from the DAP config repo.
- Using a bundle manifest or JSON/YAML config that Jenkins reads to build and deploy each pipeline version automatically.

Would you like me to draft a recommended Jenkins deployment workflow (with steps or pseudo-YAML) for Option 2?


---

## Jenkins deployment workflow for Option 2


Perfect ğŸ‘ Hereâ€™s a recommended Jenkins deployment workflow for Option 2: Separate pipeline repos + a central DAP config repository.

### ğŸ§© Goal

Deploy 20 + Databricks pipelines â€” each versioned independently â€” using a central configuration (DAP) that defines:
- Which pipelines to deploy
- The Git repo and tag/branch to use
- The target Databricks environment

---

### ğŸ—‚ï¸ Repository Structure

#### 1. Individual Pipeline Repos

Each pipeline (e.g., ingestion, transform, analytics, etc.) lives in its own Git repository.

â”œâ”€â”€ dap-pipeline-ingestion/
â”œâ”€â”€ dap-pipeline-transform/
â”œâ”€â”€ dap-pipeline-analytics/

Each repo:
- Maintains its own CI/CD tests and version tags (e.g., v1.3.2).
- Contains its own bundle.yml or Databricks bundle definition.
  
#### 2. Central DAP Deployment Repo

This repo stores configuration for all pipelines and orchestrates the deployment.

```text
dap-deployment-config/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ pipelines.yml
â”‚   â”œâ”€â”€ env-prod.yml
â”‚   â”œâ”€â”€ env-qa.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy_pipeline.sh
â”‚   â”œâ”€â”€ utils.py
â”œâ”€â”€ Jenkinsfile
```

Example pipelines.yml:

```yaml
pipelines:
  - name: dap-pipeline-ingestion
    git_url: git@github.com:org/dap-pipeline-ingestion.git
    version: v1.3.2
  - name: dap-pipeline-transform
    git_url: git@github.com:org/dap-pipeline-transform.git
    version: v2.0.1
  - name: dap-pipeline-analytics
    git_url: git@github.com:org/dap-pipeline-analytics.git
    version: v1.2.0
```

---

### âš™ï¸ Jenkins Workflow Design

#### Stage 1 â€“ Checkout DAP Config Repo

```groovy
stage('Checkout Config') {
  steps {
    git branch: 'main', url: 'git@github.com:org/dap-deployment-config.git'
  }
}
```

#### Stage 2 â€“ Parse Config and Clone Pipelines

```groovy
stage('Clone Pipelines') {
  steps {
    script {
      def pipelines = readYaml file: 'config/pipelines.yml'
      pipelines.pipelines.each { p ->
        sh """
        git clone ${p.git_url} ${p.name}
        cd ${p.name}
        git checkout ${p.version}
        cd ..
        """
      }
    }
  }
}
```

#### Stage 3 â€“ Build Bundles

Each cloned repo should contain its own bundle.yml.

```groovy
stage('Build Bundles') {
  steps {
    script {
      def pipelines = readYaml file: 'config/pipelines.yml'
      pipelines.pipelines.each { p ->
        sh """
        cd ${p.name}
        databricks bundle validate
        databricks bundle build
        cd ..
        """
      }
    }
  }
}
```

#### Stage 4 â€“ Deploy to Databricks

```groovy
stage('Deploy to Databricks') {
  steps {
    script {
      def env = params.ENVIRONMENT ?: "qa"
      def pipelines = readYaml file: 'config/pipelines.yml'
      pipelines.pipelines.each { p ->
        sh """
        cd ${p.name}
        databricks bundle deploy --target ${env}
        cd ..
        """
      }
    }
  }
}
```

#### Stage 5 â€“ Tag Deployment

Optionally, tag the DAP deployment repo to record the release.

```groovy
stage('Tag Deployment') {
  steps {
    sh """
    git tag -a release-${env}-${BUILD_NUMBER} -m "Deployed pipelines to ${env}"
    git push origin release-${env}-${BUILD_NUMBER}
    """
  }
}
```

---

### ğŸš€ Key Benefits

âœ… Deploys all pipelines with the correct versions from one central control point.

- âœ… Supports partial updates â€” only modify version tags in the config file.
- âœ… Maintains traceability (which version of each pipeline was deployed).
- âœ… Works with multiple environments (QA, Prod, etc.).

Would you like me to extend this with automatic version detection (e.g., using Git tags or commit hashes instead of manually updating the YAML)?


---


##  Automating Version Lookup


Excellent â€” automating version lookup will make your Databricks multi-pipeline deployment smoother, more reliable, and hands-off for developers.

Hereâ€™s how you can extend the Jenkins workflow so that it automatically determines the correct version (tag or latest commit) for each pipeline based on the DAP configuration repository.


### âš™ï¸ Goal

Enhance the DAP deployment pipeline so that:
- You donâ€™t need to manually edit pipelines.yml each release.
- Jenkins automatically looks up the correct Git tag or version for each pipeline.
- All pipeline versions are logged and tagged together during deployment.


---

### ğŸ§© Design Options for Automatic Version Lookup

#### Option A â€“ Use Git Tags in Individual Pipeline Repos

Each pipeline repo maintains release tags, e.g. v1.3.2, v1.3.3.

The DAP config repo simply stores the repo URL; Jenkins dynamically fetches the latest tag.

```yaml
pipelines:
  - name: dap-pipeline-ingestion
    git_url: git@github.com:org/dap-pipeline-ingestion.git
  - name: dap-pipeline-transform
    git_url: git@github.com:org/dap-pipeline-transform.git
```

Lookup Logic (Groovy):

```groovy
stage('Clone and Resolve Versions') {
  steps {
    script {
      def pipelines = readYaml file: 'config/pipelines.yml'
      pipelines.pipelines.each { p ->
        sh """
        git ls-remote --tags ${p.git_url} | grep -E 'v[0-9]+\\.[0-9]+\\.[0-9]+$' | sort -V | tail -n1 | awk '{print \$2}' | sed 's|refs/tags/||' > version.txt
        """
        p.version = readFile('version.txt').trim()
        echo "Resolved ${p.name} â†’ ${p.version}"
        // Clone repo at that version
        sh """
        git clone ${p.git_url} ${p.name}
        cd ${p.name}
        git checkout ${p.version}
        cd ..
        """
      }
    }
  }
}
```

âœ… Pros:
- No need to update config manually.
- Always picks up the latest stable tag.

âš ï¸ Cons:
- You need consistent tagging practices in every pipeline repo.

---

#### Option B â€“ Central Version Manifest in DAP Repo

Instead of manually updating pipelines.yml, a script in Jenkins determines the version of each repo based on commit history or CI tags, then dynamically builds the manifest at runtime.

Example workflow:

1. Jenkins lists all pipeline repos.
2. For each repo, it fetches the latest release tag or commit hash.
3. It builds a temporary manifest file (e.g., pipelines_resolved.yml).
4. Deployment proceeds using that generated file.

```groovy
stage('Generate Version Manifest') {
  steps {
    script {
      def pipelines = readYaml file: 'config/pipelines.yml'
      def resolved = [pipelines: []]
      pipelines.pipelines.each { p ->
        def latestTag = sh(script: "git ls-remote --tags ${p.git_url} | grep -E 'v[0-9]' | sort -V | tail -n1 | awk '{print \$2}' | sed 's|refs/tags/||'", returnStdout: true).trim()
        resolved.pipelines << [name: p.name, git_url: p.git_url, version: latestTag]
      }
      writeYaml file: 'config/pipelines_resolved.yml', data: resolved
    }
  }
}
```

Then continue deployment using config/pipelines_resolved.yml.

âœ… Pros:
- DAP repo remains the single control point.
- All resolved versions are logged automatically.
- Easy to audit deployments later.

âš ï¸ Cons:
- Slightly more complex Jenkins logic.

---

#### Option C â€“ Tag Synchronization via Git Submodules (Optional)

If you prefer Git-native automation:
- The DAP repo can include each pipeline as a submodule at a specific tag.
- Running git submodule update --remote automatically syncs to the latest tags.
- Jenkins simply deploys submodule versions.

âœ… Pros: Version control handled by Git itself.
âš ï¸ Cons: Managing 20+ submodules can be cumbersome.

---

### âœ… Recommended Setup

#### Option B â€” Central Version Manifest

Itâ€™s the most maintainable approach for teams with many pipelines, since:
- You keep a single DAP config repo.
- Jenkins automates version resolution.
- The deployment manifest can be archived with each release for auditability.

---

### ğŸ§  Enhancement Ideas

- Add a manual override parameter in Jenkins (e.g., DEPLOY_TAG=v1.3.5) to deploy specific versions.
- Store the resolved manifest as a build artifact (for traceability).
- Use the Databricks CLI to validate each bundle version before deploying.
- Integrate with GitHub API to fetch the latest â€œreleaseâ€ instead of scanning tags manually.

Would you like me to show you a complete Jenkinsfile example implementing Option B (automatic version resolution + multi-pipeline deployment)?



---


## Automating Job Using Job ID


Here is a refined response to your question about job IDs and referencing jobs in Databricks:

âœ… What we know

1. When you create a job via the Jobs API (or via UI/automation) in Databricks, it gets assigned a **job_id**, which is the canonical identifier for that job. 
2. The API supports updating (via jobs/update) the settings of an existing job â€” so you are not always forced to delete + recreate. 
3. You can list all jobs (via jobs/list) and inspect their job_id and settings.name. 
4. You can also reference a job by name in some deployment/automation tooling (for example via lookup variables in asset bundles) rather than by hard-coding the job_id. For example:
 â€œvariables: â€¦ lookup: job: 'My Job1'â€ 

---

âš ï¸ Key caveats & what happens in your scenario

- If you redeploy the bundle by creating a new job (i.e., you delete the old job and create a new one) the job_id will change â€” because each created job has a new identifier. Indeed, one user asked: â€œwhen we promote this to different environments, would the job_id be the same â€¦ or would it change?â€ The answer given: it changes.
- If instead you update the existing job settings (with the same job_id) then the job_id remains the same. That means your automation referencing job_id remains valid.
- Referencing via job name is possible for listing/lookups, but the API calls that trigger a run (for example jobs/run-now) still expect a job_id. There is no endpoint to trigger a job purely by name in the standard REST API. You must first convert name â†’ job_id (via listing/filtering) then use that job_id. Databricks Databricks

---

ğŸ“ Summary answer to your three questions

1. Does the bundle receive a fixed job ID after deploying? If you create the job once, then yes the job_id is fixed for that job instance.
2. If we redeploy the bundle, will the job ID change? Yes â€” if you delete the previous job and create a new one (or deploy as a brand-new job), then a new job_id will be assigned. If you simply update the existing job, the job_id stays.
3. When triggering the job from automation/main job: can we call by name instead of job ID? You cannot directly trigger by name in the API â€“ you must reference the job_id. However you can look up the job_id by using the jobâ€™s name (via jobs/list) and then pass the id to the run API. Also, in deployment bundles you can use name-to-id lookup variables which help avoid hardcoding the id.

If you like, I can check exactly how the job_id behaves in the context of your tooling (e.g., deployment via Databricks Asset Bundles, Terraform, or CLI) and provide a best-practice pattern (CI/CD) for maintaining stable references. Would you like me to do that?



---


## Avoid Hardcoding job_id in Databricks Automation



Great â€” letâ€™s walk through two practical examples that show how to avoid hardcoding job_id in Databricks automation:


### ğŸ§© Example 1: Lookup job_id by job name (using Databricks REST API)

When your automation script or Jenkins job needs to trigger a Databricks job by name, you can:

1. Use the jobs/list API to find the jobâ€™s ID,
2. Then call jobs/run-now with that ID.

Python example using Databricks REST API

```python
import requests
import json

DATABRICKS_INSTANCE = "https://<your-databricks-instance>"
TOKEN = "<your-personal-access-token>"
JOB_NAME = "dap-pipeline-transform"

# Step 1: List jobs and find the ID by name
headers = {"Authorization": f"Bearer {TOKEN}"}
response = requests.get(f"{DATABRICKS_INSTANCE}/api/2.1/jobs/list", headers=headers)
response.raise_for_status()
jobs = response.json().get("jobs", [])

job_id = None
for job in jobs:
    if job["settings"]["name"] == JOB_NAME:
        job_id = job["job_id"]
        break

if not job_id:
    raise ValueError(f"Job '{JOB_NAME}' not found")

print(f"Resolved job ID for {JOB_NAME}: {job_id}")

# Step 2: Trigger the job
run_payload = {"job_id": job_id}
run_response = requests.post(f"{DATABRICKS_INSTANCE}/api/2.1/jobs/run-now",
                             headers=headers, data=json.dumps(run_payload))
run_response.raise_for_status()
print(f"Triggered job {JOB_NAME} â†’ Run ID: {run_response.json()['run_id']}")
```


âœ… Benefits:
- You only store the job name in your automation logic.
- The script automatically resolves the correct job_id at runtime, even after redeployment.
- Ideal for Jenkins, GitHub Actions, or Airflow DAGs that trigger Databricks jobs.

---
### ğŸ§± Example 2: Using name-to-ID lookup variables in Databricks Asset Bundles

If you use Databricks Asset Bundles (DAB) to manage deployments, you can use the name-to-ID lookup syntax to dynamically reference resources by name, avoiding hardcoded IDs entirely.

Example bundle.yml

```yaml
bundle:
  name: dap-pipeline-bundle
  targets:
    dev:
      default: true
      workspace:
        host: https://<your-databricks-instance>

resources:
  jobs:
    dap-pipeline-transform:
      name: "dap-pipeline-transform"
      tasks:
        - task_key: transform
          notebook_task:
            notebook_path: /Repos/dap-pipelines/transform

  jobs:
    dap-pipeline-master:
      name: "dap-pipeline-master"
      tasks:
        - task_key: trigger-transform
          run_job_task:
            # Reference the transform job by name instead of hardcoding job_id
            job: ${resources.jobs.dap-pipeline-transform}
```

âœ… What this does:

- The dap-pipeline-master job references the other job (dap-pipeline-transform) by logical name in the same bundle.
- When the bundle is deployed, Databricks automatically resolves the job_id of the referenced job.
- No manual updates are needed if the job IDs change across redeployments.

---

### ğŸ Summary

| Use Case                        | Method             | Description                                                                                   |
|----------------------------------|--------------------|-----------------------------------------------------------------------------------------------|
| Automation (Jenkins, scripts)    | REST API lookup    | Use `/api/2.1/jobs/list` to find the job by name, then trigger the job using its job ID.      |
| Databricks deployment bundles    | Name-to-ID lookup  | Use `${resources.jobs.job_name}` syntax in `bundle.yml` to reference jobs by logical name.    |


Would you like me to show a Jenkins pipeline snippet that uses the first method (lookup job ID by name and trigger it automatically)?

	
---


##  name-to-ID lookup approach to reference  external jobs


Yes â€” using the name-to-ID lookup approach with Databricks Asset Bundles can be used to reference jobs that exist outside of the current bundle, in certain conditions. Here are the details, caveats, and an example.


### âœ… What works

The documentation and community discussions show that you can use a lookup variable to dynamically resolve a jobâ€™s ID by its name, even when that job is not defined within the same bundle. 

For example:

â€œIf itâ€™s created outside of the asset bundle, and exists before this bundle will be used â€¦

```yaml
variables:
  your_job_id:
    lookup:
      job: "your_job_name"
```

In the resources section, reference it:

```yaml
run_job_task:
  job_id: ${var.your_job_id}
```â€  :contentReference[oaicite:1]{index=1}
```

That means you can configure a bundle so that one job (in the bundle) triggers an external job by name, without hard-coding the job_id.

---

### âš ï¸ Important conditions & limitations

- The external job must already exist in the workspace (or in the environment target) when the bundle runs. If the job doesnâ€™t exist yet, lookup will fail. 
- The lookup syntax is documented (under â€œvariables â†’ lookupâ€ in Asset Bundles) rather than referencing ${resources.jobs.xxx.id} (which only works when the job resource is defined in the same bundle). 
- Youâ€™ll need to ensure that workspace/workflows permissions allow the lookup and the run of that job.
- This approach still requires good governance â€“ if the external job is renamed or removed, the lookup will break.
- The bundle referencing the external job may not manage the lifecycle (creation/updates) of that job â€” only triggering it.

---

### ğŸ“‹ Example YAML snippet

Hereâ€™s how you might set it up in a bundle that needs to call an existing job named â€œexisting-pipeline-jobâ€:

```yaml
bundle:
  name: example-bundle

variables:
  external_job_id:
    description: "ID lookup for external job to trigger"
    lookup:
      job: "existing-pipeline-job"

resources:
  jobs:
    trigger_external_job:
      name: "trigger-external"
      tasks:
        - task_key: run_external_task
          run_job_task:
            job_id: ${var.external_job_id}
            job_parameters:
              param1: "value1"
```


In this example:

- existing-pipeline-job is an already-deployed job outside this bundle.
- The lookup resolves its ID dynamically.
- The bundle defines a job trigger_external_job which uses run_job_task to trigger the external job.

---

### ğŸ¯ My recommendation for your scenario

Given that you have many pipelines and likely some jobs may live outside the bundle youâ€™re deploying:

- Use this lookup method if the external job(s) are stable and pre-existing.
- Ensure your CI/CD process validates that the job names you reference via lookup exist (and capture errors if not).
- If the external job will be versioned/changed often, you may want to include that job in your bundle (so you can manage it) rather than treat it as fully external â€” then you could use the simpler ${resources.jobs.jobName.id} within the same bundle.

If you like, I can check the latest documentation to see if there are any new restrictions (e.g., workspace scopes, cross-target environment issues) for using lookups across bundles/targets, and draft a full bundle template for your multi-pipeline scenario that uses both internal and external job references.


---

##  A Complete Example via the name-to-ID lookup


Excellent ğŸ‘ â€” below is a complete example architecture, repository structure, and Jenkins pipeline snippet showing how to deploy and trigger multiple Databricks Asset Bundles (DAB), including cases where one bundle triggers external jobs via the name-to-ID lookup variable method.


### ğŸ§© Architecture Overview

You have 20+ Databricks pipeline bundles â€” each representing a logical data process.
- Each bundle is developed and versioned separately.
- A central â€œorchestratorâ€ bundle (and Jenkins CI/CD job) handles deployment and triggering.

### ğŸ—‚ï¸ Recommended Repository Structure

```text
/dap-pipelines/
â”œâ”€â”€ dap-pipeline-ingestion/
â”‚   â””â”€â”€ databricks.yml
â”œâ”€â”€ dap-pipeline-transform/
â”‚   â””â”€â”€ databricks.yml
â”œâ”€â”€ dap-pipeline-analytics/
â”‚   â””â”€â”€ databricks.yml
â””â”€â”€ dap-pipeline-orchestrator/
    â””â”€â”€ databricks.yml
```

### ğŸ”¹ Each individual pipeline repo

Contains its own DAB (databricks.yml) with resources for that pipeline job.

Example (dap-pipeline-transform/databricks.yml):

```yaml
bundle:
  name: dap-pipeline-transform

resources:
  jobs:
    dap-pipeline-transform:
      name: "dap-pipeline-transform"
      tasks:
        - task_key: transform_main
          notebook_task:
            notebook_path: /Repos/dap/transform/main_notebook
```


### ğŸ”¹ Orchestrator bundle (dap-pipeline-orchestrator)

This bundle defines a job that triggers other existing pipeline jobs â€” some within this bundle, some outside it.

Example (dap-pipeline-orchestrator/databricks.yml):

```yaml
bundle:
  name: dap-pipeline-orchestrator

variables:
  ingestion_job_id:
    lookup:
      job: "dap-pipeline-ingestion"
  transform_job_id:
    lookup:
      job: "dap-pipeline-transform"
  analytics_job_id:
    lookup:
      job: "dap-pipeline-analytics"

resources:
  jobs:
    dap-pipeline-orchestrator:
      name: "dap-pipeline-orchestrator"
      tasks:
        - task_key: trigger_ingestion
          run_job_task:
            job_id: ${var.ingestion_job_id}

        - task_key: trigger_transform
          depends_on: [trigger_ingestion]
          run_job_task:
            job_id: ${var.transform_job_id}

        - task_key: trigger_analytics
          depends_on: [trigger_transform]
          run_job_task:
            job_id: ${var.analytics_job_id}

targets:
  dev:
    workspace:
      host: https://<databricks-dev>.cloud.databricks.com
  prod:
    workspace:
      host: https://<databricks-prod>.cloud.databricks.com
```

### âœ… This setup:

- Uses lookup variables to reference external jobs by name.
- Avoids hardcoding job_ids.
- Allows the orchestrator to trigger all other jobs in sequence.
- Supports deployment to multiple workspaces (dev/prod).


### âš™ï¸ Jenkins CI/CD Pipeline Example

Hereâ€™s a Jenkinsfile that:

- Checks out all pipeline repos.
- Builds and deploys each Databricks bundle.
- Finally, deploys the orchestrator bundle that references the others.

```groovy
pipeline {
  agent any

  environment {
    DATABRICKS_HOST = credentials('DATABRICKS_HOST')
    DATABRICKS_TOKEN = credentials('DATABRICKS_TOKEN')
  }

  stages {

    stage('Checkout All Repos') {
      steps {
        sh """
        git clone git@github.com:org/dap-pipeline-ingestion.git
        git clone git@github.com:org/dap-pipeline-transform.git
        git clone git@github.com:org/dap-pipeline-analytics.git
        git clone git@github.com:org/dap-pipeline-orchestrator.git
        """
      }
    }

    stage('Deploy Pipelines') {
      steps {
        script {
          def pipelines = [
            "dap-pipeline-ingestion",
            "dap-pipeline-transform",
            "dap-pipeline-analytics"
          ]
          pipelines.each { p ->
            sh """
            cd ${p}
            databricks bundle validate
            databricks bundle deploy --target prod
            cd ..
            """
          }
        }
      }
    }

    stage('Deploy Orchestrator') {
      steps {
        sh """
        cd dap-pipeline-orchestrator
        databricks bundle validate
        databricks bundle deploy --target prod
        cd ..
        """
      }
    }

    stage('Trigger Orchestrator Job') {
      steps {
        sh """
        databricks jobs run-now --job-name "dap-pipeline-orchestrator"
        """
      }
    }
  }
}
```

### âœ… What this does:

- Deploys all individual pipelines first (so the lookup variables can resolve).
- Deploys the orchestrator bundle.
- Triggers the orchestrator, which runs external jobs sequentially.


### ğŸ§  Best Practices

1. Naming Consistency â€” ensure all external jobs have stable, unique names across environments.
2. Pre-deployment Validation â€” use databricks jobs list to confirm the target jobs exist before deploying orchestrator.
3. Error Handling â€” add conditional checks in Jenkins to skip deployment if a job lookup fails.
4. Version Control â€” manage versions/tags of each pipeline repo, but orchestrator should always target deployed names.
5. Security â€” use Jenkins credentials for tokens and environment variables.

### ğŸš€ Result

With this setup:
- You can manage 20+ Databricks pipelines modularly.
- Use lookup variables for cross-bundle job references (safe from job_id changes).
- Automate deployments and orchestrations via Jenkins end-to-end.




