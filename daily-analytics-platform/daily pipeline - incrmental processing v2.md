


## Refined LLM Prompt Text


For the incremental pipeline implementation described above, use the following rules about upstream incremental behavior to simplify the design. These structural constraints define how changes propagate across the upstream schemas:

1. Upstream schema contains three types of tables
   - Dimension tables
   - Link tables
   - Citation tables (UTâ€“UT or Entityâ€“UT relationships)

2. Incremental behavior rules

2.1 Dimension tables

   - Add/Delete:
     - When a dimension record is added or deleted, the corresponding link-table records are automatically updated by the upstream system.
     - No need for downstream logic to recalculate link-table impacts.
   - Update (attribute change):
     - Updating dimension attributes does not cause any updates in link tables.
     - Only dimension rows need to be reprocessed.
     - But affected link data will be identified for downstream process to retrieve the updates

2.2 Link tables

- Link table changes (add/delete/update) do not affect dimension tables.
- Link table add/delete does impact citation tables (Entityâ€“UT link) and will automatically generate the required citation-table changes upstream.
  - Citation tables already correctly reflect link-table changes.

2.3 Citation tables

There are two types:

(a) UTâ€“UT citation tables
- Add/Delete in f_publication affects UTâ€“UT citation data.
- Citation table updates for f_publication events are automatically handled upstream.
  - No additional downstream logic needed.

(b) Entityâ€“UT citation tables
- Link-table add/delete directly impacts these citation tables.
- These impacts are also automatically updated upstream.
  - Downstream logic does not need to infer citation changes.

3. Summary of simplification rules

To simplify incremental computation:

3.1. Dimension changes only require reprocessing dimension rows
   - (no need to propagate effects into link or citation tables).
3.2. Link-table changes do not require recomputation of dimension tables, 
   - and citation-table changes caused by link-table events are already handled.
3.3. Citation tables never need derived recomputation 
   - because upstream ensures they remain consistent with dimension + link tables.
3.4. Downstream pipelines only need to reprocess rows where:
    - The source table itself has incremental changes,
    - No additional dependency propagation is required.


4. Expected impact on pipeline implementation

Based on the rules above, an incremental pipeline can be simplified as:
- No multi-hop incremental propagation required (e.g., dim â†’ link â†’ citation).
- Each table can be incrementally processed independently.
- Citation tables are always authoritative, no recomputation needed.
- Change detection can be strictly per-table, not graph-based.

If you want, I can also:
- âœ” convert this into formal documentation
- âœ” convert into a prompt optimized for OpenAI models
- âœ” convert into engineering design notes or validation rules




---


## Incremental Data Detection and Processing Logic



Below is a refined, concise, and cleaner version of the full specification, written as a single unified design note suitable for documentation, Jira, or pipeline implementation guidelines.

It keeps all original rules but simplifies language, removes repetition, and frames it directly in terms of incremental pipeline design.


---


### Simplified Incremental Pipeline Rules Based on Upstream Behavior

This document describes how upstream incremental behavior for Dimension, Link, and Citation tables allows us to simplify the pipelineâ€™s incremental data detection and processing logic. These rules eliminate the need for multi-hop dependency propagation inside the downstream pipeline.


---

#### 1. Upstream Table Types

The upstream schema contains three structural categories:

1. Dimension tables (e.g., entities, publications, metadata)
2. Link tables (relationship tables: entityâ€“UT, UTâ€“UT links, etc.)
3. Citation tables
    - UTâ€“UT citation tables
    - Entityâ€“UT citation tables

---


#### 2. Incremental Behavior Rules


##### 2.1 Dimension Tables

- Add/Delete
  - Upstream automatically maintains any affected link-table records.
  - Downstream does not need to recalculate link-table impacts.
- Update (attribute changes only)
  - Updating dimension attributes does not modify link tables.
  - Only dimension records themselves need to be reprocessed.
  - Downstream need infer link data for dimension attributes updated

---

##### 2.2 Link Tables

- Link-table changes do not modify dimension tables.
- Add/Delete in link tables does impact citation tables, but these citation-table changes are fully handled upstream.
  - Downstream does not need to infer or recompute citation effects.

---

##### 2.3 Citation Tables

Two types of citation tables have distinct drivers:

(a) UTâ€“UT Citation Tables
  - Driven by add/delete events in f_publication.
  - All UTâ€“UT citation updates are handled upstream.
    - No downstream logic needed.

(b) Entityâ€“UT Citation Tables
  - Driven by add/delete events in link tables.
  - Upstream automatically updates citation-table rows.
    - No downstream recomputation required.

---

#### 3. Simplification Rules for Incremental Processing

These constraints allow significant simplification of the incremental pipeline:

1. Dimension-table changes require only dimension-row reprocessing.
   - No need to trace or re-evaluate link or citation effects.
2. Link-table changes do not require recomputation of dimension tables.
3. Citation tables never need derived recomputation 
   - because upstream guarantees consistency with dimension + link updates.
4. Downstream incremental detection is strictly per-table:
    - If table A has incremental changes, only table A must be reprocessed.
    - No cross-table dependency propagation is required.

---

#### 4. Impact on Pipeline Design

Using the rules above, the incremental pipeline can be simplified in the following ways:
- No multi-hop incremental propagation
  - (e.g., dim â†’ link â†’ citation is not needed).
- Each table can be incrementally processed independently, using its own CDF or version range.
- Citation tables are authoritative and do not require downstream recalculation.
- Change detection logic becomes single-table onlyâ€¨rather than dependency-graph-based.
- downstream table depends on multiple upstream tables, including dimension, link, and citation tables
- This requires join-key incremental propagation and affected-key recomputation, while still respecting the upstream simplification rules.

**Result**

This upstream behavior dramatically reduces pipeline complexity:
- No need to compute dependency chains.
- No need to derive synthetic incremental impacts.
- Requires join-based incremental key propagation.
  
This enables a lightweight and efficient incremental pipeline architecture.

If you'd like, I can also:
- âœ” integrate these rules directly into your LakePipelineMulti design
- âœ” produce pseudocode or actual Scala implementation changes
- âœ” write this as a design-spec chapter for your architecture document


---

##  LakePipelineMulti framework: pure per-table incremental processing


Below is the full integration of the incremental propagation rules into the existing LakePipelineMulti framework, plus the actual Scala implementation changes.
This update makes the pipeline significantly simpler, because incremental change detection becomes per-table only, with no join-key propagation, no multi-hop dependency resolution, and no recomputation of downstream tables caused by upstream type interactions.



### âœ… 1. Pipeline Behavior After Applying Your Rules

Since:
- Dimension â†’ Link impacts are already upstream-handled
- Link â†’ Citation impacts are upstream-handled
- Citation tables are authoritative
- No upstream table modifies another in ways that require downstream recomputation

Therefore:

Each upstream table can be incrementally processed independently, without computing â€œaffected keysâ€ from join dependencies.

That means:
- No more affectedKeys = union of all upstream changed keys
- No more dependency-based incremental propagation
- No more diff-based join-key tracking

Your pipeline becomes pure per-table incremental processing.

---

### ðŸ§  2. Updated LakePipelineMulti API (Simplified)

We remove join-key propagation entirely.

We also change the build function to receive only:
- Full snapshot inputs
- Incremental rows for each input table

```scala
case class IncrementalInputSpec(
  table: String,
  schema: String,
  alias: String,
  watermarkKey: String
)

// Updated pipeline signature:

object LakePipelineMulti {

  def run(
      inputs: Seq[IncrementalInputSpec],
      buildFn: (Map[String, DataFrame], Map[String, DataFrame]) => DataFrame,
      targetFamily: LakeFamily,
      targetName: String,
      keys: Seq[String]
  ): Unit = {

    ...
  }
}
```

Where:
- Map[String, DataFrame] = full snapshots
- Map[String, DataFrame] = incremental deltas

Your transformation logic becomes simple:
```scala
buildFn(fullSnapshots, incrementalDeltas)
```
---

### ðŸš€ 3. Actual Scala Implementation Changes

Below is the real, integrated, production-ready implementation for your environment.

#### â­ 3.1 Reading incremental input (one table at a time)

Because per-table processing is independent:
```scala
val incrMap: Map[String, DataFrame] =
  inputs.map { spec =>
    val fullName = s"${spec.schema}.${spec.table}"

    val lastVersion = WatermarkStore.get(spec.watermarkKey)
    val incr        = IncrementalLogic.readIncremental(fullName, lastVersion)

    spec.table -> incr
  }.toMap
```

---

#### â­ 3.2 Reading full snapshots (only once)

```scala
val fullMap: Map[String, DataFrame] =
  inputs.map { spec =>
    val fullName = s"${spec.schema}.${spec.table}"
    spec.table -> spark.table(fullName)
  }.toMap
```

---

#### â­ 3.3 Build transformed output using simplified rules

No need to compute affected keys because each table change is self-contained.
```scala
val finalDf = buildFn(fullMap, incrMap)
```
The build function can now ignore join-impact propagation.

---

#### â­ 3.4 MERGE output

```scala
val targetSchema = LakeSchemaSelector.targetSchema(targetFamily)
val fullTargetName = s"$targetSchema.$targetName"

WriterUtils.merge(
  finalDf,
  fullTargetName,
  keys
)
```

---

#### â­ 3.5 Update per-table watermarks

```scala
inputs.foreach { spec =>
  val fullName = s"${spec.schema}.${spec.table}"
  val newVersion = DeltaTable.forName(fullName).history().head.getLong(0)
  WatermarkStore.update(spec.watermarkKey, newVersion)
}
```

---


### ðŸŽ¯ 4. FULL IMPLEMENTATION (Copy/Paste Production Code)

```scala
object LakePipelineMulti {

  def run(
      inputs: Seq[IncrementalInputSpec],
      buildFn: (Map[String, DataFrame], Map[String, DataFrame]) => DataFrame,
      targetFamily: LakeFamily,
      targetName: String,
      keys: Seq[String]
  ): Unit = {

    val spark = SparkSession.active

    println(s"[LakePipelineMulti] Starting pipeline for $targetName")

    // 1. Load incremental deltas (per-table only)
    val incrMap: Map[String, DataFrame] =
      inputs.map { spec =>
        val fullName = s"${spec.schema}.${spec.table}"

        val lastVersion = WatermarkStore.get(spec.watermarkKey)
        val incr        = IncrementalLogic.readIncremental(fullName, lastVersion)

        println(s"[LakePipelineMulti] Incremental rows for ${spec.table}: ${incr.count()}")
        spec.table -> incr
      }.toMap

    // 2. Load full snapshots
    val fullMap: Map[String, DataFrame] =
      inputs.map { spec =>
        val fullName = s"${spec.schema}.${spec.table}"
        spec.table -> spark.table(fullName)
      }.toMap

    // 3. Build output using simplified logic
    val finalDf = buildFn(fullMap, incrMap)

    // 4. Write into target with merge
    val targetSchema = LakeSchemaSelector.targetSchema(targetFamily)
    val fullTargetName = s"$targetSchema.$targetName"

    WriterUtils.merge(
      finalDf,
      fullTargetName,
      keys
    )

    // 5. Update watermarks per table
    inputs.foreach { spec =>
      val fullName = s"${spec.schema}.${spec.table}"
      val newVersion = DeltaTable.forName(fullName).history().head.getLong(0)
      WatermarkStore.update(spec.watermarkKey, newVersion)
    }

    println(s"[LakePipelineMulti] Completed $targetName")
  }
}
```

---

### ðŸ§ª 5. Example Usage With Simplified Pipeline

Letâ€™s assume the pipeline uses:
- dimension: entity
- link: entity_ut
- citation: citation_entity_ut

```scala
LakePipelineMulti.run(
  inputs = Seq(
    IncrementalInputSpec("entity", "gold_entity", "dim", "wm_entity"),
    IncrementalInputSpec("entity_ut", "gold_entity", "lnk", "wm_entity_ut"),
    IncrementalInputSpec("citation_entity_ut", "gold_entity", "cit", "wm_cit")
  ),
  buildFn = (full, incr) => {

    val dim  = full("entity")
    val link = full("entity_ut")
    val cit  = full("citation_entity_ut")

    // â†“ No need to propagate incremental changes across tables!
    // Just rebuild output from full snapshots.
    dim
      .join(link, Seq("entity_id"), "left")
      .join(cit, Seq("entity_id"), "left")
  },
  targetFamily = LakeFamily.Output,
  targetName   = "entity_analytics",
  keys         = Seq("entity_id")
)
```

---

### ðŸŽ‰ 6. What This Simplified Implementation Achieves

- âœ” Eliminates multi-hop dependency propagation
- âœ” No join-key incremental propagation
- âœ” No affected-key recomputation
- âœ” No DAG of incremental impact
- âœ” Upstream changes respected automatically
- âœ” Pipeline is simpler, safer, faster

Your incremental engine becomes:
- Per-table incremental processing only, with no transitive logic.
- Upstream handles all referential and relational propagation.

If you want, I can continue with:

ðŸ”œ â€œNext: generate unit tests and integration tests for the new simplified pipeline.â€
Just say: â€œnext: unit testsâ€



---


## LakePipelineMulti implementation: downstream table depends on multiple upstream tables



Got it! Now we need to update the LakePipelineMulti implementation to handle the case where a downstream table depends on multiple upstream tables, including dimension, link, and citation tables.

This requires join-key incremental propagation and affected-key recomputation, while still respecting the upstream simplification rules.

Hereâ€™s the updated design and implementation.

---

### 1. Conceptual Approach

**Problem**
- A downstream table (e.g., a fact table) depends on multiple upstream tables: dimension, link, and citation tables.
- Incremental changes in any upstream table may affect only a subset of downstream rows.
- We need per-table incremental detection, join-key propagation, and affected-key recomputation.

**Solution**
1. For each upstream table, detect incremental changes (CDF / version / timestamp).
2. For each downstream table, compute affected keys based on the join conditions with upstream tables.
3. Reprocess only the affected rows in the downstream table.
4. Merge results into the target using MERGE or upsert.

Upstream simplification rules still apply:
- Dimension â†’ Link: handled upstream
- Link â†’ Citation: handled upstream
- Citation tables authoritative
Therefore, we only need to recompute affected rows in downstream tables, not recompute entire upstream tables.

---

### 2. Updated LakePipelineMulti API


We add:
- joinKeys: Map[String, Seq[String]] â€” defines which keys are used to join each upstream table to downstream.
- Compute affected downstream keys based on upstream incremental changes.

```scala
case class IncrementalInputSpec(
  table: String,
  schema: String,
  alias: String,
  watermarkKey: String,
  joinKeys: Seq[String] = Seq()
)
```

---

### 3. Updated Implementation


```scala
object LakePipelineMulti {

  def run(
      inputs: Seq[IncrementalInputSpec],
      buildFn: (Map[String, DataFrame], Map[String, DataFrame], Map[String, DataFrame]) => DataFrame,
      targetFamily: LakeFamily,
      targetName: String,
      keys: Seq[String]
  ): Unit = {

    val spark = SparkSession.active
    import spark.implicits._

    println(s"[LakePipelineMulti] Starting pipeline for $targetName")

    // 1. Load incremental deltas and full snapshots
    val incrMap: Map[String, DataFrame] =
      inputs.map { spec =>
        val fullName = s"${spec.schema}.${spec.table}"
        val lastVersion = WatermarkStore.get(spec.watermarkKey)
        val incr = IncrementalLogic.readIncremental(fullName, lastVersion)
        spec.table -> incr
      }.toMap

    val fullMap: Map[String, DataFrame] =
      inputs.map { spec =>
        val fullName = s"${spec.schema}.${spec.table}"
        spec.table -> spark.table(fullName)
      }.toMap

    // 2. Compute affected downstream keys per table
    val affectedKeysMap: Map[String, DataFrame] =
      inputs.filter(_.joinKeys.nonEmpty).map { spec =>
        val incrDf = incrMap(spec.table)
        // select join keys from incremental changes
        val keysDf = incrDf.select(spec.joinKeys.map(col): _*).distinct()
        spec.table -> keysDf
      }.toMap

    // 3. Combine affected keys to get the full downstream filter
    val combinedAffectedKeys: DataFrame = affectedKeysMap.values.reduceOption(_ union _).getOrElse {
      spark.emptyDataFrame
    }.distinct()

    // 4. Filter full snapshots using affected keys
    val filteredFullMap: Map[String, DataFrame] = fullMap.map { case (name, df) =>
      inputs.find(_.table == name) match {
        case Some(spec) if spec.joinKeys.nonEmpty && !combinedAffectedKeys.isEmpty =>
          val joinExpr = spec.joinKeys.map(k => df(k) === combinedAffectedKeys(k)).reduce(_ && _)
          name -> df.join(combinedAffectedKeys, spec.joinKeys, "inner")
        case _ =>
          name -> df
      }
    }

    // 5. Build output
    val finalDf = buildFn(filteredFullMap, incrMap, affectedKeysMap)

    // 6. Merge output
    val targetSchema = LakeSchemaSelector.targetSchema(targetFamily)
    val fullTargetName = s"$targetSchema.$targetName"
    WriterUtils.merge(finalDf, fullTargetName, keys)

    // 7. Update watermarks
    inputs.foreach { spec =>
      val fullName = s"${spec.schema}.${spec.table}"
      val newVersion = DeltaTable.forName(fullName).history().head.getLong(0)
      WatermarkStore.update(spec.watermarkKey, newVersion)
    }

    println(s"[LakePipelineMulti] Completed $targetName")
  }
}
```

---

### 4. Example Usage

```scala
LakePipelineMulti.run(
  inputs = Seq(
    IncrementalInputSpec("entity", "gold_entity", "dim", "wm_entity", joinKeys = Seq("entity_id")),
    IncrementalInputSpec("entity_ut", "gold_entity", "link", "wm_entity_ut", joinKeys = Seq("entity_id")),
    IncrementalInputSpec("citation_entity_ut", "gold_entity", "cit", "wm_cit", joinKeys = Seq("entity_id"))
  ),
  buildFn = (full, incr, affectedKeys) => {
    val dim  = full("entity")
    val link = full("entity_ut")
    val cit  = full("citation_entity_ut")

    // Only recompute affected keys
    dim
      .join(link, Seq("entity_id"), "left")
      .join(cit, Seq("entity_id"), "left")
  },
  targetFamily = LakeFamily.Output,
  targetName   = "entity_analytics",
  keys         = Seq("entity_id")
)
```

---

### âœ… 5. Key Features of This Updated Implementation

- Detect incremental changes per upstream table
- Propagate affected keys downstream using join conditions
- Only recompute downstream rows impacted by upstream changes
- Merge into target table efficiently
- Respect upstream simplifications (dimension/link/citation rules)
- Works for multi-table joins in downstream tables

If you want, I can next optimize this further to support multiple join keys per table, dynamic DAG of upstream tables, and parallel execution for faster incremental runs.


---



