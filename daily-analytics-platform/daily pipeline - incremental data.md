


## End-to-end Hybrid Architecture ‚Äî CDF + Materialized View + Downstream Pipelines


I added a focused section showing an automated pattern to detect affected UTSs (unique entities/units) across multiple Delta tables, merge them into a change registry, expose them via a Serverless Materialized View, and run downstream pipelines that process only the affected UTSs.


**Architecture*- (Mermaid)

```text
flowchart LR
  subgraph Upstream
    A[delta.table.A]
    B[delta.table.B]
    C[delta.table.C]
  end

  subgraph CDF_Reader
    R1[Stream Job A]
    R2[Stream Job B]
    R3[Stream Job C]
  end

  subgraph Registry
    CR[uts_change_registry (Delta)]
    OFF[change_offsets (Delta)]
  end

  subgraph MV
    MV1[Serverless Materialized View: changed_uts_mv]
  end

  subgraph Downstream
    D1[Python/Scala Job: process_changed_uts]
  end

  A --> R1
  B --> R2
  C --> R3
  R1 --> CR
  R2 --> CR
  R3 --> CR
  CR --> MV1
  MV1 --> D1
  OFF --> R1
```

---


### Flow Summary

1. Enable CDF on upstream Delta tables. Each table produces change records (insert/update/delete) with _change_type and _commit_version.
2. A lightweight CDF reader job per source (or a single multi-source job) reads changes incrementally using stored offsets (last processed version). It extracts uts_id and change_type and writes deduplicated rows to uts_change_registry Delta table.
3. Optionally maintain a change_offsets Delta table to persist the last processed version per source table for safe restarts.
4. A Serverless Materialized View (or regular MV table) reads the uts_change_registry to expose the distinct UTS IDs changed in a given window (today/last hour). Serverless MVs provide incremental refresh and fast queries for downstream jobs.
5. Downstream ETL/ML jobs query the MV to get the list of affected UTSs and perform targeted reads/merges on operational tables.

#### Enable CDF (SQL)
```sql
ALTER TABLE catalog.schema.tableA SET TBLPROPERTIES (
  delta.enableChangeDataFeed = true
);
ALTER TABLE catalog.schema.tableB SET TBLPROPERTIES (
  delta.enableChangeDataFeed = true
);
-- Repeat for other tables
```

#### Change Offsets Table (Delta)
```sql
CREATE TABLE IF NOT EXISTS catalog.monitoring.change_offsets (
  source_table STRING,
  last_processed_version BIGINT,
  last_processed_timestamp TIMESTAMP
)
USING DELTA
PARTITIONED BY (source_table);
```

#### CDF Reader (Python streaming/batch template)
```python
from delta.tables import DeltaTable
from pyspark.sql import functions as F

# config
sources = [
  {"table":"catalog.schema.tableA", "alias":"A"},
  {"table":"catalog.schema.tableB", "alias":"B"}
]

def read_cdf_since(spark, table, starting_version):
    return (
      spark.read.format("delta")
        .option("readChangeFeed", "true")
        .option("startingVersion", starting_version)
        .table(table)
    )

for s in sources:
    # fetch last offset
    off = spark.table("catalog.monitoring.change_offsets").filter(F.col("source_table")==s['table']).collect()
    start_ver = off[0]['last_processed_version'] + 1 if off else 0

    changes_df = read_cdf_since(spark, s['table'], start_ver)
    # extract UTS ids and change_type
    uts_df = changes_df.select(F.col("uts_id"), F.col("_change_type").alias("change_type"), F.col("_commit_version"))
    # dedupe per uts_id keeping latest change per commit_version
    windowed = uts_df.withColumn("rn", F.row_number().over(Window.partitionBy("uts_id").orderBy(F.desc("_commit_version")))).filter(F.col("rn")==1).drop("rn")

    # enrich and write to registry
    registry_df = windowed.withColumn("source_table", F.lit(s['table'])).withColumn("change_ts", F.current_timestamp())
    registry_df.write.format("delta").mode("append").saveAsTable("catalog.monitoring.uts_change_registry")

    # update offset (atomic MERGE)
    latest_ver = windowed.agg(F.max("_commit_version")).collect()[0][0]
    if latest_ver is not None:
        spark.sql(f"MERGE INTO catalog.monitoring.change_offsets t USING (SELECT '{s['table']}' as source_table, {latest_ver} as last_processed_version, current_timestamp() as last_processed_timestamp) s ON t.source_table = s.source_table WHEN MATCHED THEN UPDATE SET t.last_processed_version = s.last_processed_version, t.last_processed_timestamp = s.last_processed_timestamp WHEN NOT MATCHED THEN INSERT *")
```


#### UTS Change Registry Table (Delta)
```sql
CREATE TABLE IF NOT EXISTS catalog.monitoring.uts_change_registry (
  source_table STRING,
  uts_id STRING,
  change_type STRING,
  change_ts TIMESTAMP,
  commit_version BIGINT
)
USING DELTA
PARTITIONED BY (date_trunc('day', change_ts));
```

#### Materialized View (Serverless SQL)
```sql
CREATE MATERIALIZED VIEW IF NOT EXISTS catalog.views.changed_uts_mv AS
SELECT DISTINCT uts_id
FROM catalog.monitoring.uts_change_registry
WHERE change_ts >= current_date() - 1; -- rolling 1-day window
```

#### Downstream job example (Python) ‚Äî process changed UTS only:
```python
changed = spark.table("catalog.views.changed_uts_mv")
uts = [row.uts_id for row in changed.collect()]
# Use this list to filter heavy joins or read only affected partitions
df = spark.table("catalog.app.master_table").filter(F.col("uts_id").isin(nuts))
# Process and MERGE results into target
```

**Operational Considerations**

- Idempotency: Ensure downstream processing for a given UTS is idempotent (MERGE or upsert semantics). Use run_id or process_version markers.
- Checkpointing & Offsets: Persist offsets (change_offsets) transactionally so readers resume safely after restarts.
- Deduplication: Deduplicate changes per UTS per commit_version to avoid reprocessing noisily updated rows.
- Retention: Retain registry for a window that supports MV refresh and incident investigation (e.g., 7‚Äì30 days).
- Scaling: For many sources, consider a single unified streaming job to reduce resource overhead.
- Latency: CDF read + registry write can be near real-time (seconds-minutes), MV refresh provides efficient SQL access.

If you'd like, I can now:
- Generate a runnable Databricks notebook (Python) that implements the CDF reader + registry writer + offset management; or
- Produce a Serverless SQL MV creation script + Databricks Job JSON that schedules the downstream processor.
Which do you want next?


---

## Materialized Views in Databricks and Serverless 


Excellent and very relevant question ‚Äî let‚Äôs unpack both sides carefully: Materialized Views in Databricks and Serverless compute limitations for SQL vs. Python/Scala workloads.

### üß† 1. Materialized Views in Databricks ‚Äî Concept

A Materialized View (MV) in Databricks is a managed Delta Live Table (DLT) or Databricks SQL object that stores precomputed results of a query for fast reuse ‚Äî similar to Snowflake or BigQuery MVs.
It‚Äôs defined in SQL, typically like:
```sql
CREATE MATERIALIZED VIEW IF NOT EXISTS analytics.daily_pipeline_summary
AS
SELECT
  pipeline_name,
  date,
  COUNT(*) AS total_runs,
  AVG(duration_sec) AS avg_duration
FROM monitoring.pipeline_logs
GROUP BY pipeline_name, date;
```

---

### ‚öôÔ∏è 2. Optimization & Incremental Refresh

Databricks MVs automatically handle incremental refresh using Delta‚Äôs transaction log and change data feed (CDF) under the hood.

*Key Features*
- Incremental recomputation: Only reprocesses new/changed Delta files.
- Automatic dependency tracking: Knows which source tables changed.
- Refresh frequency: Configurable (manual or auto-refresh every few minutes).
- Storage: Results stored as managed Delta tables.

**Optimization Tips**
- ‚úÖ Partition your source Delta tables (e.g., by date)
- ‚úÖ Use Z-Ordering on frequent filters
- ‚úÖ Enable Change Data Feed (CDF) on base tables
- ‚úÖ Use serverless SQL compute for low-latency refresh and queries

---

### üöÄ 3. Can You Use Python/Scala Pipelines with Serverless?

Short answer: ‚ùå Not directly.

Databricks Serverless SQL Warehouse is for SQL-only workloads ‚Äî you cannot run arbitrary Python or Scala Spark jobs*- there.


| Compute Type            | Supports SQL | Supports Python/Scala | Notes                                              |
|-------------------------|:------------:|:---------------------:|----------------------------------------------------|
| Serverless SQL Warehouse| ‚úÖ Yes       | ‚ùå No                 | Fast, autoscaling, low-latency, SQL-only           |
| All-purpose cluster     | ‚úÖ Yes       | ‚úÖ Yes                | Interactive, slower startup, supports full Spark    |
| Job cluster             | ‚úÖ Yes       | ‚úÖ Yes                | Best for production pipelines                      |
| DLT (Delta Live Tables) | ‚úÖ SQL & Python | ‚úÖ Limited Python   | Used for declarative pipelines (managed DAGs)       |

---

### üí° Recommended Hybrid Pattern

For your 20+ pipeline ecosystem:

1. Run your ETL/ELT pipelines in Python or Scala on job clusters.
2. Store results into Delta tables.
3. Expose aggregated or summary views as Materialized Views using Databricks SQL Serverless for:
    - Dashboards
    - Monitoring
    - Ad hoc analytics
  
Example Architecture

```css
[Python/Scala Pipelines] ‚îÄ‚ñ∂ [Delta Tables] ‚îÄ‚ñ∂ [Materialized Views in SQL Warehouse] ‚îÄ‚ñ∂ [Dashboards / Alerts]
```

**Example MV for Logs**
```sql
CREATE OR REPLACE MATERIALIZED VIEW monitoring.mv_pipeline_daily_summary
AS
SELECT
  pipeline_name,
  date,
  COUNT(*) AS total_runs,
  SUM(CASE WHEN status='SUCCESS' THEN 1 ELSE 0 END) AS success_count,
  SUM(CASE WHEN status='FAILED' THEN 1 ELSE 0 END) AS failure_count,
  AVG(duration_sec) AS avg_duration
FROM monitoring.pipeline_logs
GROUP BY pipeline_name, date;
```

---

### üîÑ 4. Incremental Refresh Pattern (Hybrid)


If you want to simulate incremental refresh without using the built-in MV feature:

- Enable Change Data Feed (CDF) on base tables:
```sql
ALTER TABLE monitoring.pipeline_logs SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
```

- Build a Python or SQL job that merges only changed data:
```sql
 MERGE INTO monitoring.pipeline_summary AS t
 USING (
   SELECT - FROM table_changes('monitoring.pipeline_logs', 0)
 ) s
 ON t.pipeline_name = s.pipeline_name AND t.date = s.date
 WHEN MATCHED THEN UPDATE SET ...
 WHEN NOT MATCHED THEN INSERT ...;
```

This works even if you mix Python pipelines and SQL Serverless MVs.

---

### ‚úÖ Summary


| Feature              | Serverless SQL           | Python/Scala Pipelines        |
|----------------------|-------------------------|-------------------------------|
| Compute type         | SQL-only                | Cluster-based                 |
| Incremental refresh  | Automatic (Materialized View) | Manual (via code or DLT) |
| Storage              | Delta table             | Delta table                   |
| Best use             | Analytics, dashboards, monitoring | ETL/ELT, complex processing |
| Integration          | Read/write shared Delta tables | Same tables feed into SQL MVs |

---

### üëâ Recommendation


Use Databricks SQL Serverless for:
- Materialized Views
- Incremental metrics dashboards
- Fast analytics queries

Use Python/Scala job clusters for:
- Heavy ETL / ingestion / transformation
- Upstream pipeline execution


---


## Incremental Change Detection Across Multiple Sources


Excellent and very practical question ‚Äî this touches on Delta architecture design + incremental change detection across multiple sources. Let‚Äôs go step-by-step.

### üß≠ Problem Context

You have multiple upstream Delta tables (e.g. raw/bronze/silver) updated daily or via stream.

Downstream jobs need to:
- Detect which UTs (units or unique entities) are affected by upstream changes, and
- Efficiently process only the changed UTs, not the entire dataset.

You are considering using Materialized Views (MVs) in Databricks SQL Serverless as a middle layer to merge or pre-aggregate changes.

---

### ‚öôÔ∏è Option 1 ‚Äî Materialized Views (Serverless ELT)

‚úÖ Pros:
- Fully managed, auto-optimized, incremental refresh (only processes changed partitions).
- Serverless compute scales automatically.
- Ideal for ELT aggregation, dimensional joins, or change tracking at scale.
- If all your sources are Delta and registered in Unity Catalog, MV refreshes efficiently using Delta transaction logs.

‚ö†Ô∏è Cons:
- MVs are SQL-only (no Python/Scala logic).
- Limited transformation logic (no complex custom functions, APIs, or machine learning).
- Refresh schedule granularity is coarse (typically minutes or hours).
- You cannot trigger MVs directly on every micro-batch event like Structured Streaming can.

üëâ Best when your upstream Delta tables are updated periodically (batch) and you need SQL-level incremental consolidation.


---

### üß© Option 2 ‚Äî Auto-Detect Changed Data via Delta Change Data Feed (CDF)

Delta tables with CDF enabled (delta.enableChangeDataFeed = true) provide efficient incremental change tracking.
You can:
```python
df = spark.read.format("delta")
  .option("readChangeFeed", "true")
  .option("startingVersion", last_version)
  .table("catalog.schema.table")
```

Then union all changes from multiple tables, extract affected UTSs, and drive your downstream updates.


‚úÖ Pros:
- Full flexibility (Python/Scala).
- Event-driven ‚Äî great for micro-batch or continuous streaming.
- Scales to 10M+ changes easily.
- You can maintain a small state store of last processed version per table.

‚ö†Ô∏è Cons:
- You manage incremental logic yourself (tracking offsets, merges, etc.).
- Slightly more engineering effort than MV.
- You need compute clusters (serverless SQL can‚Äôt do arbitrary Python/Scala logic).

üëâ Best when your pipelines are continuous or you need fine-grained incremental control.

---

### üß† Recommended Architecture (Hybrid)


For your case (‚Äúmultiple Delta streams ‚Üí detect changed UTS ‚Üí downstream pipeline‚Äù), the optimal design is:

1. Enable CDF on all upstream Delta tables.
2. Maintain a central ‚Äúchange registry‚Äù table:
```sql
 CREATE TABLE catalog.logs.uts_change_registry (
  source_table STRING,
  uts_id STRING,
  change_type STRING,
  change_ts TIMESTAMP
 )
 USING DELTA
 PARTITIONED BY (date_trunc('day', change_ts))
```

3. Ingest changes incrementally via Python streaming job:
    - Read CDF from each table incrementally.
    - Extract unique uts_id.
    - Append to uts_change_registry.

4. Build a Materialized View (SQL Serverless) that merges daily UTS-level view:
```sql
CREATE MATERIALIZED VIEW catalog.views.daily_changed_uts
 AS
 SELECT DISTINCT uts_id
 FROM catalog.logs.uts_change_registry
  WHERE change_ts >= current_date() - 1
```

5. Downstream pipelines (Python/Scala or SQL) query daily_changed_uts (or use it in a join) to fetch only impacted data.


‚úÖ This combines:
- Serverless incremental refresh (via MV)
- Scalable change detection (via CDF)
- Flexibility for downstream compute (Python or Scala pipelines)

---

### ‚ö° Efficiency & Scalability


| Aspect         | Materialized View (MV) | CDF-based Stream         |
|----------------|-----------------------|--------------------------|
| Latency        | Minutes                | Seconds‚Äìminutes          |
| Cost           | Lower (serverless)     | Slightly higher (compute)|
| Custom Logic   | Limited                | Fully flexible           |
| Auto Incremental | Yes                  | Manual (fine-grained)    |
| Best For       | Batch ELT              | Streaming / fine-grained ETL |


---
### üîç Summary Recommendation

- If your updates are daily/hourly: Use Materialized Views in SQL Serverless for incremental merge + pre-aggregation.
- If your updates are continuous / event-driven: Use CDF + Structured Streaming to detect changes and update a ‚Äúchanged UTS‚Äù registry.
- For most real-world data platforms ‚Üí hybrid design (CDF + MV) gives the best tradeoff between automation, cost, and flexibility.



---



## Incremental Change Detection Across Multiple Source  - Column Level Chnage


Here‚Äôs a clear and comprehensive rewritten version of your prompt:


### Prompt (Revised Version)

**Topic**: Incremental Change Detection Across Multiple Source Tables in Databricks Delta Lake

**Goal**:
- Design an efficient mechanism to detect incremental data changes across multiple Delta tables using Serverless SQL and Materialized Views (MVs) in Databricks. The objective is to ensure that when any change occurs in one or more source tables, all related data across those tables can be captured and processed together ‚Äî even if the change originates from only a single table.
- 
**Example Use Case**:
- Consider a table f_publication with columns such as uid, pub_year, title, and issn. We need to detect fine-grained (column-level) changes ‚Äî for instance, when the value of pub_year changes ‚Äî and propagate only those updated records to downstream tables.
  
**Requirements**:
1. Implement an incremental change detection mechanism that supports multi-table dependency tracking.
2. Detect column-level changes (single or multiple columns).
3. Use Databricks Serverless SQL + Materialized Views (MV) if possible for scalability and cost efficiency.
4. If Serverless SQL or MVs do not fully support column-level change detection or multi-table triggers, recommend the best alternative approach, ensuring:
   -  High efficiency (minimal recomputation)
   -  Support for incremental updates
   -  Compatibility with Delta Lake features (e.g., Change Data Feed, MERGE)
   -  Simplicity in operational maintenance

**Expected Output:**
- Recommended design pattern (architecture + logic flow)
- Example SQL or Delta Live Table / PySpark implementation
- Explanation of trade-offs (Serverless SQL vs. DLT vs. custom CDC approach)
- Best-practice recommendation for production workloads

Would you like me to extend this prompt into a Databricks architecture diagram and example SQL/PySpark workflow (showing how MV + Delta CDF or DLT could be combined for column-level change detection)?


---

Great ‚Äî here‚Äôs a complete architecture + concrete SQL and PySpark/Delta examples that implement column-level incremental change detection across multiple Delta tables, plus trade-offs and a recommended production approach.

---

### 1) High-level architecture (text + ASCII diagram)

**Components**:

- **Source Delta tables*- (e.g., f_publication, f_author, f_journal) ‚Äî enable Delta Change Data Feed (CDF) or record source timestamps/versions.
- **Lightweight change extractor*- ‚Äî Structured Streaming or job that reads CDF (or uses version ranges) and produces a change stream with uid, changed_columns, row_hash, source_table, change_type, commit_ts/version.
- **Change state store*- ‚Äî a Delta table that stores last-seen row_hash per uid per source table (for dedupe + column-level diff).
- **Join/Materialization layer*- ‚Äî either:
  -  Serverless SQL Materialized View (if you only need occasional refresh + Serverless supports your use case), or
  -  Delta Live Tables (DLT) / Structured Streaming job that MERGEs changes into downstream aggregated/denormalized table(s).
- **Downstream consumer*- ‚Äî BI, reporting, ETL pipelines that consume only changed / merged rows.
- **Orchestration & Monitoring*- ‚Äî job scheduling, watermarking, alerting, metrics.

**ASCII diagram**:

```pqsql
[Source Delta Tables w/ CDF]
       |  |  |
       v  v  v
[Change Extractor (stream batch) - reads CDF]
       |
       v
[Change Stream: uid, source_table, row_hash, changed_cols, version_ts]
       |
       v
[Change State Store (Delta)] <---- also used by Serverless SQL MV if applicable
       |
       v
[Join/Materialization Layer]
   - Option A: Serverless SQL Materialized View (read state store)
   - Option B: DLT / Spark Structured Streaming + MERGE (recommended for robust)
       |
       v
[Downstream Tables / Consumers]
```

---


### 2) Two recommended implementation patterns & when to use each


#### Pattern A ‚Äî Serverless SQL + Materialized View (lightweight)

Use if:
- Workloads are mostly ad-hoc queries and materialized view functions you need are supported.
- Low-latency ~minutes sufficient.
- You can express change detection purely via SQL and materialized views over Delta tables/state.

Limitations:
- Serverless SQL may not natively read Delta CDF or stream CDF events; materialized views may not support incremental logic needed for CDF semantics.
- Harder to run complex dedupe / merge operations safely (no easy MERGE automation).
- Less flexible for robust streaming / schema evolution.

#### Pattern B ‚Äî Delta Live Tables (DLT) or Spark Structured Streaming (recommended for production)

Use if:
- You need reliable, resumable streaming, column-level diffs, strong idempotency, and merges into downstream tables.
- You want built-in orchestration, quality controls, and easier support for CDF, MERGE, and schema changes.

Advantages:
- Reliable micro-batch streaming with MERGE semantics.
- Easy to use Delta CDF and handle version-based reads.
- Better for multi-table joins + complex transformation logic.

---

### 3) Core idea for column-level detection (approach)


1. For each source table, compute a deterministic row hash of the selected columns you care about for change detection (e.g., md5(concat_ws('|', pub_year, title, issn))).
2. Maintain a state table that stores the last-seen row_hash and last_seen_version/ts for each uid per source table.
3. When a change event arrives (via CDF or re-read), compute new hash and compare to stored hash:
   -  If hash changed ‚Üí compute changed_columns (optional: compare column by column) and emit a change record.
   -  If hash same ‚Üí skip.
4. Use emitted change records to MERGE/upsert into downstream tables or re-evaluate any derived views that depend on that uid.

This approach detects single-column or multi-column changes depending on which columns are included in the hash, and allows both row-level and column-level awareness if you compute individual column hash flags too.


---


### 4) Concrete examples

#### A. Create row hash in SQL (Serverless SQL style)

```sql
-- Example: create a change-state view or table (non-materialized)
SELECT
  uid,
  md5(concat_ws('|', cast(pub_year as string), coalesce(title,''), coalesce(issn,''))) AS row_hash,
  current_timestamp() as extracted_at
FROM delta.`/mnt/delta/f_publication`
```

To persist state:

```sql
CREATE TABLE IF NOT EXISTS prod.f_publication_state
USING DELTA
AS
SELECT uid, row_hash, extracted_at
FROM (
  SELECT uid,
         md5(concat_ws('|', cast(pub_year as string), coalesce(title,''), coalesce(issn,''))) AS row_hash,
         current_timestamp() as extracted_at
  FROM delta.`/mnt/delta/f_publication`
);

```

Then you can compare the current computed hash with prod.f_publication_state to find changed rows and update the state table.

---

#### B. Using Delta Change Data Feed + Spark Structured Streaming (PySpark) ‚Äî recommended


Enable CDF on table (one-time):
```sql
ALTER TABLE prod.f_publication SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
```

PySpark streaming job that reads CDF and computes diffs, then MERGEs:

```python
from pyspark.sql.functions import md5, concat_ws, col, lit
from delta.tables import DeltaTable

# Input change feed path
input_path = "/mnt/delta/f_publication"

# Read CDF as streaming source (micro-batch)
df_changes = (
    spark.readStream
         .format("delta")
         .option("readChangeFeed", "true")
         .option("startingVersion", 0)   # or use startingTimestamp
         .load(input_path)
)

# Compute row-level hash for select columns for detection
hash_cols = ["pub_year", "title", "issn"]
df_with_hash = df_changes.withColumn(
    "row_hash",
    md5(concat_ws("|", *[col(c).cast("string") for c in hash_cols]))
).select("uid", "row_hash", "_change_type", "_commit_version", "_commit_timestamp", *hash_cols)

# Write into a checkpointed streaming sink that will process each micro-batch
def foreach_batch(batch_df, batch_id):
    # Load state table (Delta)
    state_path = "/mnt/delta/state/f_publication_state"
    if not DeltaTable.isDeltaTable(spark, state_path):
        # create initial state
        batch_df.select("uid", "row_hash").write.format("delta").mode("overwrite").save(state_path)
        return

    state = DeltaTable.forPath(spark, state_path)

    # Prepare a temporary view for merging
    batch_df.createOrReplaceTempView("batch_changes")

    # Merge logic: update state and push changed rows to downstream
    # Example: merge into state table by uid; when row_hash differs, update and also output the changed rows
    merge_sql = """
    MERGE INTO delta.`{state}` s
    USING (SELECT uid, row_hash, _commit_version, _commit_timestamp, pub_year, title, issn FROM batch_changes) b
    ON s.uid = b.uid
    WHEN MATCHED AND s.row_hash <> b.row_hash
      THEN UPDATE SET *
    WHEN NOT MATCHED
      THEN INSERT *
    """.format(state=state_path)

    spark.sql(merge_sql)

    # Optionally capture changed rows to downstream table via join with previous state or directly from batch
    changed_rows = spark.sql("SELECT b.- FROM batch_changes b LEFT JOIN delta.`{}` s ON b.uid = s.uid WHERE s.row_hash IS NULL OR s.row_hash <> b.row_hash".format(state_path))
    # write changed_rows to downstream (MERGE/upsert)
    changed_rows.write.format("delta").mode("append").save("/mnt/delta/consumers/publication_changes")
```

Start stream:

```python
query = (
    df_with_hash.writeStream
                .foreachBatch(foreach_batch)
                .option("checkpointLocation", "/mnt/delta/checkpoints/f_publication_cdf")
                .start()
)
query.awaitTermination()
```

**Notes**:
- This creates a state table (f_publication_state) with last-seen row_hash. Each batch MERGEs in new hashes and identifies changed rows.
- changed_rows can be used to MERGE into denormalized downstream tables.

---

### C. Column-level detection (identify which columns changed)

If you need to know which columns changed (not just that "something changed"), compute per-column hashes/flags:

from pyspark.sql.functions import md5, when

```python
df = df_changes.withColumn("hash_pub_year", md5(col("pub_year").cast("string"))) \
               .withColumn("hash_title", md5(col("title"))) \
               .withColumn("hash_issn", md5(col("issn")))

# Join with previous state and compare each column hash to detect which columns changed
```

In SQL:

```sql
SELECT b.uid,
       CASE WHEN s.hash_pub_year IS NULL OR s.hash_pub_year <> b.hash_pub_year THEN 'pub_year' END as pub_year_changed,
       CASE WHEN s.hash_title IS NULL OR s.hash_title <> b.hash_title THEN 'title' END as title_changed,
       ...
FROM batch_changes b
LEFT JOIN prod.f_publication_state s ON b.uid = s.uid
WHERE s.row_hash IS NULL OR s.row_hash <> b.row_hash;
```

Record the changed column names in an array field changed_columns or boolean flags.

---

### 5) Handling multiple source tables & related data

If your downstream depends on data from several tables (e.g., f_publication + f_author + f_journal), maintain per-table state stores and a dependency graph that describes how downstream records are constructed.

**Workflow**:

1. A change in any source table emits a change record with uid (or join key), source_table.
2. The materializer (DLT or streaming MERGE) receives change records from all source change streams (either unioned into a single stream or separate micro-batches).
3. The materializer re-evaluates the downstream derived record for the affected uid by joining latest snapshots of all related tables (fast because you only process the impacted uid(s)). Then MERGE the result into downstream table.

Implementation tip: use a small targeted re-compute pattern (recompute only the affected keys) rather than recomputing the entire derived dataset.

---


### 6) Serverless SQL concerns & recommended alternative

- Serverless SQL + Materialized Views: good for queries and dashboards. But:
-  Materialized Views might not support direct, reliable incremental updates from Delta CDF in all Databricks releases.
-  You may need to implement additional state (persisted table) and scheduled refreshes to ensure correctness.
- Recommendation for production: use Delta Change Data Feed + Delta Live Tables (DLT) or Spark Structured Streaming micro-batch with MERGE semantics. DLT has built-in guarantees, declarative pipelines, and quality controls.
If you still want Serverless SQL:
- Use Serverless SQL to query the changes and state Delta tables for reporting and ad-hoc queries.
- Use a streaming job (DLT / Structured Streaming) to produce/maintain the state and changes tables.

---

### 7) Best practices & operational tips

- Enable CDF for source tables when you need committed-level change events.
- Use deterministic hashes (md5/sha) for exactly-detecting changes ‚Äî avoid floating or non-deterministic columns in hash.
- Partition state and change tables by logical key (e.g., first letter of uid or year) to improve MERGE performance.
- Use MERGE for idempotent upserts into downstream tables.
- Checkpointing: always use streaming checkpointing for fault tolerance.
- Backfills: plan how to bootstrap state from full snapshots initially.
- Schema evolution: handle new columns by adding them to the hash or by versioning the hash algorithm; keep hash_version in state.
- Monitoring & lineage: capture commit_version, commit_ts, source_table in change records to trace origin and ordering.
- Throughput tuning: batch micro-batch interval and small-key re-evaluations keep compute efficient.
- Transactional guarantees: Always use Delta MERGE to ensure atomic update of state + downstream.

---

### 8) Trade-offs summary (short)

- Serverless SQL + MV
  -  Pros: easy to query from BI, low operational overhead for read-only views.
  -  Cons: limited streaming/CDC capabilities; might need scheduled full refreshes; weaker guarantees.
- DLT / Structured Streaming + MERGE
  -  Pros: Robust streaming, CDF support, idempotency, production-ready.
  -  Cons: More infrastructure (jobs) to manage; slightly more engineering up-front.

---

### 9) Suggested next steps & deliverables I can produce now

I can:

1. Provide a ready-to-run DLT pipeline code (Python) that:
   -  Reads CDF from 2‚Äì3 example source tables,
   -  Maintains per-table state,
   -  Computes per-row and per-column diffs,
   -  MERGEs changes into a denormalized downstream table.
2. Create a detailed runbook (bootstrap steps, backfill, restart procedures, monitoring queries).
3. Produce an example Serverless SQL + MV cookbook that shows an approach if you must use Serverless SQL.


Which of those would you like me to generate now? I can start with the DLT pipeline code (recommended) and include the runbook.



---


## Delta Live Tables (DLT) Python Pipeline

Nice ‚Äî below is a ready-to-run Delta Live Tables (DLT) Python pipeline that implements full per-table state, per-row & per-column hashes, column-level diff detection, and target MERGE/upsert into a denormalized downstream table. I include:

DLT pipeline code (single Python file / notebook cell) ‚Äî ready to paste into a Databricks DLT pipeline.

Short bootstrap steps (enable CDF, initial backfill).

Runbook notes and operational tips.

You can run this as a DLT pipeline (create a new pipeline in Databricks, choose Notebook/SQL, paste this Python notebook code, configure cluster, and start).


### DLT pipeline (Python)

```python
# Delta Live Tables pipeline: column-level incremental change detection across multiple source tables
# - Reads Delta Change Data Feed (CDF) from source tables
# - Computes per-row and per-column hashes
# - Uses dlt.apply_changes to maintain per-table state
# - Emits denormalized downstream table by recomputing only affected keys

import dlt
from pyspark.sql.functions import md5, concat_ws, col, lit, array, struct, expr
from pyspark.sql import DataFrame

########
# Configuration - adjust paths / table names
########
PUBLICATION_PATH = "/mnt/delta/f_publication"   # Delta table location for publications
AUTHOR_PATH      = "/mnt/delta/f_author"        # Delta table location for authors
PUBLISH_STATE    = "state.f_publication_state"  # DLT-managed state table name
AUTHOR_STATE     = "state.f_author_state"       # DLT-managed state table name
CHANGES_TABLE    = "changes.union_changes"      # intermediate unified change events
DENORM_TABLE     = "consumers.publication_author" # final denormalized table

# The list of columns we care to detect changes for each source table
PUB_KEY = "uid"
PUB_TRACK_COLS = ["pub_year", "title", "issn"]
AUTH_KEY = "author_id"
AUTH_TRACK_COLS = ["name", "affiliation", "email"]

# Helper: compute per-column hashes and a combined row_hash
def compute_hashes(df: DataFrame, cols: list, prefix: str = "") -> DataFrame:
    """
    Add per-column md5 hashes and a combined row_hash to the DataFrame.
    Assumes df contains the specified cols.
    """
    # per-column hash columns
    for c in cols:
        df = df.withColumn(f"hash_{c}", md5(col(c).cast("string")))

    # combined row hash deterministically ordered
    df = df.withColumn(
        "row_hash",
        md5(concat_ws("|", *[col(c).cast("string") for c in cols]))
    )
    return df

########
# 1) Raw CDF reads (as DLT views) - these will include metadata columns like:
#    _change_type, _commit_version, _commit_timestamp when read with readChangeFeed option
########
@dlt.table(
    comment="Raw change feed for publications (CDF)",
    table_properties = {"quality": "bronze"}
)
def raw_publication_cdf():
    # Ensure source has CDF enabled (see bootstrap steps)
    return (
        spark.read
             .format("delta")
             .option("readChangeFeed", "true")
             .option("startingVersion", 0)   # or startingTimestamp
             .load(PUBLICATION_PATH)
    )

@dlt.table(
    comment="Raw change feed for authors (CDF)",
    table_properties = {"quality": "bronze"}
)
def raw_author_cdf():
    return (
        spark.read
             .format("delta")
             .option("readChangeFeed", "true")
             .option("startingVersion", 0)
             .load(AUTHOR_PATH)
    )

########
# 2) Staged computed hashes for publications & authors
########
@dlt.table(
    comment="Publications with per-column hashes + row_hash (staged)",
    table_properties = {"quality": "silver"}
)
def staged_publication_hashes():
    df = dlt.read("raw_publication_cdf")
    df = df.selectExpr("*")  # keep metadata columns from CDF (_change_type, _commit_version, _commit_timestamp)
    df = compute_hashes(df, PUB_TRACK_COLS)
    # keep minimal set + metadata for apply_changes
    select_cols = [PUB_KEY, "row_hash", "_change_type", "_commit_version", "_commit_timestamp"] + [f"hash_{c}" for c in PUB_TRACK_COLS] + PUB_TRACK_COLS
    return df.select(*select_cols)

@dlt.table(
    comment="Authors with per-column hashes + row_hash (staged)",
    table_properties = {"quality": "silver"}
)
def staged_author_hashes():
    df = dlt.read("raw_author_cdf")
    df = compute_hashes(df, AUTH_TRACK_COLS)
    select_cols = [AUTH_KEY, "row_hash", "_change_type", "_commit_version", "_commit_timestamp"] + [f"hash_{c}" for c in AUTH_TRACK_COLS] + AUTH_TRACK_COLS
    return df.select(*select_cols)

########
# 3) Use dlt.apply_changes to maintain state tables per source
#    dlt.apply_changes(source, target, keys=..., sequence_by=...)
#    - `sequence_by` should be CDF commit/version column to ensure ordering
#    - The source must contain the metadata columns provided by CDF
########
@dlt.table(
    comment="Materialized publication state (last-seen row_hash & column hashes)",
    table_properties = {"quality": "gold"}
)
def publication_state():
    staged = dlt.read("staged_publication_hashes")
    # apply_changes will create/maintain the target state table
    dlt.apply_changes(
        staged,
        target = PUBLISH_STATE,
        keys = PUB_KEY,
        sequence_by = "_commit_version"
    )
    # dlt.apply_changes returns a DataFrame for the target; return dlt.read of target
    return dlt.read(PUBLISH_STATE)

@dlt.table(
    comment="Materialized author state (last-seen row_hash & column hashes)",
    table_properties = {"quality": "gold"}
)
def author_state():
    staged = dlt.read("staged_author_hashes")
    dlt.apply_changes(
        staged,
        target = AUTHOR_STATE,
        keys = AUTH_KEY,
        sequence_by = "_commit_version"
    )
    return dlt.read(AUTHOR_STATE)

########
# 4) Unified change events (union changed keys from multiple state updates)
#    We derive the set of affected publication keys (or author keys) so we can trigger targeted recompute.
########
@dlt.table(
    comment="Union of changed keys from source state changes",
    table_properties = {"quality": "silver"}
)
def union_changes():
    # Extract changed publication keys from staged -> publication_state comparison:
    pub_staged = dlt.read("staged_publication_hashes").select(PUB_KEY, "row_hash", "_commit_version", *[f"hash_{c}" for c in PUB_TRACK_COLS])
    pub_state  = dlt.read(PUBLISH_STATE).select(PUB_KEY, "row_hash" )
    # changed pubs: staged row_hash != state row_hash OR state missing (new)
    changed_pub = pub_staged.join(pub_state, on=PUB_KEY, how="left") \
                            .filter((col("row_hash") != col("row_hash")) | col("row_hash").isNull()) \
                            .selectExpr(f"'{PUBLICATION_PATH}' as source_table", PUB_KEY + " as key", "_commit_version as seq")

    # For authors, if downstream depends on author -> publication relationship, include changed author keys.
    auth_staged = dlt.read("staged_author_hashes").select(AUTH_KEY, "row_hash", "_commit_version")
    auth_state  = dlt.read(AUTHOR_STATE).select(AUTH_KEY, "row_hash")
    changed_auth = auth_staged.join(auth_state, on=AUTH_KEY, how="left") \
                              .filter((col("row_hash") != col("row_hash")) | col("row_hash").isNull()) \
                              .selectExpr(f"'{AUTHOR_PATH}' as source_table", AUTH_KEY + " as key", "_commit_version as seq")

    # Standardize union into key_type/key to be used for targeted recompute.
    # Here we keep them separate; final downstream recompute will decide how to find affected publications by joining author->publication mapping.
    union_df = changed_pub.unionByName(changed_auth, allowMissingColumns=True)
    return union_df

########
# 5) Final denormalized table: recompute only affected keys
#    Approach:
#      - Use union_changes to get the set of keys that changed
#      - For each affected publication uid, join latest snapshots of publication_state & author_state (or other tables)
#      - Upsert results into the denormalized target (idempotent)
########
@dlt.table(
    comment="Denormalized publication + author info (only recompute affected keys)",
    table_properties = {"quality": "gold"}
)
def publication_author_denorm():
    # Latest snapshots
    pubs = dlt.read(PUBLISH_STATE)    # contains uid, row_hash, hashes, pub columns...
    authors = dlt.read(AUTHOR_STATE)  # contains author_id, row_hash, name, affiliation...
    changes = dlt.read("changes.union_changes")

    # Strategy:
    # - For simplicity of this example assume there is a direct publication.author_id field,
    #   otherwise you'd map authors -> publications using a linking table.
    # - Collect affected publication uids from changes (either pub keys or author keys that map to pubs).
    affected_pub_keys = (
        changes.filter(col("source_table") == PUBLICATION_PATH).selectExpr("key as uid")
    )

    # If changes include author keys, map authors -> publications (example join with backref table "f_publication_author" if exists)
    # This example assumes publications have author_id column; adjust to your real schema.
    affected_author_keys = (
        changes.filter(col("source_table") == AUTHOR_PATH).selectExpr("key as author_id")
    )

    # Map changed authors to their publications (optional): do a lookup in latest publication snapshots
    pubs_by_author = pubs.join(affected_author_keys, pubs.author_id == affected_author_keys.author_id, how="inner") \
                         .select(pubs[PUBLICATION_PATH.split('/')[-1] if False else "uid"]) # placeholder

    # Union both sets of affected publication keys
    # NOTE: replace the mapping above with your actual author->publication mapping logic.
    affected = affected_pub_keys.selectExpr("uid").unionByName(pubs_by_author.selectExpr("uid")).distinct()

    # Recompute denorm for affected uids by joining latest snapshots
    denorm = affected.join(pubs, on="uid", how="left") \
                     .join(authors, pubs.author_id == authors.author_id, how="left") \
                     .select(
                         pubs.uid.alias("uid"),
                         pubs.pub_year,
                         pubs.title,
                         pubs.issn,
                         authors.author_id,
                         authors.name.alias("author_name"),
                         authors.affiliation.alias("author_affiliation"),
                         # include metadata for lineage
                         pubs.row_hash.alias("pub_row_hash"),
                         authors.row_hash.alias("author_row_hash")
                     )

    # Upsert into the denormalized table using apply_changes-style logic: we can use dlt.apply_changes directly
    # Build a source with denorm + required sequence_by
    dlt.apply_changes(
        denorm,
        target = DENORM_TABLE,
        keys = "uid",
        sequence_by = "pub_row_hash"  # sequence_by expects ordering column; use commit/version if available in denorm
    )

    return dlt.read(DENORM_TABLE)


```


---

### Bootstrap steps (one-time / before pipeline runs)


#### 1. Enable Change Data Feed (CDF) on each source Delta table (one-time SQL):

```sql
ALTER TABLE delta.`/mnt/delta/f_publication` SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
ALTER TABLE delta.`/mnt/delta/f_author` SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
```

2. Initial snapshot/backfill (if you already have production data):

- Option A (recommended): run a one-time backfill job that writes startingVersion=0 CDF reads into the DLT pipeline (the pipeline uses startingVersion=0 to process all historical commits).
- Option B: if you prefer, snapshot the initial state table by reading the full table and writing to the state.- location before enabling streaming.

3. Create a DLT pipeline in Databricks:

- In the UI, create a pipeline, pick Production/Development as you prefer.
- Add this notebook as the pipeline notebook.
- Configure cluster and permissions, and ensure the identity running the pipeline has read/write to the delta paths and metastore.
- Start pipeline ‚Äî DLT will materialize the tables in the order of dependencies.

---

### Operational notes & adjustments

- Sequence ordering: In production prefer using a committed sequence column such as _commit_version or _commit_timestamp from the CDF as sequence_by in dlt.apply_changes. In the code above, we used row-level hashes as a placeholder for sequence; replace with _commit_version from staged tables if you need strict ordering.
- Mapping author -> publication: Real schemas often use link/junction tables (publication_author). Adjust the publication_author_denorm() mapping logic to walk from changed authors to affected publication uids before recomputing.
- Column-level changed_columns: To emit an explicit list of changed columns, compute per-column hash comparisons in the staged vs state join and produce an array column changed_columns (e.g., array_remove(array(IF(hash_pub_year_changed,'pub_year',NULL), ...), NULL)).
- Partitioning & performance:
    - Partition state tables by a logical shard (e.g., hash(uid) % N) if you have very large cardinality to avoid small-file problems.
    - Tune DLT cluster size for expected throughput and distribution.
- Schema evolution: When you add new track columns, add them to the PUB_TRACK_COLS/AUTH_TRACK_COLS lists and implement hash_version stored in state so you can detect when hash algorithm changed and trigger full re-hash/backfill.
- Idempotency: dlt.apply_changes + Delta MERGE guarantee idempotent application when using keys and sequence_by.
- Monitoring:
    - Use DLT's pipeline UI for table-level job/quality metrics.
    - Capture metrics like change-rate, UIDs processed/minute, and failed merges.

---

### Quick example: emitting changed_columns

Inside the staged_publication_hashes() or in a small batch compare step, you can produce a changed_columns array like:
```sql
SELECT
  b.uid,
  array_remove(array(
    CASE WHEN s.hash_pub_year IS NULL OR s.hash_pub_year <> b.hash_pub_year THEN 'pub_year' END,
    CASE WHEN s.hash_title    IS NULL OR s.hash_title    <> b.hash_title    THEN 'title' END,
    CASE WHEN s.hash_issn     IS NULL OR s.hash_issn     <> b.hash_issn     THEN 'issn' END
  ), NULL) as changed_columns
FROM staged_publication_hashes b
LEFT JOIN state.f_publication_state s ON b.uid = s.uid
WHERE s.row_hash IS NULL OR s.row_hash <> b.row_hash
```

Save that into a publication_changes table and feed downstream logic from it.


---

### Final recommendations (short)

- Use Delta CDF + DLT (this code) for robust production CDC with per-column diff capabilities.
- Use Serverless SQL only for ad-hoc analysis and reporting on the resulting state and changes tables ‚Äî not for the primary CDC enforcement.
- Keep sequence_by as _commit_version (from CDF) for strict ordering and consistency.
- Add a small author->publication mapping job or view if your author changes should re-evaluate publications.

---

If you want, I can now:
- Convert the publication_author_denorm() recompute logic into a complete real-world mapping using a publication_author linking table (if you paste your schema), or
- Generate a ready-to-run SQL snippet that populates a changed_columns table and an example MERGE into downstream, or
- Produce a runbook (step-by-step commands) tailored to your production setup.
Which of those should I do next?


---


###  Databricks SQL snippet

Excellent ‚Äî here‚Äôs a ready-to-run Databricks SQL snippet that:
1. Detects and logs column-level changes in a source Delta table (f_publication) using Change Data Feed (CDF).
2. Populates a changed_columns Delta table with detailed information on what changed (table, primary key, changed columns, old/new values, timestamp).
3. Performs an incremental MERGE into a downstream table (dim_publication) using that change log.


‚úÖ 1. Enable Change Data Feed on the Source Table

```sql
-- Enable CDF for your source table
ALTER TABLE f_publication SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
```


‚úÖ 2. Create a Change Tracking Table

This table will store column-level change details.
```sql
CREATE TABLE IF NOT EXISTS changed_columns (
    table_name STRING,
    uid STRING,
    column_name STRING,
    old_value STRING,
    new_value STRING,
    change_type STRING,
    change_timestamp TIMESTAMP
) USING delta;
```


‚úÖ 3. Detect Column-Level Changes Using CDF

The following query compares the before and after versions of each row emitted by the Delta CDF stream and extracts which columns have changed.

```sql
-- Replace timestamp with your last successful pipeline run time
DECLARE last_run TIMESTAMP = TIMESTAMP('2025-11-05T00:00:00Z');

INSERT INTO changed_columns
SELECT
  'f_publication' AS table_name,
  b.uid,
  c.column_name,
  c.old_value,
  c.new_value,
  CASE WHEN b._change_type = 'update_postimage' THEN 'UPDATE'
       WHEN b._change_type = 'insert' THEN 'INSERT'
       WHEN b._change_type = 'delete' THEN 'DELETE'
  END AS change_type,
  b._commit_timestamp AS change_timestamp
FROM (
  SELECT *
  FROM table_changes('f_publication', last_run)
  WHERE _change_type IN ('update_postimage', 'insert', 'delete')
) b
LATERAL VIEW explode(
  FILTER(
    ARRAY(
      NAMED_STRUCT('column_name', 'pub_year', 'old_value', CAST(b.pub_year_before AS STRING), 'new_value', CAST(b.pub_year AS STRING)),
      NAMED_STRUCT('column_name', 'title', 'old_value', CAST(b.title_before AS STRING), 'new_value', CAST(b.title AS STRING)),
      NAMED_STRUCT('column_name', 'issn', 'old_value', CAST(b.issn_before AS STRING), 'new_value', CAST(b.issn AS STRING))
    ),
    x -> x.old_value IS DISTINCT FROM x.new_value
  )
) c;
```

üí° This approach uses LATERAL VIEW + explode() to generate one row per changed column.
If your table has many columns, you can dynamically generate the array using the information schema.


‚úÖ 4. Example MERGE into Downstream Table

Now we can propagate only changed records into a downstream table such as dim_publication.

```sql
MERGE INTO dim_publication AS tgt
USING (
  SELECT DISTINCT uid
  FROM changed_columns
  WHERE table_name = 'f_publication'
    AND change_timestamp >= (SELECT max(change_timestamp) FROM dim_publication)
) AS src
ON tgt.uid = src.uid
WHEN MATCHED THEN
  UPDATE SET
    tgt.pub_year = src.pub_year,
    tgt.title = src.title,
    tgt.issn = src.issn,
    tgt.last_update = current_timestamp()
WHEN NOT MATCHED THEN
  INSERT (uid, pub_year, title, issn, last_update)
  VALUES (src.uid, src.pub_year, src.title, src.issn, current_timestamp());
```


‚úÖ 5. Optional Materialized View (for Multi-Table Incremental Tracking)

If you want to automatically combine changes from multiple source tables, create an MV to union all change logs:

```sql
CREATE OR REPLACE MATERIALIZED VIEW mv_all_changes AS
SELECT - FROM changed_columns
WHERE table_name IN ('f_publication', 'f_author', 'f_journal');
```

Then use this MV as your central change stream for downstream pipelines.


‚öôÔ∏è Summary of Flow

1. f_publication (Delta table with CDF) ‚Üí detects fine-grained updates.
2. changed_columns (Delta log table) ‚Üí stores per-column change info.
3. dim_publication (downstream) ‚Üí receives updates via incremental MERGE.
4. Optional MV ‚Üí aggregates all table changes for unified incremental load.

Would you like me to extend this snippet to include an automated Serverless SQL Task pipeline (so it runs incrementally and updates the last_run timestamp automatically after each successful load)?


---


## Production-Grade Databricks Serverless SQL Pipeline


Perfect ‚úÖ ‚Äî here‚Äôs a ready-to-run, production-grade Databricks Serverless SQL pipeline setup that automates the incremental column-level change detection and downstream MERGE, including state management (last_run tracking).


### ‚öôÔ∏è Overall Design

**Goal**:

Detect fine-grained changes (column-level) from multiple Delta tables and load them incrementally into downstream tables ‚Äî automatically via a Serverless SQL Task pipeline.

### 1. Create a Job State Table

Stores the timestamp of the last successful pipeline execution.

```sql
CREATE TABLE IF NOT EXISTS job_state (
  job_name STRING,
  last_run TIMESTAMP
) USING delta;

-- Initialize if empty
INSERT INTO job_state VALUES ('publication_cdc_job', TIMESTAMP('2025-11-05T00:00:00Z'))
  ON CONFLICT DO NOTHING;
```

### 2. Create a Table for Changed Columns

```sql
CREATE TABLE IF NOT EXISTS changed_columns (
    table_name STRING,
    uid STRING,
    column_name STRING,
    old_value STRING,
    new_value STRING,
    change_type STRING,
    change_timestamp TIMESTAMP
) USING delta;
```

### 3. Create a Downstream Table

```sql
CREATE TABLE IF NOT EXISTS dim_publication (
    uid STRING,
    pub_year STRING,
    title STRING,
    issn STRING,
    last_update TIMESTAMP
) USING delta;
```


### 4. Incremental Change Detection SQL Task

**Task**: detect_publication_changes.sql

- This query uses Delta Change Data Feed (CDF) and automatically reads last_run from job_state.
```sql
-- Get the last run timestamp
DECLARE last_run TIMESTAMP;
SET last_run = (SELECT COALESCE(max(last_run), TIMESTAMP('2025-01-01')) 
                FROM job_state WHERE job_name = 'publication_cdc_job');

-- Insert column-level changes into change log
INSERT INTO changed_columns
SELECT
  'f_publication' AS table_name,
  b.uid,
  c.column_name,
  c.old_value,
  c.new_value,
  CASE 
    WHEN b._change_type = 'update_postimage' THEN 'UPDATE'
    WHEN b._change_type = 'insert' THEN 'INSERT'
    WHEN b._change_type = 'delete' THEN 'DELETE'
  END AS change_type,
  b._commit_timestamp AS change_timestamp
FROM (
  SELECT *
  FROM table_changes('f_publication', last_run)
  WHERE _change_type IN ('update_postimage', 'insert', 'delete')
) b
LATERAL VIEW explode(
  FILTER(
    ARRAY(
      NAMED_STRUCT('column_name', 'pub_year', 'old_value', CAST(b.pub_year_before AS STRING), 'new_value', CAST(b.pub_year AS STRING)),
      NAMED_STRUCT('column_name', 'title', 'old_value', CAST(b.title_before AS STRING), 'new_value', CAST(b.title AS STRING)),
      NAMED_STRUCT('column_name', 'issn', 'old_value', CAST(b.issn_before AS STRING), 'new_value', CAST(b.issn AS STRING))
    ),
    x -> x.old_value IS DISTINCT FROM x.new_value
  )
) c;
```

### 5. Incremental Merge Task

**Task**: merge_publication_changes.sql

```sql
MERGE INTO dim_publication AS tgt
USING (
  SELECT DISTINCT uid
  FROM changed_columns
  WHERE table_name = 'f_publication'
) AS src
ON tgt.uid = src.uid
WHEN MATCHED THEN
  UPDATE SET
    tgt.pub_year = src.pub_year,
    tgt.title = src.title,
    tgt.issn = src.issn,
    tgt.last_update = current_timestamp()
WHEN NOT MATCHED THEN
  INSERT (uid, pub_year, title, issn, last_update)
  VALUES (src.uid, src.pub_year, src.title, src.issn, current_timestamp());
```

### 6. Update Job State

**Task**: update_job_state.sql

```sql
INSERT OVERWRITE job_state
SELECT 'publication_cdc_job' AS job_name, current_timestamp() AS last_run;
```

### 7. Serverless SQL Task Pipeline Configuration

In Databricks UI (or YAML if using Databricks Workflows API):

```yaml
name: Publication Incremental CDC Pipeline
tasks:
  - name: detect_publication_changes
    type: sql
    source: dbfs:/sql/detect_publication_changes.sql
    warehouse_id: <your_serverless_warehouse_id>

  - name: merge_publication_changes
    type: sql
    depends_on: [detect_publication_changes]
    source: dbfs:/sql/merge_publication_changes.sql
    warehouse_id: <your_serverless_warehouse_id>

  - name: update_job_state
    type: sql
    depends_on: [merge_publication_changes]
    source: dbfs:/sql/update_job_state.sql
    warehouse_id: <your_serverless_warehouse_id>
```

### üß† Key Behavior:

- Each run automatically reads the previous run‚Äôs timestamp from job_state.
- Only new changes (post last run) are processed.
- After successful merge, the pipeline updates job_state for the next run.
- Can run on Databricks SQL Serverless Warehouse for cost-effective scaling.

### 8. Optional: Extend for Multiple Source Tables

To support multiple tables (e.g., f_author, f_journal):
- Create one detect_<table>_changes.sql per table.
- Use a single shared changed_columns table.
- Union all into a materialized view:
```sql
CREATE OR REPLACE MATERIALIZED VIEW mv_all_changes AS
SELECT - FROM changed_columns
WHERE change_timestamp > current_timestamp() - INTERVAL 7 DAYS;
Then use that MV for any downstream delta merge or lineage tracking.
```

### ‚úÖ Result

This design achieves:
- Incremental column-level change detection
- Fully automated state management
- Serverless SQL performance and cost efficiency
- Scalable multi-table support with optional MV layer

Would you like me to generate the PySpark equivalent (for use in Delta Live Tables or notebooks) that does the same pipeline with programmatic state tracking and logging to Delta?








