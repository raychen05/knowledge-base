## Best Approach: Event-Driven Triggering with Structured Streaming & DLT Pipelines


When a workflow job has multiple upstream data sources (e.g., Live Tables, Regular Delta Tables, S3 Locations) and you want any update to trigger the job, you should use a combination of Structured Streaming Triggers and Delta Live Tables (DLT) Pipelines.


---

üìå Problem Definition

You have a job that needs to be triggered when any of the following sources is updated:
1.	Live Tables (CDC)
2.	Regular Delta Tables (Batch Processing)
3.	S3 Location (External Data Sources)

---

üîë Approach 1: Structured Streaming + DLT Pipelines (Recommended)

How it Works
1.	Use Structured Streaming to monitor all upstream sources as streaming DataFrames.
2.	Implement DLT Pipelines for Live Tables to maintain incremental updates.
3.	Use a single Workflow Job that consolidates data from all sources and triggers downstream processing.

‚∏ª

üìã Example Setup

üî® Step 1: Define Sources

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Monitoring a Live Table (CDC-based streaming)
live_table_stream = spark.readStream.format("delta").table("live_table_source")

# Monitoring a Regular Delta Table (using Structured Streaming)
regular_table_stream = spark.readStream.format("delta").table("regular_delta_table_source")

# Monitoring an S3 Location (JSON/CSV files etc.)
s3_stream = spark.readStream.format("cloudFiles") \
    .option("cloudFiles.format", "json") \
    .option("cloudFiles.schemaLocation", "dbfs:/mnt/delta/schema/") \
    .load("s3://your-bucket/path/")
```

üî® Step 2: Combine Streams

```python
from pyspark.sql import DataFrame
from pyspark.sql.functions import col

# Union all streams to create a single consolidated stream
combined_stream = live_table_stream.union(regular_table_stream).union(s3_stream)

# Process the combined stream and write to a Delta table
query = (combined_stream.writeStream
         .format("delta")
         .outputMode("append")
         .option("checkpointLocation", "/mnt/delta/checkpoints/combined_stream")
         .table("consolidated_output_table"))
```


üî® Step 3: Define Workflow Job with Triggers

- 	Create a Databricks Workflow.
- 	Add a single job with the above notebook as the task.
- 	Use Continuous Trigger Mode if you need near-real-time processing.


---


üîë Approach 2: Delta Live Tables (DLT) Event Triggering (Alternative)

If all your sources are Delta Tables or Live Tables, you can leverage DLT Pipelines.

1.	Define each source as a LIVE TABLE or STREAM.
2.	Use APPLY CHANGES INTO for CDC-based sources.
3.	Combine sources using a MERGE or JOIN operation.


üìã Example DLT Pipeline Definition

```sql
CREATE LIVE TABLE consolidated_output_table
TBLPROPERTIES ("quality" = "gold")
AS
SELECT * FROM
  (SELECT * FROM LIVE.live_table_source
   UNION ALL
   SELECT * FROM LIVE.regular_delta_table_source
   UNION ALL
   SELECT * FROM LIVE.s3_stream_source)
```


--- 


üîë Approach 3: Trigger-Based Orchestration (If API Triggering is Required)

If you need to trigger a workflow job only when certain conditions are met:

1.	Monitor upstream tables via Structured Streaming.
2.	On a detected update, use the Databricks REST API to trigger downstream processing.

```python
import requests

def trigger_workflow(job_id, token, host):
    url = f"{host}/api/2.1/jobs/run-now"
    headers = {"Authorization": f"Bearer {token}"}
    payload = {"job_id": job_id}
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print("Workflow triggered successfully!")
    else:
        print(f"Failed to trigger workflow: {response.text}")
```

---

üìå Comparison

| Approach                     | Pros                                                   | Cons                               |
|------------------------------|--------------------------------------------------------|------------------------------------|
| **Structured Streaming + DLT (Recommended)** | Real-time, seamless integration of various sources, scalable. | Slightly complex setup.           |
| **DLT Pipeline Only**         | Simple, high performance for Delta sources.            | Limited to Delta sources.         |
| **Trigger-Based Orchestration** | High control, easy API-based triggering.               | More manual management.           |


---

üìå Recommendation

Use Approach 1 (Structured Streaming + DLT Pipelines) for maximum flexibility and compatibility with Live Tables, Delta Tables, and S3 locations.

- 	This approach ensures real-time processing and unified event handling.
- 	Works well with Databricks Workflows for orchestration.
    

---

‚úÖ Detecting Deleted Records with Change Data Feed (CDF)


By default, streaming reads from a Delta table do not detect deletions. They only detect newly appended rows. However, if you enable Change Data Feed (CDF), deletions can be detected and processed.

1.	Enable CDF on Your Delta Table

```sql
ALTER TABLE arga_analytics_data_dev.researchers_v1_0.test SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
```

2.	Stream Read with CDF Enabled

```scala
val deltaStream = spark.readStream
  .format("delta")
  .option("readChangeData", "true")
  .option("startingVersion", "0")  // Or use 'startingTimestamp'
  .table("your_table_name")
```

üîç How Deletions Are Detected

When CDF is enabled, streaming queries can detect different types of changes:
- 	insert - New records added.
- 	update_preimage - The old version of rows before they were updated.
- 	update_postimage - The new version of rows after they were updated.
- 	delete - Rows that were deleted.

These change types are stored in a special metadata column called _change_type.

---

üìå Checking for Deleted Records

To filter deleted records:

```scala
import org.apache.spark.sql.functions._

val deletedRecords = deltaStream.filter(col("_change_type") === "delete")

deletedRecords.writeStream
  .format("console")
  .start()
  .awaitTermination()
```


üî• Important Points

1.	Enabling CDF is Required: Without CDF, deletions are completely ignored by streaming queries.
2.	Storage Cost Impact: CDF maintains additional data for each change, so storage costs may increase.
3.	Retention Period: You can configure how long change data is retained using the property:

```sql
ALTER TABLE your_table_name SET TBLPROPERTIES (delta.changeDataFeed.retentionDuration = '30 days')
```

---

## üî• Implementing a Full Streaming Pipeline with Change Data Feed (CDF)


We‚Äôll build a pipeline that:
1.	Reads a Delta table as a stream, capturing inserts, updates, and deletions.
2.	Processes changes based on _change_type (insert, update_preimage, update_postimage, delete).
3.	Outputs the changes to a console (for simplicity) or another Delta table.

üìå Step 1: Enable CDF on Delta Table

First, enable CDF on your Delta table:

```sql
ALTER TABLE your_table_name SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
```


üìå Step 2: Write Data to Delta Table (Simulate Changes)

```scala
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._

val spark = SparkSession.builder()
  .appName("DeltaTableCDFExample")
  .getOrCreate()

import spark.implicits._

val tableName = "your_table_name"

// Initial write (creating the table)
val initialData = Seq(
  ("pguid_1", 100, 200),
  ("pguid_2", 150, 300)
).toDF("pguid", "wos_count", "tot_cites")

initialData.write.format("delta").mode("overwrite").saveAsTable(tableName)

// Simulate Update & Deletion
val updatedData = Seq(
  ("pguid_1", 200, 400)  // Update pguid_1
).toDF("pguid", "wos_count", "tot_cites")

updatedData.write.format("delta").mode("overwrite").option("mergeSchema", "true").saveAsTable(tableName)

val deletedData = Seq(
  ("pguid_2", 150, 300)
).toDF("pguid", "wos_count", "tot_cites")

deletedData.write.format("delta").mode("overwrite").option("mergeSchema", "true").saveAsTable(tableName)
```

üìå Step 3: Stream Read Using CDF

Now, let‚Äôs read from the Delta table with CDF enabled.

```scala
val deltaStream = spark.readStream
  .format("delta")
  .option("readChangeData", "true")
  .option("startingVersion", "0") // You can also use a timestamp
  .table(tableName)
```

üìå Step 4: Process Changes by Type

```scala
import org.apache.spark.sql.functions._

val processedStream = deltaStream.select("*")
  .withColumn("change_type", col("_change_type"))

// Handling Inserts
val inserts = processedStream.filter(col("change_type") === "insert")

// Handling Updates
val updates = processedStream.filter(col("change_type") === "update_postimage")

// Handling Deletes
val deletes = processedStream.filter(col("change_type") === "delete")
```

üìå Step 5: Write Output to Console (Or Another Delta Table)

```scala
// Display Inserts
inserts.writeStream
  .format("console")
  .outputMode("append")
  .start()

// Display Updates
updates.writeStream
  .format("console")
  .outputMode("append")
  .start()

// Display Deletes
deletes.writeStream
  .format("console")
  .outputMode("append")
  .start()
```

üìå Step 6: Handling Upstream Data Sources in Workflow Jobs

You can configure the workflow job to trigger when any of the upstream data sources (live tables, regular delta tables, S3 locations) is updated. For Delta tables with CDF enabled, you can rely on triggers that check for the presence of new _change_type entries (insert, update_postimage, delete).


üåü Explanation
- 	Insert Handling: New rows are directly processed.
- 	Update Handling: Only update_postimage is processed to keep the latest version of rows.
- 	Delete Handling: Rows with _change_type = 'delete' are processed separately.
    

--

## Triggering Downstream Jobs Based on CDF Changes in Databricks Workflow

When your pipeline involves multiple jobs triggered by changes in Live Tables, regular Delta Tables, or S3 locations, you need a reliable way to detect changes and trigger dependent jobs. We‚Äôll focus on CDF-enabled Delta Tables here.


üîë Approach

1.	Track Delta Table Changes (Insert, Update, Delete) using CDF.
2.	Trigger Downstream Jobs via Event-Driven Architecture or Databricks Workflows.
3.	Chain Jobs using the Databricks Workflow API or Delta Live Tables (DLT) Triggering.


üìå Step 1: Write Changes to a Notification Table

We‚Äôll write change records (inserts, updates, deletes) to a notification table that will act as a trigger source for downstream jobs.

```scala
import org.apache.spark.sql.functions._

val notificationTable = "notification_table"

// ProcessedStream from previous steps
val changesWithTriggerFlag = processedStream
  .withColumn("trigger_time", current_timestamp())
  .select("pguid", "wos_count", "tot_cites", "_change_type", "trigger_time")

changesWithTriggerFlag.writeStream
  .format("delta")
  .option("checkpointLocation", "s3://your-checkpoint-path/notification/")
  .table(notificationTable)
```

---

üìå Step 2: Configure a Workflow Job Trigger

In Databricks Workflows, you can create jobs and set dependencies. We‚Äôll trigger downstream jobs based on new entries in the notification table.

Workflow Job Configuration

1.	Go to the Workflows tab in your Databricks workspace.
2.	Click Create Job.
3.	Define a Triggering Job (e.g., Change Detection Job).
4.	Add Dependent Jobs (e.g., Aggregation Job, Notification Job).

---

üìå Step 3: Use Delta Table Versioning & CDF for Triggering

To make sure downstream jobs only process new changes, use a version checkpoint or a timestamp.

```scala
val lastProcessedVersion = spark.read.table("notification_table")
  .select(max("version"))
  .as[Long]
  .collect()
  .headOption
  .getOrElse(0L)

val newChanges = spark.read.format("delta")
  .option("readChangeData", "true")
  .option("startingVersion", lastProcessedVersion.toString)
  .table(notificationTable)

newChanges.writeStream
  .format("console")
  .start()
```

---

üìå Step 4: Chain Jobs Using Databricks Workflow API

To chain jobs with triggers:

```python
from databricks_cli.jobs.api import JobsApi
from databricks_cli.sdk import ApiClient

client = ApiClient(
    host = 'https://<your-databricks-workspace>',
    token = 'your-databricks-token'
)

jobs_api = JobsApi(client)

# Trigger the downstream job
trigger_job_response = jobs_api.run_now(job_id="<downstream_job_id>")
print(trigger_job_response)
```

You can also use the Jobs API to schedule, trigger, and monitor jobs programmatically.

---

üìå Step 5: Notification Table Cleanup (Optional)

To avoid accumulating old entries, periodically clean up the notification_table.

```sql
DELETE FROM notification_table WHERE trigger_time < current_timestamp() - INTERVAL 7 DAYS;
```

---

‚úÖ Summary

- 	Stream Changes using CDF and write them to a notification_table.
- 	Trigger downstream jobs by detecting new rows in notification_table.
- 	Chain jobs using the Databricks Workflows UI or the Jobs API.
- 	Optionally clean up the notification table to maintain performance.

---



‚úÖ Option 1: Single Job Handling All Operations (Recommended for Simplicity)

- 	üî• Process appends, updates, and deletions within the same job.
- 	Ideal if all changes (inserts, updates, deletes) require the same downstream processing logic or similar post-processing steps.
- 	Easier to manage and monitor since all changes are processed in a unified stream.

Example: Processing All Changes in a Single Job

```scala
val deltaStream = spark.readStream
  .format("delta")
  .option("readChangeData", "true")
  .option("startingVersion", "0")
  .table("source_table")

val processedStream = deltaStream
  .withColumn("trigger_time", current_timestamp())
  .select("pguid", "wos_count", "tot_cites", "_change_type", "trigger_time")

processedStream.writeStream
  .format("delta")
  .option("checkpointLocation", "s3://your-checkpoint-path/changes/")
  .table("processed_notification_table")
```

---

‚úÖ Option 2: Separate Jobs for Append, Update, and Delete (Recommended for Complex Pipelines)

- 	üî• Create separate jobs for processing appends, updates, and deletions.
- 	Useful if each type of change needs different processing logic or triggers different downstream jobs.
- 	More flexible for advanced pipelines where each operation needs unique handling.

Example: Separate Job Handling
	1.	Append Job: Processes only new records (_change_type = 'insert').
	2.	Update Job: Processes modified records (_change_type = 'update').
	3.	Delete Job: Processes deleted records (_change_type = 'delete').

```scala
// Append Job
val appendStream = deltaStream.filter(col("_change_type") === "insert")
appendStream.writeStream
  .format("delta")
  .option("checkpointLocation", "s3://your-checkpoint-path/appends/")
  .table("append_notification_table")

// Update Job
val updateStream = deltaStream.filter(col("_change_type") === "update")
updateStream.writeStream
  .format("delta")
  .option("checkpointLocation", "s3://your-checkpoint-path/updates/")
  .table("update_notification_table")

// Delete Job
val deleteStream = deltaStream.filter(col("_change_type") === "delete")
deleteStream.writeStream
  .format("delta")
  .option("checkpointLocation", "s3://your-checkpoint-path/deletions/")
  .table("delete_notification_table")
```

---

üîë Best Practice Recommendation

- 	If the processing logic is similar for all changes, go with Option 1 (Single Job).
- 	If the processing logic differs significantly, or you need different triggers or workflows for each type, choose Option 2 (Separate Jobs).

üåü Hybrid Approach

You can also use a hybrid approach, where you have a main processing job for common logic and trigger additional jobs based on _change_type.


---

For your use case, using regular Delta tables is typically sufficient and efficient for daily batch updates. However, using Delta Live Tables (DLT) provides additional benefits if you need enhanced reliability, monitoring, and automated error handling.


| Aspect              | Regular Delta Table (Batch)                         | Delta Live Table (DLT)                                        |
|---------------------|-----------------------------------------------------|---------------------------------------------------------------|
| **Reliability**     | Manual handling of errors and retries              | Automatic error handling, recovery, and retries               |
| **Monitoring**      | Limited to manual logging and metrics collection   | Built-in monitoring, lineage tracking, and logging            |
| **Schema Evolution**| Handled by setting `.option("mergeSchema", "true")` | Automatic schema enforcement and evolution                    |
| **Complexity**      | More control but requires manual management        | Simplifies complex pipelines with declarative syntax          |
| **Performance**     | High, but dependent on your implementation         | Optimized performance tuning by Databricks                    |
| **Triggering Pipeline** | Databricks Workflows, Airflow, or Cron Jobs       | DLT Pipelines with event-based triggers or scheduled triggers |



Recommendation

- 	If you need simplicity and monitoring features, use Delta Live Tables (DLT).
- 	If you need full control over processing logic and optimization, and monitoring is handled externally, use Regular Delta Tables.
- 	Given your scenario of daily batch updates, DLT is recommended if you want automated monitoring and error handling.

Triggering the Pipeline

Use Databricks Workflows for triggering your jobs. This allows you to:
1.	Schedule jobs daily or on a custom schedule.
2.	Chain multiple jobs and handle dependencies using tasks.
3.	Trigger jobs on specific conditions, such as new data availability in your S3 location.
