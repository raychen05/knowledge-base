## Delta Lake Data Versioning

When managing multiple tables in a single schema in Delta Lake, the decision to version data at the schema level, table level, or both depends on the specific use cases and the nature of your data. Here’s a breakdown of the best practices for versioning data:

---

### 1. Table-Level Versioning

Best practice for versioning at the table level is typically the most common and flexible approach, especially when:
- 	Different tables have different update cycles or requirements.
- 	Some tables need frequent schema changes or updates while others do not.
- 	Each table represents a distinct logical dataset or entity (e.g., publications, grants, funding_sources), and versioning these tables individually helps manage each table’s evolution independently.

Advantages:
- 	Granularity: Allows version control at a fine-grained level for each table, so if only one table requires an update or schema change, you don’t need to version the entire schema.
- 	Flexibility: It is easier to evolve and maintain tables with different lifecycle requirements independently.
- 	Isolation: Changes in one table’s schema (e.g., adding columns) won’t affect others, providing better isolation for versioning changes.

When to use table-level versioning:
- 	If different tables represent independent entities with different schemas (e.g., wos_publications, grants, researchers).
- 	When you want to apply schema evolution or changes to specific tables while leaving others unchanged.
- 	If complex data pipelines require maintaining multiple versions of different datasets.

Example:
- 	For the wos_publications table, you can maintain version history via time travel:
```python
# Retrieve historical data from a previous version of the table
historical_df = spark.read.format("delta").option("timestampAsOf", "2023-01-01").table("publications.wos_publications")
```
In this case, only wos_publications table would have its own versioning, independent of other tables like grants

---


### 2. Schema-Level Versioning

Best practice for versioning at the schema level is generally recommended when:
- 	Multiple tables have shared schema structures (i.e., similar column names and data types).
- 	You want to track changes across the entire schema rather than on individual tables.
- 	You want to ensure cohesiveness across all tables within the schema when applying versioning (e.g., all tables evolve simultaneously, so they remain in sync).

Advantages:
- 	Unified versioning: Tracks the schema changes across the entire schema, which is useful if multiple tables have closely related data and schema dependencies.
- 	Consistency: Ensures that all tables evolve together, maintaining consistency in the schema structure.
- 	Simplified management: In cases where tables share the same schema structure, versioning at the schema level can simplify managing changes for all tables.

When to use schema-level versioning:
- 	If you have multiple tables that share the same or similar schema (e.g., wos_publications, grants, patents all have similar data structures).
- 	If changes to the schema (e.g., adding/removing columns) affect all tables in the schema.
- 	When coordinating schema changes across tables (e.g., adding new columns across tables at the same time).

Example:
- 	If the schema publications has several tables (wos_publications, grants, patents), you might maintain versions of the schema itself. Any changes to the schema, such as new columns or data types, could be propagated across all tables within the schema.

```python
# Creating a new version of the schema with all changes applied
spark.sql("ALTER SCHEMA publications SET VERSION 'v2'")
```

---

### 3. Combined Table-Level and Schema-Level Versioning

In some scenarios, combined versioning at both the table and schema level is the most comprehensive approach. This can be useful in more complex systems where some tables evolve independently, while others require synchronized changes.

Advantages:
- 	Comprehensive control: Provides flexibility for versioning some tables independently (table-level) while keeping the schema as a whole synchronized (schema-level).
- 	Flexibility: Supports cases where certain tables need to be versioned independently, while schema-level changes (like adding shared columns or partition keys) can apply to all tables at once.

When to use combined versioning:
- 	If the schema has some tables that evolve independently (e.g., versioned separately) but need to be synchronized with others in certain cases.
- 	If you need to version schema-wide changes (e.g., adding a shared column to all tables) but also need to version individual tables independently for other changes.

Example:
- 	Table-level: Version wos_publications as v1, then update its schema independently.
- 	Schema-level: For schema-wide changes, you might change the entire publications schema version to v2.


---

### Best Practices for Versioning in Delta Lake

1.	Table-Level Versioning: Generally preferred for isolated entities or independent datasets. Allows flexible and isolated updates without affecting other data.
2.	Schema-Level Versioning: Useful when tables share a similar structure and you want to enforce schema consistency across multiple tables.
3.	Combined Approach: Use both methods when your data pipelines are complex and you need flexibility in managing independent tables and cohesive schema-wide changes.


Example of Versioning Implementation:
- 	Table-level versioning:

```python
# Using time travel on a specific table
df_v1 = spark.read.format("delta").option("versionAsOf", 1).table("publications.wos_publications")
```

- 	Schema-level versioning:
    
```python
# Alter schema version
spark.sql("ALTER SCHEMA publications SET VERSION 'v2'")
```


Conclusion:
- 	Use table-level versioning when tables have different data structures or lifecycle requirements.
- 	Use schema-level versioning when multiple tables share the same schema or should evolve together.
- 	Use both methods if your data structure requires flexibility in handling independent updates to tables while maintaining schema-level consistency.

---


## Delta Lake Naming  


Naming Conventions for Live Tables (Streaming) and Regular Delta Tables (Baseline) in Delta Lake

When managing both streaming (live) updates and regular (baseline) batch updates in Delta Lake, using a clear and consistent naming convention is crucial for maintainability and clarity. Below are best practices for naming live tables and baseline tables:


1️⃣ Naming Live (Streaming) Tables

Live tables process continuous updates (streaming ingestion). These tables are typically used for near real-time data processing.

Recommended Naming Conventions:
- 	Use a suffix like _live, _stream, or _incremental
- 	Use a separate schema for live streaming tables
- 	Keep the name consistent with its corresponding baseline table

| Table Type                             | Suggested Naming Pattern    | Example                  |
|----------------------------------------|-----------------------------|--------------------------|
| **Streaming (Live) Table**             | {base_table}_live           | publications_live        |
| **Streaming Table (Alternative)**      | {base_table}_stream         | publications_stream      |
| **Streaming Table with Schema**       | streaming.{base_table}      | streaming.publications    |


Example Usage:


```sql
CREATE TABLE streaming.publications 
AS SELECT * FROM delta.`/mnt/data/publications` WHERE 1=0;
```

```python
df.writeStream \
  .format("delta") \
  .option("checkpointLocation", "/mnt/checkpoints/publications") \
  .outputMode("append") \
  .table("streaming.publications")
```

---

2️⃣ Naming Regular (Baseline) Delta Tables

Baseline tables store batch-processed historical data, usually loaded periodically.

Recommended Naming Conventions:
- 	Use a clear name without a suffix (default for batch tables)
- 	If multiple baseline versions exist, use _v1, _v2, etc.
- 	Consider using a separate schema for batch data (historical, batch, etc.)

| Table Type                    | Suggested Naming Pattern   | Example                |
|-------------------------------|----------------------------|------------------------|
| **Regular Delta Table**        | {base_table}               | publications           |
| **Versioned Delta Table**      | {base_table}_v1            | publications_v1        |
| **Baseline Table with Schema** | historical.{base_table}    | historical.publications |



Example Usage:


```sql
CREATE TABLE historical.publications 
AS SELECT * FROM delta.`/mnt/data/publications` WHERE 1=0;
```

```pthon
df.write.format("delta").mode("overwrite").saveAsTable("historical.publications")
```


---


3️⃣ Unified Approach: Schema-Based vs. Suffix-Based Naming

Option 1: Schema-Based Naming (Recommended)

Using separate schemas for streaming vs. batch improves clarity and data governance.

| Schema      | Table Name   | Purpose                 |
|-------------|--------------|-------------------------|
| streaming   | publications | Live streaming updates  |
| historical  | publications | Regular batch baseline  |


Benefits:

✅ Avoids long table names
✅ Clearly separates batch and streaming data
✅ Easier schema management in Delta Lake



Example Query Using Schema-Based Naming:

``` sql
SELECT * FROM streaming.publications;  -- Live table (Streaming updates)
SELECT * FROM historical.publications; -- Baseline table (Batch updates)
```


Option 2: Suffix-Based Naming

If using a single schema, suffix-based naming works well.

| Table Name           | Purpose                 |
|----------------------|-------------------------|
| publications_live     | Live streaming updates  |
| publications_baseline | Regular batch baseline  |


```sql
SELECT * FROM publications_live;  -- Live table
SELECT * FROM publications_baseline; -- Baseline table
```

---

4️⃣ Final Recommendation

| Approach            | Naming Example                          | Recommended Use                         |
|---------------------|-----------------------------------------|-----------------------------------------|
| Schema-Based Naming | streaming.publications, historical.publications | ✓ Best for clear separation            |
| Suffix-Based Naming | publications_live, publications_baseline | ✓ Works well in a single schema        |


✅ Best Practice: Use schema-based naming for better data organization!


---


## Best Practices for Schema Design in Delta Lake


Schema design in Delta Lake is critical for organizing tables, managing access control, and ensuring scalability. The decision between a schema per table or a schema for multiple tables depends on factors like data governance, logical grouping, and query performance.



1️⃣ Schema per Table vs. Schema per Group of Tables

| Approach                        | Description                                                       | Pros âœ…                                                            | Cons âŒ                                                    |
|----------------------------------|-------------------------------------------------------------------|---------------------------------------------------------------------|------------------------------------------------------------|
| **Schema per Table**            | Each table has its own schema (e.g., publications.publications, authors.authors). | âœ… Fine-grained access control <br> âœ… Clear separation of tables <br> âœ… Avoids schema conflicts | âŒ Too many schemas can create complexity <br> âŒ Harder to manage related datasets |
| **Schema per Group of Tables (Recommended)** | A schema groups related tables (e.g., publications.{articles, citations, metadata}). | âœ… Easier management of related tables <br> âœ… Simplifies access control <br> âœ… Logical grouping improves usability | âŒ Requires careful schema design to avoid excessive grouping |


---


2️⃣ Recommended Approach: Schema per Functional Group

A schema should group related tables to improve usability and data organization.

Example Schema Design in Delta Lake

| Schema Name    | Tables                                     | Purpose                              |
|-----------------|--------------------------------------------|--------------------------------------|
| **publications** | articles, citations, metadata              | Stores academic publication data    |
| **researchers**  | profiles, affiliations, collaborations      | Researcher profile and affiliations |
| **grants**       | funding_awards, funders, grant_recipients   | Grant funding data                  |
| **patents**      | patent_metadata, patent_citations           | Patent data                         |
| **lookup**       | funder_mapping, category_mapping, country_codes | Reference and mapping tables       |
| **intermediate** | staging_publications, staging_authors       | Temporary processing tables         |
| **streaming**    | publications_live, citations_live           | Streaming ingestion tables          |


---

3️⃣ Key Considerations for Schema Design

✔️ When to Use a Schema for a Single Table
- 	When the table contains highly sensitive data (e.g., security.audit_logs).
- 	If the table is exceptionally large and requires unique governance.
- 	When access control must be isolated per table.

✔️ When to Use a Schema for Multiple Tables (Preferred)
- 	When tables are logically related (e.g., publications.articles and publications.citations).
- 	When managing consistent access policies across related tables.
- 	When organizing tables by data domain (e.g., publications, grants, patents).


---

4️⃣ Best Practices

✅ Group tables logically (e.g., publications.articles, grants.funders).
✅ Use a separate schema for temporary or intermediate data (intermediate.staging_publications).
✅ Separate streaming data from batch tables (streaming.publications_live).
✅ Keep lookup/reference data in a dedicated schema (lookup.category_mapping).
✅ Ensure access control is managed at the schema level for security and governance.

Final Recommendation:

Use a schema per functional group (not per table) to simplify management, improve performance, and enforce governance.


---

###  Design Example

Since these tables are all related to researchers, the best practice is to use a single schema to group them logically while keeping each table independent.

Recommended Schema Design
- 	Schema: researchers
- 	Tables:
- 	researchers.profile → Stores core researcher details
- 	researchers.metrics → Contains research impact metrics
- 	researchers.topics → Tracks researcher topics and expertise
- 	researchers.summary → Stores summarized profiles

Why Use a Single Schema?

✅ Logical Grouping: All tables are related to researchers, so keeping them under one schema simplifies organization.
✅ Simplified Access Control: Manage permissions for the entire researcher dataset instead of separate schemas.
✅ Easier Querying: Queries involving multiple tables (e.g., joining profile with metrics) are easier within a single schema.
✅ Consistent Data Management: Schema evolution, versioning, and governance are easier to apply across related tables.

When to Use Multiple Schemas?
- 	If these tables belong to entirely different workflows (e.g., metrics is used by a separate analytics pipeline).
- 	If different teams manage different tables and require strict access control separation.
- 	If you plan to version one dataset independently (e.g., metrics_v2 but not profile_v2).

Final Recommendation

Use one schema (researchers) to keep all related tables together unless strict separation is required for governance or operational reasons.

---

## Trigger downstream updates in real time

Unlike Delta Live Tables (DLT), which has built-in real-time triggers for downstream pipelines, a normal Delta Table does not automatically trigger downstream updates in real time. However, you can achieve similar functionality using the following approaches:


---

### 1️⃣ Using Structured Streaming on Delta Table (Recommended for Near Real-Time Updates)

You can set up a Delta Table as a streaming source and process new updates in downstream pipelines using Structured Streaming.

Steps:
1.	Write data to the Delta Table normally (batch or append mode).
2.	Use Structured Streaming in the downstream pipeline to process changes.

Example (Downstream Consumer Using Streaming Read):

```python
from pyspark.sql.functions import *

# Read Delta table as a streaming source
df = (spark.readStream
    .format("delta")
    .option("ignoreChanges", "true")  # Ignores schema evolution issues
    .table("publications"))

# Perform any transformations (e.g., filtering)
df_transformed = df.filter("year >= 2024")

# Write to downstream system (e.g., another Delta table, Kafka, etc.)
(df_transformed.writeStream
    .format("delta")  # Write updates to another Delta table
    .option("checkpointLocation", "/mnt/checkpoints/publications_updates")
    .trigger(availableNow=True)  # Processes all new data since the last checkpoint
    .start("/mnt/delta/publications_updates"))
```

✅ Best for: Near real-time processing where downstream tables need continuous updates.


---

### 2️⃣ Using Auto Loader for Change Detection (Trigger Based on New Data)

If your upstream writes new files to a Delta Lake location, you can use Databricks Auto Loader to monitor the directory and trigger the downstream pipeline.

Example:

```python
df = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "delta") 
    .load("dbfs:/mnt/delta/publications"))

# Process and write updates
df.writeStream \
    .format("delta") \
    .option("checkpointLocation", "/mnt/checkpoints/publications_autoload") \
    .start("/mnt/delta/publications_processed")
```


---


### 3️⃣ Using Databricks Jobs with Triggering Logic (Periodic Execution)

If real-time updates are not mandatory, you can schedule Databricks jobs to check for new data in Delta tables and trigger downstream pipelines at fixed intervals.

Example Job Trigger Logic (Detecting New Data)

```python
df = spark.read.table("publications")
max_timestamp = df.agg({"_commit_timestamp": "max"}).collect()[0][0]

# If new data is found, trigger downstream processing
if max_timestamp > last_processed_timestamp:
    trigger_downstream_pipeline()
```
✅ Best for: Batch processing when real-time is not necessary.


---

### 4️⃣ Using Databricks Delta Change Data Feed (CDC)

If Change Data Capture (CDC) is needed, you can enable Change Data Feed (CDF) to track row-level changes in the Delta Table.

Enable Change Data Feed on a Delta Table

```sql
ALTER TABLE publications SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
```


Downstream Process Reads Only New Changes

```python
df = (spark.readStream
    .format("delta")
    .option("readChangeData", "true")
    .option("startingVersion", 0)  # Track changes since the first version
    .table("publications"))

df.writeStream.format("delta").start("/mnt/delta/publications_cdc")
```

✅ Best for: Capturing row-level inserts, updates, and deletes in real-time.


---

### Which Approach Should You Use?

| Use Case                        | Best Approach          |
|----------------------------------|------------------------|
| **Real-time downstream updates** | Structured Streaming   |
| **Triggering based on new data** | Auto Loader            |
| **Scheduled pipeline runs**      | Databricks Jobs        |
| **Row-level change tracking**    | Change Data Feed       |




If real-time is critical, Structured Streaming on Delta Tables is the best option. Otherwise, Change Data Feed or periodic batch jobs can also be effective.

---

### Normal Delta Table (CDC with MERGE operation)  vs.  Live Stream Table

In Databricks, the difference between Change Data Capture (CDC) in normal Delta tables and live stream tables (Delta Live Tables - DLT) is mostly about how changes are detected, processed, and made available for consumption.

✅ 1. Normal Delta Table (CDC with MERGE operation)

- 	Mechanism: CDC is implemented using techniques like MERGE, INSERT, UPDATE, and DELETE operations.
- 	Data Processing: Batch processing approach. Changes are applied periodically through explicit jobs or triggers.
- 	Data Consumption: Consumers can read the latest snapshot of the table or use Delta Time Travel to access historical data.
- 	Latency: Higher latency, as changes are applied only when triggered by batch jobs.
- 	Use Cases: Data warehouses, periodic ETL jobs, where near real-time processing is not required.

Example:
  
```sql
MERGE INTO target_table t
USING source_table s
ON t.id = s.id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *
```

---

✅ 2. Live Stream Table (Delta Live Tables - DLT with CDC)
- 	Mechanism: Built-in CDC support via APPLY CHANGES INTO and STREAMING functionalities.
- 	Data Processing: Continuous streaming processing. DLT automatically processes and updates tables with minimal latency.
- 	Data Consumption: Consumers can query tables in near real-time, as DLT continuously ingests and processes data.
- 	Latency: Low latency, designed for near real-time processing.
- 	Use Cases: Streaming analytics, event-driven architectures, real-time dashboards, and operational analytics.

Example:

```sql
CREATE LIVE TABLE target_table
TBLPROPERTIES ("quality" = "gold")
AS
APPLY CHANGES INTO target_table
FROM STREAM(source_table)
KEYS (id)
SEQUENCE BY timestamp
```


---
🔑 Differences

| Aspect               | Normal Delta Table (CDC with MERGE)    | Live Stream Table (DLT)               |
|----------------------|---------------------------------------|---------------------------------------|
| **Processing Mode**   | Batch processing                      | Continuous streaming processing      |
| **Latency**           | High (batch intervals)                | Low (near real-time)                 |
| **CDC Support**       | Manual with MERGE                     | Built-in with APPLY CHANGES INTO      |
| **Consumption Pattern**| Snapshot-based reads                  | Streaming-based reads                |
| **Complexity**        | Higher (manual management required)   | Lower (automatic handling via DLT)   |
| **Use Cases**         | Periodic ETL, Data Warehousing        | Real-time analytics, Event-driven architectures |



🚀 When to Use Which:
- 	Use Normal Delta Table CDC if you’re working with batch workloads, like weekly or daily processing.
- 	Use Live Stream Table (DLT) if you need low-latency data processing and want to build real-time data applications.