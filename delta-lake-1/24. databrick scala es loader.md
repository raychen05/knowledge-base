
### Scala Databricks Delta → Elasticsearch Loader (Production Template)


This template provides a production-grade Scala/Spark pipeline for Databricks that reads from Delta Lake (Unity Catalog), performs validation & transformations, and bulk loads to Elasticsearch with idempotency, retries, DLQ, metrics, and config-driven execution.


I dropped a full production-ready Scala Databricks → Delta → Elasticsearch loader template into the canvas with:
    - SBT project, config, and Scala source (validation, idempotent IDs, retries, DLQ).
    - Unit test scaffolding (ScalaTest).
    - Databricks Job JSON, widgets, and a runbook.
    - ES index template + ILM examples.

Want me to tailor it to your exact Unity Catalog table names, ES version, and batch vs streaming mode?


#### 1.  Repository Layout

```text
.
├── README.md
├── build.sbt
├── project
│   └── plugins.sbt
├── conf
│   └── application.conf
├── src
│   ├── main
│   │   └── scala
│   │       └── com
│   │           └── acme
│   │               └── pipeline
│   │                   ├── EsLoader.scala
│   │                   ├── config
│   │                   │   └── Settings.scala
│   │                   ├── util
│   │                   │   └── Retry.scala
│   │                   └── validation
│   │                       └── Validators.scala
│   └── test
│       └── scala
│           └── com
│               └── acme
│                   └── pipeline
│                       ├── EsLoaderSpec.scala
│                       └── ValidatorsSpec.scala
└── databricks
    ├── job.json
    └── widgets.md
```

---


#### 2. build.sbt


```scala

ThisBuild / scalaVersion := "2.12.17"
ThisBuild / version      := "0.1.0"
ThisBuild / organization := "com.acme"

lazy val sparkVersion = "3.5.0"

libraryDependencies ++= Seq(
  // Spark (provided on Databricks clusters)
  "org.apache.spark" %% "spark-sql" % sparkVersion % "provided",
  "org.apache.spark" %% "spark-core" % sparkVersion % "provided",

  // ES Hadoop connector (for DataFrame <-> ES)
  "org.elasticsearch" % "elasticsearch-hadoop" % "8.12.2",

  // Config
  "com.typesafe" % "config" % "1.4.3",

  // Logging
  "org.slf4j" % "slf4j-api" % "2.0.13",
  "ch.qos.logback" % "logback-classic" % "1.4.14",

  // Testing
  "org.scalatest" %% "scalatest" % "3.2.18" % Test
)

Compile / compile / scalacOptions ++= Seq(
  "-deprecation", "-feature", "-Xlint", "-Ywarn-dead-code", "-Ywarn-value-discard"
)

Test / parallelExecution := false
Test / fork := true

```

---

#### 3. conf/application.conf

```scala
app {
  name = "delta-to-es-loader"

  // Unity Catalog fully-qualified tables
  source_table = "catalog.schema.source_table"
  dlq_table    = "catalog.schema.dlq_table"

  // Transform options
  // columns to hash for deterministic _id
  id_columns = ["source_id", "event_date"]

  // Validation rules
  required_columns = ["source_id", "event_date", "metric_value"]
  non_negative_columns = ["metric_value"]

  // Elasticsearch
  es {
    nodes = "https://es-host:9200"     // or comma-separated
    api_key_id     = ${?ES_API_KEY_ID}
    api_key_secret = ${?ES_API_KEY_SECRET}
    index_alias    = "myindex"
    index_write    = "myindex-write"    // alias pointing to current write index
    bulk {
      bytes   = 5242880                 // 5 MB
      entries = 5000
      retries = 5
      backoff_initial_ms = 1000
      backoff_max_ms     = 60000
    }
    refresh_interval = "1s"
  }

  // Runtime
  mode = "batch" // batch | streaming
  checkpoint_path = "/mnt/checkpoints/delta-to-es"
  watermark = "5 minutes"
}

```

---

#### 4.  Settings.scala


```scala
package com.acme.pipeline.config

import com.typesafe.config.{Config, ConfigFactory}

final case class ESBulk(bytes: Int, entries: Int, retries: Int, backoffInitialMs: Int, backoffMaxMs: Int)
final case class ES(nodes: String, apiKeyId: String, apiKeySecret: String, indexAlias: String, indexWrite: String, bulk: ESBulk, refreshInterval: String)
final case class AppConf(
  name: String,
  sourceTable: String,
  dlqTable: String,
  idColumns: Seq[String],
  requiredColumns: Seq[String],
  nonNegativeColumns: Seq[String],
  es: ES,
  mode: String,
  checkpointPath: String,
  watermark: String
)

object Settings {
  def load(): AppConf = {
    val c: Config = ConfigFactory.load()
    val ec = c.getConfig("app.es")
    val bulk = ESBulk(
      bytes = ec.getConfig("bulk").getInt("bytes"),
      entries = ec.getConfig("bulk").getInt("entries"),
      retries = ec.getConfig("bulk").getInt("retries"),
      backoffInitialMs = ec.getConfig("bulk").getInt("backoff_initial_ms"),
      backoffMaxMs = ec.getConfig("bulk").getInt("backoff_max_ms")
    )
    val es = ES(
      nodes = ec.getString("nodes"),
      apiKeyId = sys.env.getOrElse("ES_API_KEY_ID", ec.getString("api_key_id")),
      apiKeySecret = sys.env.getOrElse("ES_API_KEY_SECRET", ec.getString("api_key_secret")),
      indexAlias = ec.getString("index_alias"),
      indexWrite = ec.getString("index_write"),
      bulk = bulk,
      refreshInterval = ec.getString("refresh_interval")
    )
    AppConf(
      name = c.getString("app.name"),
      sourceTable = c.getString("app.source_table"),
      dlqTable = c.getString("app.dlq_table"),
      idColumns = c.getStringList("app.id_columns").toArray.toSeq.map(_.toString),
      requiredColumns = c.getStringList("app.required_columns").toArray.toSeq.map(_.toString),
      nonNegativeColumns = c.getStringList("app.non_negative_columns").toArray.toSeq.map(_.toString),
      es = es,
      mode = c.getString("app.mode"),
      checkpointPath = c.getString("app.checkpoint_path"),
      watermark = c.getString("app.watermark")
    )
  }
}
```

---

### 5. Retry.scala

```scala
package com.acme.pipeline.util

import scala.concurrent.duration._
import scala.util.control.NonFatal

object Retry {
  def withBackoff[T](maxAttempts: Int, initial: FiniteDuration, maxBackoff: FiniteDuration)(f: => T): T = {
    var attempt = 0
    var delay = initial
    var lastErr: Throwable = null
    while (attempt < maxAttempts) {
      try {
        return f
      } catch {
        case NonFatal(e) =>
          lastErr = e
          attempt += 1
          if (attempt >= maxAttempts) throw lastErr
          Thread.sleep(delay.toMillis)
          delay = (delay * 2) min maxBackoff
      }
    }
    throw lastErr
  }
}

```

---
#### 6. Validators.scala

```scala
import org.apache.spark.sql.{DataFrame}
import org.apache.spark.sql.functions._

object Validators {
  def requireColumns(df: DataFrame, cols: Seq[String]): DataFrame = {
    val missing = cols.filterNot(df.columns.contains)
    require(missing.isEmpty, s"Missing required columns: ${missing.mkString(",")}")
    df
  }

  def nonNegative(df: DataFrame, cols: Seq[String]): (DataFrame, DataFrame) = {
    val bad = cols.map(c => col(c) < 0).reduce(_ or _)
    val badDf = df.filter(bad)
    val goodDf = df.except(badDf)
    (goodDf, badDf.withColumn("dq_reason", lit("negative_values")))
  }

  def notNull(df: DataFrame, cols: Seq[String]): (DataFrame, DataFrame) = {
    val bad = cols.map(c => col(c).isNull).reduce(_ or _)
    val badDf = df.filter(bad)
    val goodDf = df.except(badDf)
    (goodDf, badDf.withColumn("dq_reason", lit("null_in_required")))
  }
}
```

---
#### 7. EsLoader.scala


```scala
package com.acme.pipeline

import com.acme.pipeline.config.Settings
import com.acme.pipeline.util.Retry
import com.acme.pipeline.validation.Validators
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._

object EsLoader {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("DeltaToES").getOrCreate()
    val conf = Settings.load()
    import spark.implicits._

    spark.sparkContext.setLogLevel("INFO")

    // Read from Unity Catalog Delta table
    val raw = spark.read.table(conf.sourceTable)

    // Validation
    Validators.requireColumns(raw, conf.requiredColumns)
    val (nnGood, nnBad) = Validators.notNull(raw, conf.requiredColumns)
    val (good, negBad)  = Validators.nonNegative(nnGood, conf.nonNegativeColumns)

    val dlq = nnBad.unionByName(negBad, allowMissingColumns = true).withColumn("dlq_ts", current_timestamp())
    if (dlq.count() > 0) {
      dlq.write.mode("append").format("delta").saveAsTable(conf.dlqTable)
    }

    // Deterministic _id
    val idExpr = concat_ws("|", conf.idColumns.map(col): _*)
    val withId = good.withColumn("_id", sha1(idExpr))

    // Minimal transformation example
    val transformed = withId
      .withColumn("ingest_ts", current_timestamp())
      .withColumn("event_date", to_date(col("event_date")))

    // ES write options
    val esOpts = Map(
      "es.nodes" -> conf.es.nodes,
      "es.nodes.wan.only" -> "true",
      "es.net.http.auth.api_key_id" -> conf.es.apiKeyId,
      "es.net.http.auth.api_key_secret" -> conf.es.apiKeySecret,
      "es.resource" -> conf.es.indexWrite,
      "es.write.operation" -> "index",
      "es.mapping.id" -> "_id",
      "es.batch.size.bytes" -> conf.es.bulk.bytes.toString,
      "es.batch.size.entries" -> conf.es.bulk.entries.toString,
      "es.batch.write.retry.count" -> conf.es.bulk.retries.toString,
      "es.batch.write.retry.wait" -> s"${conf.es.bulk.backoffInitialMs}ms",
      "es.index.refresh_interval" -> conf.es.refreshInterval
    )

    // Write with retry wrapper (at batch granularity)
    Retry.withBackoff(
      maxAttempts = conf.es.bulk.retries,
      initial = scala.concurrent.duration.Duration(conf.es.bulk.backoffInitialMs, "millis"),
      maxBackoff = scala.concurrent.duration.Duration(conf.es.bulk.backoffMaxMs, "millis")
    ) {
      transformed
        .coalesce(math.max(1, spark.sparkContext.defaultParallelism / 2))
        .write
        .format("org.elasticsearch.spark.sql")
        .options(esOpts)
        .mode("append")
        .save()
    }

    spark.stop()
  }
}

```

---

#### 8. ValidatorsSpec.scala


```scala
package com.acme.pipeline

import org.scalatest.funsuite.AnyFunSuite
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import com.acme.pipeline.validation.Validators

class ValidatorsSpec extends AnyFunSuite {
  private val spark = SparkSession.builder().master("local[2]").appName("test").getOrCreate()
  import spark.implicits._

  test("notNull splits rows correctly") {
    val df = Seq((1, "2024-01-01"), (null.asInstanceOf[Integer], "2024-01-02")).toDF("source_id", "event_date")
    val (good, bad) = Validators.notNull(df, Seq("source_id", "event_date"))
    assert(good.count() == 1)
    assert(bad.count() == 1)
  }
}

```


---


