
## A. Comprehensive Use Cases for Schema Changes in Delta Lake


Schema changes in Delta Lake can occur due to business needs, data structure evolution, or performance optimizations. Below is a detailed breakdown of all possible schema change scenarios and the best strategies to handle them.



### üîπ 1. Adding New Columns (Minor Change)

Use Case:
- 	A new attribute (e.g., funding_source) needs to be added to an existing table.
- 	Existing queries and data pipelines should remain functional.

Solution: Schema Evolution (mergeSchema=True)
üîπ Automatically updates schema when new data arrives.

Databricks Python (PySpark)
```python
df.write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save("dbfs:/mnt/datalake/publications")
```

- ‚úÖBest for: Minor updates, appending new fields.
- ‚ö†Ô∏è Caution: Cannot rename or remove columns.



### üîπ 2. Renaming Columns (Breaking Change)

Use Case:
- 	pub_year needs to be renamed to publication_year.
- 	Existing queries and data pipelines will break if using the old name.

Solution: Create a New Versioned Schema (publications_v2)


```sql
CREATE OR REPLACE TABLE my_catalog.publications_v2 AS
SELECT id, title, pub_year AS publication_year, funding_source
FROM my_catalog.publications;
```

- ‚úÖBest for: Preventing query failures while allowing transition.
- ‚ö†Ô∏è Caution: Requires updating queries and ETL pipelines.


### üîπ 3. Removing Columns (Breaking Change)

Use Case:
- 	author_email field is no longer needed.
- 	Existing queries referencing it will fail.

Solution: Create a New Schema & Migrate Data (publications_v2)

```sql
CREATE OR REPLACE TABLE my_catalog.publications_v2 AS
SELECT id, title, publication_year, funding_source
FROM my_catalog.publications;
```

- ‚úÖBest for: Safely dropping columns without affecting existing queries.
- ‚ö†Ô∏è Caution: Requires modifying all dependent queries.


### üîπ 4. Changing Data Type (Breaking Change)

Use Case:
- 	publication_year is stored as STRING but needs to be INT.
- 	Queries performing calculations on it will break if type is incorrect.

Solution: Create a New Schema & Transform Data (publications_v2)

```sql
CREATE OR REPLACE TABLE my_catalog.publications_v2 AS
SELECT id, title, CAST(publication_year AS INT) AS publication_year, funding_source
FROM my_catalog.publications;
```

- ‚úÖBest for: Preventing data type conflicts and maintaining correctness.
- ‚ö†Ô∏è Caution: Must verify data transformation logic.


### üîπ 5. Splitting a Large Table into Multiple Tables (Structural Change)

Use Case:
- 	publications contains mixed data (articles, journals, conference_papers).
- 	Queries frequently filter these categories, causing performance bottlenecks.

Solution: Split into Dedicated Tables

```sql
CREATE TABLE my_catalog.articles AS 
SELECT * FROM my_catalog.publications WHERE publication_type = 'Article';

CREATE TABLE my_catalog.journals AS 
SELECT * FROM my_catalog.publications WHERE publication_type = 'Journal';
```

###  üîπ 6. Merging Multiple Tables into One (Schema Unification)

Use Case:
- 	wos_publications, bci_publications, and ddidw_publications need to be combined into one table for easier querying.

Solution: Create a Unified Table & Migrate Data

```sql
CREATE OR REPLACE TABLE my_catalog.publications_combined AS
SELECT * FROM my_catalog.wos_publications
UNION ALL
SELECT * FROM my_catalog.bci_publications
UNION ALL
SELECT * FROM my_catalog.ddidw_publications;
```


### üîπ 7. Changing Primary Key or Partition Strategy

Use Case:
- 	A table is currently partitioned by year, but year, country would provide better performance.

Solution: Create a New Table with the New Partition Key

```sql
CREATE TABLE my_catalog.publications_v2
PARTITIONED BY (publication_year, country) AS 
SELECT * FROM my_catalog.publications;
```

### üîπ 8. Handling Schema Drift (Unstructured Data Sources)

Use Case:
- 	Incoming data has inconsistent fields (e.g., JSON logs from multiple sources).
- 	Schema is not fixed and can change dynamically.

Solution: Enable mergeSchema=True & Use Auto-Generated Columns

```python
df.write.format("delta") \
    .option("mergeSchema", "true") \
    .save("dbfs:/mnt/datalake/publications")
```

üîπ 9. Rolling Back Schema Changes (Time Travel)

Use Case:
- 	A recent schema update introduced an error.
- 	Need to restore the table to a previous version.

Solution: Use Delta Lake Time Travel

```sql
SELECT * FROM my_catalog.publications VERSION AS OF 5;
```


```sql
SELECT * FROM my_catalog.publications TIMESTAMP AS OF '2024-03-25T12:00:00Z';
```

### üîπ 10. Deleting an Entire Schema (Cleanup Process)

Use Case:
- 	An old schema (publications_v1) is no longer used and should be fully deleted.

Solution: Drop Schema in Databricks

```sql
DROP SCHEMA my_catalog.publications_v1 CASCADE;
```

---

###  Final Recommendation



| Change Type            | Strategy            | Approach                         |
|------------------------|--------------------|----------------------------------|
| Adding New Columns     | Schema Evolution   | mergeSchema=True                |
| Renaming Columns      | Versioned Schema   | publications_v2                 |
| Removing Columns      | Versioned Schema   | publications_v2                 |
| Changing Data Type    | Versioned Schema   | publications_v2                 |
| Splitting Tables      | Structural Change  | articles, journals              |
| Merging Tables        | Schema Unification | publications_combined           |
| Changing Partition Key| Optimized Schema   | publications_v2                 |
| Schema Drift         | Dynamic Schema     | Auto-merge (mergeSchema=True)   |
| Rolling Back         | Time Travel        | VERSION AS OF                   |
| Deleting Schema      | Cleanup            | DROP SCHEMA CASCADE             |


---

### Databricks Notebook: Schema Change Strategies in Delta Lake

```python
# Databricks Notebook: Schema Change Strategies in Delta Lake

# COMMAND ----------
# Import required libraries
from delta.tables import DeltaTable
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("SchemaChangeExamples").getOrCreate()

# COMMAND ----------
# 1Ô∏è‚É£ Adding New Columns (Schema Evolution)
data = [(1, "Article 1"), (2, "Article 2")]
df = spark.createDataFrame(data, ["id", "title"])
df.write.format("delta").mode("overwrite").save("/mnt/delta/publications")

new_data = [(3, "Article 3", "New Funding Source")]
df_new = spark.createDataFrame(new_data, ["id", "title", "funding_source"])
df_new.write.format("delta").mode("append").option("mergeSchema", "true").save("/mnt/delta/publications")

# COMMAND ----------
# 2Ô∏è‚É£ Renaming Columns (Versioned Schema)
spark.sql("""
CREATE OR REPLACE TABLE my_catalog.publications_v2 AS
SELECT id, title, pub_year AS publication_year, funding_source
FROM my_catalog.publications;
""")

# COMMAND ----------
# 3Ô∏è‚É£ Removing Columns
spark.sql("""
CREATE OR REPLACE TABLE my_catalog.publications_v2 AS
SELECT id, title, publication_year, funding_source
FROM my_catalog.publications;
""")

# COMMAND ----------
# 4Ô∏è‚É£ Changing Data Type
spark.sql("""
CREATE OR REPLACE TABLE my_catalog.publications_v2 AS
SELECT id, title, CAST(publication_year AS INT) AS publication_year, funding_source
FROM my_catalog.publications;
""")

# COMMAND ----------
# 5Ô∏è‚É£ Splitting Table by Category
spark.sql("""
CREATE TABLE my_catalog.articles AS 
SELECT * FROM my_catalog.publications WHERE publication_type = 'Article';
""")

# COMMAND ----------
# 6Ô∏è‚É£ Merging Tables
spark.sql("""
CREATE OR REPLACE TABLE my_catalog.publications_combined AS
SELECT * FROM my_catalog.wos_publications
UNION ALL
SELECT * FROM my_catalog.bci_publications;
""")

# COMMAND ----------
# 7Ô∏è‚É£ Changing Partition Strategy
spark.sql("""
CREATE TABLE my_catalog.publications_v2
PARTITIONED BY (publication_year, country) AS 
SELECT * FROM my_catalog.publications;
""")

# COMMAND ----------
# 8Ô∏è‚É£ Handling Schema Drift
df.write.format("delta").option("mergeSchema", "true").save("dbfs:/mnt/datalake/publications")

# COMMAND ----------
# 9Ô∏è‚É£ Rolling Back Changes (Time Travel)
df_rollback = spark.read.format("delta").option("versionAsOf", 5).load("/mnt/delta/publications")
df_rollback.show()

# COMMAND ----------
# üîü Deleting Schema
spark.sql("DROP SCHEMA my_catalog.publications_v1 CASCADE;")


```

----

## B. Best Practices for Versioning in Delta Lake

1.	Table-Level Versioning: Generally preferred for isolated entities or independent datasets. Allows flexible and isolated updates without affecting other data.
2.	Schema-Level Versioning: Useful when tables share a similar structure and you want to enforce schema consistency across multiple tables.
3.	Combined Approach: Use both methods when your data pipelines are complex and you need flexibility in managing independent tables and cohesive schema-wide changes.


Example of Versioning Implementation:

- 	Table-level versioning:
```python
# Using time travel on a specific table
df_v1 = spark.read.format("delta").option("versionAsOf", 1).table("publications.wos_publications")
```

- 	Schema-level versioning:
```python
    # Alter schema version
spark.sql("ALTER SCHEMA publications SET VERSION 'v2'")
```

Conclusion:

- 	Use table-level versioning when tables have different data structures or lifecycle requirements.
- 	Use schema-level versioning when multiple tables share the same schema or should evolve together.
- 	Use both methods if your data structure requires flexibility in handling independent updates to tables while maintaining schema-level consistency.


---

## C. Naming Conventions for Live Tables (Streaming) and Regular Delta Tables (Baseline) in Delta Lake

When managing both streaming (live) updates and regular (baseline) batch updates in Delta Lake, using a clear and consistent naming convention is crucial for maintainability and clarity. Below are best practices for naming live tables and baseline tables:

---

### 1Ô∏è‚É£ Naming Live (Streaming) Tables

Live tables process continuous updates (streaming ingestion). These tables are typically used for near real-time data processing.

Recommended Naming Conventions:
- 	Use a suffix like _live, _stream, or _incremental
- 	Use a separate schema for live streaming tables
- 	Keep the name consistent with its corresponding baseline table


| Table Type                    | Suggested Naming Pattern | Example               |
| ----------------------------- | ------------------------ | --------------------- |
| Streaming (Live) Table         | `{base_table}_live`      | `publications_live`   |
| Streaming Table (Alternative)  | `{base_table}_stream`    | `publications_stream` |
| Streaming Table with Schema    | `streaming.{base_table}` | `streaming.publications` |


Example Usage:

```sql
CREATE TABLE streaming.publications 
AS SELECT * FROM delta.`/mnt/data/publications` WHERE 1=0;
```

```python
df.writeStream \
  .format("delta") \
  .option("checkpointLocation", "/mnt/checkpoints/publications") \
  .outputMode("append") \
  .table("streaming.publications")
```

---

### 2Ô∏è‚É£ Naming Regular (Baseline) Delta Tables

Baseline tables store batch-processed historical data, usually loaded periodically.

Recommended Naming Conventions:
- 	Use a clear name without a suffix (default for batch tables)
- 	If multiple baseline versions exist, use _v1, _v2, etc.
- 	Consider using a separate schema for batch data (historical, batch, etc.)


| Table Type                     | Suggested Naming Pattern | Example               |
| ------------------------------ | ------------------------ | --------------------- |
| Regular Delta Table            | `{base_table}`           | `publications`        |
| Versioned Delta Table          | `{base_table}_v1`        | `publications_v1`     |
| Baseline Table with Schema     | `historical.{base_table}`| `historical.publications` |


Example Usage:

```sql
CREATE TABLE historical.publications 
AS SELECT * FROM delta.`/mnt/data/publications` WHERE 1=0;
```

```python
df.write.format("delta").mode("overwrite").saveAsTable("historical.publications")
```

---

### 3Ô∏è‚É£ Unified Approach: Schema-Based vs. Suffix-Based Naming

Option 1: Schema-Based Naming (Recommended)

Using separate schemas for streaming vs. batch improves clarity and data governance.

| Schema     | Table Name    | Purpose               |
| ---------- | ------------- | --------------------- |
| streaming  | publications  | Live streaming updates|
| historical | publications  | Regular batch baseline|


Benefits:

- ‚úÖ Avoids long table names
- ‚úÖ Clearly separates batch and streaming data
- ‚úÖ Easier schema management in Delta Lake

Example Query Using Schema-Based Naming:

```sql
SELECT * FROM streaming.publications;  -- Live table (Streaming updates)
SELECT * FROM historical.publications; -- Baseline table (Batch updates)
```

Option 2: Suffix-Based Naming

If using a single schema, suffix-based naming works well.

| Table Name         | Purpose               |
| ------------------ | --------------------- |
| publications_live  | Live streaming updates|
| publications_baseline | Regular batch baseline|



Example Query Using Suffix-Based Naming:


```sql
SELECT * FROM publications_live;  -- Live table
SELECT * FROM publications_baseline; -- Baseline table
```

---

4Ô∏è‚É£ Final Recommendation

| Approach           | Naming Example                      | Recommended Use                |
| ------------------ | ------------------------------------ | ----------------------------- |
| Schema-Based Naming| streaming.publications, historical.publications | ‚úî Best for clear separation   |
| Suffix-Based Naming| publications_live, publications_baseline | ‚úî Works well in a single schema|

‚úÖ Best Practice: Use schema-based naming for better data organization!

---

## D. Stream and Baseline Tables

When deciding whether to use both a streaming table and a baseline table, or just one table to store both stream and baseline updates, it‚Äôs important to consider the use case, data consistency requirements, and the type of updates being made.


### When to use both stream and baseline tables

1. Separation of Concerns:

- Stream Table: Used for capturing real-time or incremental updates. It handles continuous data updates and is useful for real-time analytics or applications that need up-to-date information.

- Baseline Table: Stores the stable, historical data. This table typically captures batch updates and is used for long-term storage of complete data, which remains consistent over time unless explicitly updated.

Having both tables allows you to keep real-time updates separate from historical, stable data. This is useful if you need to perform time-based queries, track historical trends, or roll back to a previous state if something goes wrong in real-time updates.

2. Data Consistency and Integrity:

With separate stream and baseline tables, you can keep historical data (the baseline) intact while adding real-time updates (the stream) in parallel. This helps to ensure the integrity of historical data while still being able to process incremental changes.

3. Performance Considerations:

Using both tables allows you to optimize performance. The streaming table handles fast, incremental updates, while the baseline table is optimized for slower, batch-based processes that manage larger sets of data.

---

### When to use just one streaming table for both stream and baseline updates:

- Simplicity: If your use case doesn‚Äôt require tracking historical states separately, and you can handle both live updates and historical updates in a single stream, combining both might simplify your architecture.

- Use Case: For scenarios where the distinction between streaming and baseline data isn‚Äôt critical (e.g., if you're only interested in the most current state of data), you may choose a single streaming table to keep everything in one place.

---

### Best Practice:

Best Practice: Generally, it's better to separate streaming and baseline tables, especially in environments where:

- You need to differentiate between real-time updates and stable data.
- You want to avoid the risk of corrupting historical data with live updates.
- You need to retain a clear versioning of data (e.g., using versioned tables for incremental updates).

This approach improves data integrity, scalability, and performance in most real-time or large-scale data applications.


---

### Conclusion:

- Use both stream and baseline tables when data consistency, historical tracking, and clear separation of real-time and stable data are important.

- Use one stream table only if simplicity is preferred and you don‚Äôt need to manage separate histories or long-term stability of the data.

The best practice tends to be separating the stream and baseline tables for better management of data and performance, especially in production environments.


---

## E. Synchronization Stream table vs. Baseline table

When separating the stream and baseline tables, the baseline data can still be updated, but it is generally done in a controlled, batch-based process rather than continuously like the stream table. Here‚Äôs a breakdown of how this works:

---

### 1. Baseline Table Updates:
   
- The baseline table typically stores historical, stable data. However, it can be updated periodically through batch jobs or scheduled updates.
- For example, the baseline table can be refreshed at regular intervals (e.g., daily or weekly) by processing the data from the stream table and applying the changes to the baseline.
- In some use cases, you may have an append-only baseline, meaning it will only grow over time without overwriting existing data, but you can also perform updates or overwrites to keep the baseline consistent with the current state.

---
### 2. Stream Table Updates:

- The stream table stores incremental updates or real-time data, meaning it contains only the most recent data or changes that have occurred.
- It is typically append-only and stores new records or changes to the data, such as new entries, updates, or deletions (depending on the system‚Äôs design).
- Stream tables often use time-based windowing or change data capture (CDC) to process the latest changes.

---
### 3. How to Get New Baseline Data:

To get the new baseline data to process from two tables, you can follow these approaches:

- Option 1: Periodic Batch Updates from Stream to Baseline
 - In this approach, you periodically merge the incremental changes from the stream table into the baseline table.  This can be done in a scheduled process.
 - For instance:
    - Extract the latest records from the stream table (e.g., using a timestamp or an ID field to get all records since the last update).
    - Apply any necessary transformations or updates to these records.
    - Insert them into the baseline table, either as new records or by updating existing records if necessary (e.g., updating the baseline if the stream data represents changes to existing entities).

This ensures that the baseline table stays current with the data being streamed while retaining the ability to query historical data.


- Option 2: Snapshot-Based Approach
  - Another approach is to periodically take a snapshot of the stream table and replace or update the baseline table with the snapshot. This is useful if you want a "point-in-time" representation of the baseline data and don‚Äôt need real-time changes reflected in the baseline.
  - A snapshot approach may involve:
    - Storing the entire set of streaming data at a specific time.
    - Replacing or merging it with the baseline table in batch operations.


- Option 3: Delta Processing (if using Delta Lake or similar technologies)
    - If using Delta Lake, you can take advantage of merge operations to continuously update the baseline table with data from the stream table, allowing for incremental updates and data deduplication.
    - A typical MERGE operation could:
      - Update existing records in the baseline table with the new data from the stream.
      - Insert new records that do not yet exist in the baseline.

Example (in Spark with Delta Lake):
```scala
streamingDF
  .writeStream
  .format("delta")
  .foreachBatch { (batchDF, batchId) =>
    batchDF.createOrReplaceTempView("stream_table")
    
    spark.sql("""
      MERGE INTO baseline_table AS b
      USING stream_table AS s
      ON b.id = s.id
      WHEN MATCHED THEN
        UPDATE SET b.data = s.data
      WHEN NOT MATCHED THEN
        INSERT (id, data) VALUES (s.id, s.data)
    """)
  }

```

---

### 4. How to Process New Baseline Data:

Once the baseline table has been updated (either incrementally or by a full snapshot), you can process the data from both tables:

- For Real-Time Data (Stream Table): You would query the stream table for the most recent updates or new data. This data can be processed and acted upon immediately.

- For Historical or Stable Data (Baseline Table): You would query the baseline table to get a stable, consistent snapshot of the data at any point in time. This is especially useful for queries that need to reference complete, non-changing data over time.

---
### Conclusion:

- Baseline table: Can be periodically updated with batch processes or incremental updates from the stream table. It is not meant to be constantly updated in real-time like the stream table, but you can still perform updates to maintain consistency.

- Stream table: Stores incremental updates and is append-only. It is not suitable for long-term storage of stable data but works well for processing real-time changes.

- New baseline data: Can be generated by merging the data from the stream table into the baseline table either periodically (batch updates) or through continuous merge operations (if using Delta Lake or similar).

The best practice is to keep the baseline and stream data separate to avoid complications in real-time processing and historical tracking. Regular updates from the stream table to the baseline table ensure that the baseline data is kept fresh and accurate.


---

### E.  Stream Table Data Management

If you have both a stream table and a baseline table, and you're periodically updating the baseline table with data from the stream table, it is important to manage the data in the stream table to avoid it growing indefinitely. Stream tables often store incremental updates or real-time data, and without removing or archiving old data, the table size can grow uncontrollably, which can lead to performance issues.

---

### Why You Need to Remove Data from the Stream Table:

1. Prevent Unbounded Growth:

   - Stream tables are typically append-only, meaning new data is constantly added, but older data is not deleted. If you don‚Äôt remove processed data from the stream table, it will grow indefinitely, leading to high storage costs and performance degradation over time.

2. Data Expiry or Archiving:

   - After you've processed or merged the incremental data into the baseline table, the data in the stream table should either be deleted or archived, depending on your needs.

   - This ensures the stream table only contains unprocessed or new data that has not yet been moved to the baseline.

3. Performance Considerations:

   - Query performance: A constantly growing stream table can slow down queries, especially if the table is queried for unprocessed data or data that has already been ingested into the baseline.

   - Storage: As the data grows, it will consume more storage, which could become expensive or inefficient.

4. Data Management:

   - Regularly cleaning up or archiving data from the stream table ensures that the streaming pipeline remains efficient and scalable.

   - You can set a retention policy or use time-based partitions to delete processed data from the stream table after a certain period.
  
---

### How to Handle Removing Data from the Stream Table:

1. Using Time-Based Retention:

   - You can set a retention policy that removes data after a certain period (e.g., 1 day, 1 week).

   - For example, if you're using Apache Kafka for the stream table, you can configure a retention policy to automatically delete messages after they are processed.

2. Batch Cleanup after Data Processing:

    - After each batch job that updates the baseline table, you can run a cleanup operation to delete the data that has been successfully processed.

    - For example, in Apache Spark, after processing and updating the baseline table, you can use a DELETE or TRUNCATE operation to remove the processed records from the stream table.

3. Using Partitioning:

   - If you're using partitioned tables (e.g., in Delta Lake or similar systems), you can delete data based on partition columns like timestamp or batch_id to efficiently remove processed data in batches.

Example in Delta Lake:
```scala
val lastProcessedTime = // get the time of the last processed batch
val streamTable = "your_stream_table"
spark.sql(s"DELETE FROM $streamTable WHERE timestamp < '$lastProcessedTime'")
```

4. Marking Processed Data:

   - In some cases, you may not need to delete the data immediately. You can add a processing flag or status column to the stream table to mark records that have been processed and then archive or delete them later.

5. Archiving Data:

   - If you need to retain historical data from the stream table for auditing or other purposes, you could archive the data to a different storage location before deletion. This allows you to keep a historical record of all changes without burdening the stream table.

--- 
### Best Practices:

   - Clean up after processing: Always ensure that once data is processed and moved to the baseline table, it is removed from the stream table to avoid unbounded growth.

   - Partition and Retention Policy: Use partitioning and time-based retention to manage data size in the stream table effectively.

   - Automate Cleanup: Automate the cleanup process after each batch update to avoid manual intervention and ensure the system remains scalable and efficient.

---

### Conclusion:

To prevent the stream table from growing endlessly, you should regularly remove or archive the data that has been processed and moved to the baseline table. This is essential for maintaining performance, storage efficiency, and scalability in your data pipeline. If you're using streaming platforms like Kafka or Delta Lake, leverage their built-in retention policies or perform periodic cleanup and archiving based on processing intervals.


---

## F. Use Case -  incremental batch processing


Given the scenario you‚Äôve described, where the data update is incremental batch processing (one or several batches daily), and you‚Äôre dealing with a substantial amount of data (1-2 million records updated daily and 80 million total records), it is important to evaluate whether you need a separate streaming table or if it's better to update the baseline table directly.

---
### Key Considerations:

1. Data Volume & Frequency:

    - You are processing 1-2 million records daily, which is substantial but manageable within a batch-oriented processing system.

    - Since updates are incremental and not real-time, the need for a streaming table diminishes. Instead, you can focus on updating the baseline table directly with the new or modified records.

2. Real-Time vs Batch Processing:

   - A streaming table is typically designed for real-time data processing, where you need continuous ingestion and processing of new data. However, your updates are batch-based and can be handled in scheduled windows (e.g., several batches per day).

   - If the batch frequency is frequent enough (like multiple times a day) and there is no immediate need for real-time querying or processing of the latest data, you can directly update the baseline table without needing a separate streaming table.

3. Data Consistency and Integrity:

   - The baseline table will eventually hold the latest data, and you can incrementally apply the batch updates directly to it.

   - If you use the streaming table, you would need to periodically merge it into the baseline table. This introduces additional complexity, especially when managing large volumes of data.

   - By directly updating the baseline table, you simplify the data flow, reduce storage requirements, and avoid the overhead of managing a separate streaming table for incremental updates.

4. Processing Overhead and Complexity:

   - Using a streaming table adds complexity because you'd need a system that can handle real-time or near-real-time ingestion (like Apache Kafka, Structured Streaming, or similar). If the updates are not frequent or real-time, it‚Äôs often simpler and more efficient to perform batch-based updates directly on the baseline.

   - Direct updates to the baseline table will reduce the complexity of managing two separate tables (streaming and baseline). You can use incremental batch jobs (e.g., with Delta Lake or traditional batch processing) to efficiently update the baseline table.

5. Performance Considerations:

   - Batch updates can be done in a distributed manner (using Spark, for example) to efficiently update the baseline table.

   - If you directly update the baseline table, ensure that the batch processing job handles the new or updated records efficiently by using techniques such as upserts or MERGE operations (especially with Delta Lake or other transactional systems).

--- 

### Conclusion:

In your case, since the updates are incremental batches and the records are not extremely time-sensitive (real-time updates are not required), it is likely better to update the baseline table directly.

---

### Suggested Approach:

1. Update the Baseline Table Directly:

   - Use batch processing (e.g., Spark or other ETL tools) to apply the incremental updates directly to the baseline table.

   - You can use MERGE operations or upsert techniques to ensure that only the relevant records are inserted or updated in the baseline.

   - This approach will be simpler and more efficient than managing a separate stream table.

2. When to Consider a Streaming Table:

   - If you later find that real-time updates or processing are needed for certain use cases, you can consider using a streaming table.

   - However, for the current scenario, where updates are incremental and batched, there is no immediate need for a separate streaming table.

By focusing on updating the baseline table directly, you simplify the architecture, avoid unnecessary complexity, and maintain the performance and integrity of your data.


---

## G. Best Practices for Schema Design in Delta Lake


Schema design in Delta Lake is critical for organizing tables, managing access control, and ensuring scalability. The decision between a schema per table or a schema for multiple tables depends on factors like data governance, logical grouping, and query performance.

####  intermediate or temporal  data tables

For intermediate or temporal data in Delta Lake, it‚Äôs important to use schema names that clearly represent the transitional, processing, or temporary nature of the data. These schema names should help convey the purpose of the data and its role within the processing pipeline. Below is a list of candidate schema names for intermediate or temporal data

Best Practices Summary

- 	Clarity: The name should clearly communicate its purpose within the pipeline (e.g., staging, processing, snapshot).
- 	Consistency: Follow a consistent naming convention across your pipeline to avoid confusion (e.g., always use staging_ for raw data, transform_ for transformed data).
- 	Purpose-Driven: The name should reflect the role of the data, such as intermediate for transitional data or final for data that‚Äôs ready for production.

#### constant data tables

For storing constant data tables like exchange rates, category mappings, funder catalogs, NUTS mapping lists, publisher unification data, etc., the schema names should clearly indicate their role as reference data or static datasets. Here are some recommendations for best Delta Lake schema names


Best Practices for Naming

- 	Descriptive and Purpose-Driven: Choose schema names that immediately communicate the role of the data in the pipeline.
- 	Consistency: Stick to a consistent naming convention to avoid confusion.
- 	Simplicity: Avoid overly complex names, but ensure they are descriptive enough to convey the data‚Äôs role (e.g., reference_data, config_data).


Recommended Schema Names
- 	reference_data
- 	lookup_data
- 	mapping_data
- 	config_data
- 	master_data
- 	static_data
- 	catalog_data
- 	reference_lists
- 	auxiliary_data


---

#### Schema Update

When using schema evolution in Delta Lake for minor updates to your publications schema, the idea is to allow flexibility in handling changes that don‚Äôt drastically alter the structure of your data but still require schema modifications. Here are some common minor schema changes that can be handled with schema evolution:


Best Practices for Minor Schema Changes with Schema Evolution

- 	Use mergeSchema=True: When performing minor updates like adding new columns, always use mergeSchema=True to allow Delta Lake to automatically handle schema evolution.
- 	Maintain Backward Compatibility: Ensure that changes are backward-compatible (e.g., adding nullable fields or expanding enum values) to avoid breaking downstream applications or processes.
- 	Test Changes: Even for minor changes, it‚Äôs best to test schema changes in a non-production environment to ensure that the updates don‚Äôt break data pipelines or analysis.
- 	Version Control: Although schema evolution allows automatic updates, it‚Äôs still a good practice to track schema versions in your ETL pipeline or documentation for transparency and debugging purposes.


| Change Type            | Strategy            | Approach                         |
|------------------------|--------------------|----------------------------------|
| Adding New Columns     | Schema Evolution   | mergeSchema=True                |
| Renaming Columns      | Versioned Schema   | publications_v2                 |
| Removing Columns      | Versioned Schema   | publications_v2                 |
| Changing Data Type    | Versioned Schema   | publications_v2                 |
| Splitting Tables      | Structural Change  | articles, journals              |
| Merging Tables        | Schema Unification | publications_combined           |
| Changing Partition Key| Optimized Schema   | publications_v2                 |
| Schema Drift         | Dynamic Schema     | Auto-merge (mergeSchema=True)   |
| Rolling Back         | Time Travel        | VERSION AS OF                   |
| Deleting Schema      | Cleanup            | DROP SCHEMA CASCADE             |


---

