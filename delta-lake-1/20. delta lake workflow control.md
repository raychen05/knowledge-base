## Delta Lake Workflow Control


To coordinate two upstream Delta tables and ensure the downstream loader only runs after both are updated, you can implement a trigger mechanism using Delta Lakeâ€™s Change Data Feed (CDF) or metadata such as commit version or timestamp-based watermarking, combined with a Databricks Workflow orchestration.

Below is an implementation outline and example Scala Spark code using Delta Lake features and a control mechanism to coordinate the downstream job.


---

âœ… Goal

Downstream job should process data only after both upstream Delta tables have been updated for a specific batch (e.g., daily or by some batch ID).


---

ðŸ§© Option 1: Use a Metadata Control Table (Recommended)

You maintain a control table that tracks when each upstream table finishes loading for a specific batch. Downstream loader checks this table before proceeding.

1. Upstream Tables Update

Each upstream job writes to its own Delta table and also logs its status to a control table:

```scala
val batchId = "20240619"  // for example, a daily batch id

// after writing data to delta table
upstreamDf.write.format("delta").mode("overwrite").save("/delta/upstream1")

// mark completion in control table
val statusDf = Seq(("upstream1", batchId, current_timestamp()))
  .toDF("source", "batch_id", "completed_at")

statusDf.write
  .format("delta")
  .mode("append")
  .saveAsTable("pipeline_control_table")
```
Repeat similarly for the second upstream table (upstream2).


2. Downstream Loader Checkpoint

Downstream loader waits until both upstreams report completion for a given batch_id.

```scala
val batchId = "20240619"

val controlDf = spark.table("pipeline_control_table")
  .filter($"batch_id" === batchId)

val sourcesReady = controlDf.select("source").distinct().as[String].collect().toSet

if (Set("upstream1", "upstream2").subsetOf(sourcesReady)) {
  // Proceed to load and join both upstream tables
  val df1 = spark.read.format("delta").load("/delta/upstream1")
  val df2 = spark.read.format("delta").load("/delta/upstream2")

  val joined = df1.join(df2, Seq("id")) // or any key

  joined.write.format("delta").mode("overwrite").save("/delta/downstream_output")
} else {
  println(s"Waiting for all sources to be ready for batch $batchId.")
}
```


---


ðŸ§© Option 2: Use Delta Change Data Feed (CDF)

If youâ€™re tracking updates via Deltaâ€™s CDF, you can define a common watermark or batch timestamp, and downstream waits until it sees new data from both:

```scala
val batchTs = Timestamp.valueOf("2024-06-19 00:00:00")

val df1 = spark.read.format("delta")
  .option("readChangeData", "true")
  .option("startingTimestamp", batchTs.toString)
  .table("upstream1")

val df2 = spark.read.format("delta")
  .option("readChangeData", "true")
  .option("startingTimestamp", batchTs.toString)
  .table("upstream2")

if (!df1.isEmpty && !df2.isEmpty) {
  // Proceed
  val joined = df1.join(df2, Seq("id"))
  joined.write.format("delta").mode("overwrite").save("/delta/downstream_output")
}
```

But Option 1 (control table) gives more explicit and reliable coordination.


---

ðŸ§© Orchestration with Databricks Workflows

You can also orchestrate these jobs with Databricks Workflows:
1.	Create a job task for upstream1_writer
2.	Create a job task for upstream2_writer
3.	Set both as dependencies for downstream_loader

So downstream runs only after both upstreams succeed.


----
âœ… Summary


| **Approach**           | **Key Mechanism**                    | **Notes**                         |
|------------------------|--------------------------------------|-----------------------------------|
| Control Table          | Explicit batch completion tracking   | Most robust & recommended         |
| Delta CDF              | Timestamp-based change detection     | Requires aligned timestamps       |
| Databricks Workflow    | Orchestrated job dependencies        | Simplifies coordination logic     |




