## Implementing Delta Table Streaming with CDF: Checkpoint, Rollback, Restart, and Fetching Latest Version


To achieve reliable streaming reads using Change Data Feed (CDF) on a regular Delta Table, we need to:
1.	Enable Change Data Feed (CDF) on the Delta Table.
2.	Implement Checkpointing for fault-tolerance and recovery.
3.	Implement Rollback Mechanism to restart processing from a specific point.
4.	Restart Processing on Failure using checkpoints.
5.	Fetch Latest Version of Updates to ensure consistency.


---

1. Enable Change Data Feed (CDF) on Delta Table

To enable CDF on a Delta Table, set the table property delta.enableChangeDataFeed to true.
```scala
spark.sql("""
  ALTER TABLE your_database.your_table
  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
""")
```

---

2. Implement Checkpointing

Checkpointing ensures that your streaming job can resume from where it left off in case of failure.
- 	Set a unique checkpoint location in a durable storage location (e.g., S3).
  
```scala
val checkpointLocation = "s3://your-checkpoint-path/your-table-checkpoints/"
```

---
3. Implement Rollback Mechanism

Rollback can be achieved by:
- 	Specifying a previous version of the Delta Table using the versionAsOf or timestampAsOf options.
- 	Restarting the streaming job from a particular checkpoint version.

Example:
```scala
val deltaStream = spark.readStream
  .format("delta")
  .option("readChangeData", "true")
  .option("startingVersion", "10") // Rollback to version 10
  .table("your_database.your_table")
```

---

4. Restart Processing on Failure
   
- 	Delta Lake supports fault-tolerant streaming using checkpoints.
- 	When a job fails, it will resume from the last successful commit recorded in the checkpoint directory.
  
```scala
val query = deltaStream.writeStream
  .format("console")
  .option("checkpointLocation", checkpointLocation)
  .outputMode("append")
  .start()

query.awaitTermination()
```

---

5. Fetching Latest Version of Updates

To fetch the latest version of updates, read from the table without specifying a starting version or timestamp.

```scala
val deltaStream = spark.readStream
  .format("delta")
  .option("readChangeData", "true")
  .table("your_database.your_table")
```
This will automatically start processing from the latest committed version in the Delta Table.


---

üîç Complete Code Example

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder.appName("DeltaStreamExample").getOrCreate()

// Enable CDF
spark.sql("""
  ALTER TABLE your_database.your_table
  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
""")

val checkpointLocation = "s3://your-checkpoint-path/your-table-checkpoints/"

// Read Stream with CDF Enabled
val deltaStream = spark.readStream
  .format("delta")
  .option("readChangeData", "true")
  .table("your_database.your_table")

// Processing Change Data Feed
val processedStream = deltaStream.select("*")
  .withColumn("change_type", col("_change_type"))

// Handling Inserts, Updates, and Deletes Separately
val inserts = processedStream.filter(col("change_type") === "insert")
val updates = processedStream.filter(col("change_type") === "update_postimage")
val deletes = processedStream.filter(col("change_type") === "delete")

// Write Inserts to Console (Replace with S3, Delta Table, etc. for production use)
val query = inserts.writeStream
  .format("console")
  .option("checkpointLocation", checkpointLocation)
  .outputMode("append")
  .start()

query.awaitTermination()
```

üîë Explanation

| Aspect            | Description                                                                 |
|-------------------|-----------------------------------------------------------------------------|
| `Checkpointing`   | Ensures fault-tolerance. Checkpoints are stored in S3 (checkpointLocation) to resume processing from the last successful state. |
| `Rollback`        | Achieved via `startingVersion` or `timestampAsOf` options to reprocess from a specific version. |
| `Restart Processing` | Automatic when checkpointing is configured. Resumes from the last commit. |
| `Fetch Latest`    | When starting a new query without specifying `startingVersion` or `timestampAsOf`, the latest version is read. |
| `CDF`             | Enables tracking of insert, update, and delete operations via `readChangeData`. |


---


üìå Best Practices

1.	Use Unique Checkpoint Paths: Ensure each job uses a distinct checkpoint path to avoid conflicts.
2.	Store Checkpoints on Durable Storage: Use S3 or similar for resilience against node failures.
3.	Set Proper Retention Periods: Ensure Delta Table has a sufficient retention period for your checkpointing needs (delta.logRetentionDuration and delta.checkpointRetentionDuration).
4.	Use Structured Streaming for Scalability: Ensure your jobs use Spark‚Äôs structured streaming API for resilience and fault tolerance.


---


üîë CDF Retention Policy


Change Data Feed (CDF) data in Delta tables is NOT persisted forever by default. It is retained based on the table‚Äôs retention policies.
The retention duration for CDF data is controlled by the delta.changeDataFeed.retentionDuration table property. By default, it is set to 30 days.

```sql
ALTER TABLE your_database.your_table
SET TBLPROPERTIES (delta.changeDataFeed.retentionDuration = '30 days')
```

- 	Default Value: 30 days
- 	Maximum Value: No hard limit, but storing CDF data indefinitely can result in excessive storage costs.
- 	Minimum Value: Must be at least the value of delta.logRetentionDuration (default is 30 days).

---

üìå Important Points

1.	Retention Duration:
- 	After the retention period, CDF data is no longer accessible for streaming or batch queries.
- 	If you try to read changes older than the retention period, you will get an error.

2.	Data Storage Cost:
- 	Keeping CDF data indefinitely can cause the Delta log and underlying storage to grow rapidly, impacting both cost and performance.
- 	To avoid this, only retain CDF data as long as necessary for your processing needs.

3.	Checkpointing vs. Retention:
- 	Checkpointing ensures that your streaming query state is saved for fault tolerance.
- 	If your query is paused for longer than the CDF retention period, you may lose access to old changes and have to restart processing from a newer snapshot.

4.	Archiving CDF Data (if needed):
- 	If you need to keep CDF data longer, periodically snapshot the table or archive the change data by copying it to another Delta table or storage location.

---

üí° Best Practice Recommendation

- 	Set delta.changeDataFeed.retentionDuration based on your business needs and processing frequency.
- 	If you are processing data weekly, ensure your retention duration is at least slightly longer than your processing interval.
- 	Regularly archive older CDF data to a different table if you need to maintain a history beyond the default retention period.

---

üìå Steps to Take a Baseline Snapshot

1.	Read the Full Table Without CDF:

Read the Delta table as a regular batch query. This will give you the latest state of the table.

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder().appName("BaselineSnapshot").getOrCreate()

val tableName = "arga_analytics_data_dev.researchers_v1_0.test"

// Read the entire table as a baseline snapshot
val baselineSnapshot = spark.read.format("delta").table(tableName)

// Show a sample of the snapshot
baselineSnapshot.show(5)
```


2.	Write Baseline Snapshot to a New Delta Table (for Archiving or Restarting CDF Processing):

If you want to keep this snapshot as a baseline for future processing, save it to another Delta table.

```scala
baselineSnapshot.write
  .format("delta")
  .mode("overwrite")
  .saveAsTable("arga_analytics_data_dev.researchers_v1_0.baseline_snapshot")
```


3.	Restart CDF Processing Using the Baseline:

After creating the baseline, if you want to resume processing with CDF, follow these steps:
- 	Start reading from the baseline snapshot as your starting point.
- 	Apply CDF processing on top of this snapshot for all new updates moving forward.

```scala
val cdfStream = spark.readStream.format("delta")
  .option("readChangeData", "true")
  .option("startingVersion", "latest") // Starts from the latest available changes
  .table(tableName)
```

üí° Why This Works

- 	When CDF data expires, only the change logs (_change_type) are no longer accessible. The actual table state remains intact.
- 	Taking a full snapshot gives you a complete view of the table at that point in time.
- 	You can then re-enable CDF streaming on this snapshot by specifying a new checkpoint or restarting your query from a specific version.


üî• Best Practice

- 	Periodically create snapshots if you need to maintain a consistent baseline or want to archive historical states.
- 	Automate snapshot creation with Databricks Workflows if needed.

  
----
## Basics of Stream Processing


### üìå Options for Delta Table Streaming Reads (spark.readStream.format("delta"))


| Option              | Definition                                                                 | Use Case                                                                | Example / Table View                                        |
|---------------------|---------------------------------------------------------------------------|------------------------------------------------------------------------|-------------------------------------------------------------|
| `ignoreChanges`      | When true, processes only new rows (appends), ignoring updates and deletes. | When you only care about newly inserted records and want to ignore updates/deletes. | Sales logs where only new purchases are important.          |
| `readChangeData`     | When true, enables Change Data Feed (CDF) to capture insert, update, and delete. | When you need to process full change history for auditing, replication, or ETL. | Full CDC processing: ETL job that replicates tables across systems. |
| `ignoreDeletes`      | When true, ignores rows marked for deletion.                              | When deletions are not important, like for log processing or appending datasets. | Log table where deleted records are not relevant.           |
| `skipChangeCommits`  | When true, ignores entire transactions committed via operations like MERGE, DELETE, UPDATE. | When you are only interested in newly inserted data, ignoring batch updates. | High-throughput ingestion where frequent updates are rare or irrelevant. |


üöÄ Summary

- 	ignoreChanges: Skip updates and deletes, only process new rows.
- 	readChangeData: Enable CDF for capturing all data changes.
- 	ignoreDeletes: Skip processing deletions.
- 	skipChangeCommits: Ignore transactions from MERGE, UPDATE, DELETE.

---

### checkpointLocation


Why We Need .option("checkpointLocation", "s3://your-checkpoint-path/updates/")

The checkpointLocation option is required in Spark Structured Streaming because it provides fault tolerance and consistency for streaming queries.


üîç What is checkpointLocation?

It is a directory where Spark stores metadata about the streaming query‚Äôs progress, including:

1.	Offsets: Tracking which data has already been processed from the source (e.g., Delta table, Kafka).
2.	State Information: For stateful operations (e.g., aggregations, joins).
3.	Metadata Logs: Keeping track of batches processed so far to resume from where it left off if the stream is restarted.

üîí Why is it Important?

1.	Fault Tolerance:
- If your streaming query crashes or is manually stopped, Spark can resume processing from the last recorded checkpoint instead of starting from scratch. This is essential for reliable data processing.
  
2.	Exactly Once Processing:
- For sources that support it (like Delta Lake), checkpoints help ensure that data is processed exactly once, avoiding duplicates or missing records.

3.	Streaming Query Restarting:
- When you restart a streaming query, Spark reads the checkpoint logs to pick up from the last processed point.

4.	Stateful Operations Support:
- If you are doing complex operations like aggregations, joins, or window functions, Spark needs to persist intermediate results which are stored in the checkpoint.



üî• Example Usage

```scala
val insertQuery = inserts.writeStream
  .format("console")
  .option("checkpointLocation", "s3://your-checkpoint-path/inserts/")
  .outputMode("append")
  .start()
```

üìå Best Practice

Always specify a checkpointLocation when you start a streaming query. Use a persistent and reliable storage location like:

- 	S3 for cloud environments.
- 	DBFS (Databricks File System) for Databricks.
- 	HDFS for on-premise clusters.


---

The checkpoint will record state every time a batch is successfully completed, not only when it fails.

‚úÖ How Checkpointing Works in Delta Lake / Spark Streaming

1.	During Processing:
- 	As the batch or streaming job is processing data, Spark will periodically write progress and metadata information to the checkpoint directory.
- 	This includes offsets, the state of aggregation operations, and other necessary metadata to resume processing.

2.	On Successful Completion:
- 	When a batch processing is completed successfully, Spark commits the state to the checkpoint location.
- 	This ensures that if the job is restarted later, it resumes from the last successfully completed batch.

3.	On Failure:
- 	If a batch processing job fails before completion, the state for that batch will not be committed to the checkpoint directory.
- 	When restarted, the job will start processing from the last successfully committed state in the checkpoint directory.

üìÇ What Gets Stored in Checkpoints?

- 	Offsets: Tracking which data has been processed (especially important for streaming).
- 	Aggregations State: If you‚Äôre performing any aggregations, the intermediate results are saved.
- 	Metadata: Configuration settings, schema information, etc.
- 	Committed Files List: If writing to Delta tables, the list of successfully written files.
 
Example Checkpoint Path Usage:

 ```scala
 val df = spark.readStream.format("delta")
  .option("readChangeData", "true")
  .table("your_table_name")

df.writeStream
  .format("delta")
  .option("checkpointLocation", "s3://your-checkpoint-path/")
  .start("s3://your-output-path/")
```

üîë Key Takeaway

- 	Checkpointing is always performed after a successful batch completion.
- 	If a job fails before completion, the state from that failed attempt is not recorded.
- 	Restarting the job will resume from the last successful checkpoint.

  
---

### awaitTermination


üîç What .awaitTermination() Does

.awaitTermination() is a method you call on a streaming query to block the main thread until the streaming query is terminated manually or by an error.

When you start a stream using:

```scala
val query = yourDataFrame.writeStream
  .format("console")
  .start()
```
The start() method returns immediately with a StreamingQuery object. If you do not call .awaitTermination(), the Spark application will end immediately, and your streaming job will stop processing.


üö© Why .awaitTermination() Was Not Included Above

The reason I didn‚Äôt include it is because you are launching multiple streaming queries (inserts, updates, deletes) separately. Calling .awaitTermination() on any single query would cause the other queries to stop being processed.


If you want to process all streams continuously, you need to collect them and call .awaitTermination() on each or manage them properly. Here‚Äôs how to do it:

```scala
val insertQuery = inserts.writeStream
  .format("console")
  .option("checkpointLocation", "s3://your-checkpoint-path/inserts/")
  .outputMode("append")
  .start()

val updateQuery = updates.writeStream
  .format("console")
  .option("checkpointLocation", "s3://your-checkpoint-path/updates/")
  .outputMode("append")
  .start()

val deleteQuery = deletes.writeStream
  .format("console")
  .option("checkpointLocation", "s3://your-checkpoint-path/deletes/")
  .outputMode("append")
  .start()

// Block until all streams have terminated
insertQuery.awaitTermination()
updateQuery.awaitTermination()
deleteQuery.awaitTermination()
```


üî• Best Practice

If you have multiple streams, you should consider monitoring them with something like:

```scala
val queries = Seq(insertQuery, updateQuery, deleteQuery)
queries.foreach(_.awaitTermination())
```
Or, for graceful shutdown handling:

```scala
import scala.concurrent.ExecutionContext.Implicits.global
import scala.concurrent.Future

Future { insertQuery.awaitTermination() }
Future { updateQuery.awaitTermination() }
Future { deleteQuery.awaitTermination() }

// Block the main thread indefinitely
Thread.currentThread().join()
```


---


## Example to Test readStream  from Delta tbale with CDF  Neabled


### Enable CDF in the delta table

```sql
ALTER TABLE arga_analytics_data_dev.researchers_v1_0.test SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
```



### Consumer Notebook Example

Example-1

```scala
import org.apache.spark.sql.functions.{lit, current_timestamp}
import org.apache.spark.sql.functions.size
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.{SparkSession, DataFrame, Dataset, Row}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Column
import io.delta.tables._


val tb_name = "arga_analytics_data_dev.researchers_v1_0.test"


val deltaStream = spark.readStream.format("delta")
  .option("readChangeData", "true")
   .table(tb_name)

val checkpointLocation = "s3a://wos-searchdata-dev/delta_checkpoints/test-table/"

// Output the stream to the console
val query = deltaStream.writeStream
  .format("console")
  .outputMode("append")
  .option("truncate", "false")   
  .option("checkpointLocation", checkpointLocation)
  .start()

// Await termination of the query
query.awaitTermination()

```


Example-2

```scala
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.Trigger

val tb_name = "arga_analytics_data_dev.researchers_v1_0.test"

val deltaStream = spark.readStream
  .format("delta")
  .option("readChangeData", "true")
  .table(tb_name)

val processedStream = deltaStream
  .withColumn("change_type", col("_change_type"))

// Handling Inserts
val inserts = processedStream.filter(col("change_type") === "insert")

// Handling Updates
val updates = processedStream.filter(col("change_type") === "update_postimage")

// Handling Deletes
val deletes = processedStream.filter(col("change_type") === "delete")

// Writing Inserts to Console
val insertQuery = inserts.writeStream
  .format("console")
 // .option("checkpointLocation", "s3://your-checkpoint-path/inserts/")
  .outputMode("append")
  .start()

// Writing Updates to Console
val updateQuery = updates.writeStream
  .format("console")
 // .option("checkpointLocation", "s3://your-checkpoint-path/updates/")
  .outputMode("append")
  .start()

// Writing Deletes to Console
val deleteQuery = deletes.writeStream
  .format("console")
 // .option("checkpointLocation", "s3://your-checkpoint-path/deletes/")
  .outputMode("append")
  .start()



// Block until all streams have terminated
insertQuery.awaitTermination()
updateQuery.awaitTermination()
deleteQuery.awaitTermination()
```



### Source Update Notebook Example

```sql

UPDATE arga_analytics_data_dev.researchers_v1_0.test
SET 
    wos_count = 25,            -- New value for wos_count
    tot_cites = 600,           -- New value for tot_cites
    grant_count = 5,           -- New value for grant_count
    award_amount_usd = 100000, -- New value for award_amount_usd (if applicable)
    intl_collab_docs = 1,      -- New value for intl_collab_docs (if applicable)
    max_year = 2022,           -- New value for max_year
    min_year = 2016,           -- New value for min_year
    hindex = 10,               -- New value for hindex
    highly_cited_docs = 5,     -- New value for highly_cited_docs
    top_10_percent_docs = 6,   -- New value for top_10_percent_docs
    cites_from_patents = 2,    -- New value for cites_from_patents
    lastupdated = CURRENT_TIMESTAMP  -- Set lastupdated to current timestamp
WHERE pguid = 'urn:spm:10';  -- Use the specific pguid for updating the row

```

```sql

INSERT INTO your_table_name (
    pguid, 
    wos_count, 
    tot_cites, 
    grant_count, 
    award_amount_usd, 
    intl_collab_docs, 
    max_year, 
    min_year, 
    hindex, 
    highly_cited_docs, 
    top_10_percent_docs, 
    cites_from_patents, 
    created, 
    lastupdated
)
VALUES (
    'urn:spm:10',         -- pguid
    20,                   -- wos_count
    542,                  -- tot_cites
    NULL,                 -- grant_count (NULL as per your data)
    NULL,                 -- award_amount_usd (NULL as per your data)
    0,                    -- intl_collab_docs
    2020,                 -- max_year
    2015,                 -- min_year
    8,                    -- hindex
    0,                    -- highly_cited_docs
    4,                    -- top_10_percent_docs
    NULL,                 -- cites_from_patents (NULL as per your data)
    CURRENT_TIMESTAMP,    -- created (set to current timestamp)
    CURRENT_TIMESTAMP     -- lastupdated (set to current timestamp)
);

```


```sql
DELETE FROM arga_analytics_data_dev.researchers_v1_0.test WHERE pguid = 'urn:spm:10';

```
