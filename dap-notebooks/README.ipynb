{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d1916d7-612f-4f69-a7fb-549ebebff28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Incremental  Workflow Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f979e6-9e74-44c0-87eb-d65a9b3a0bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "##### List of Scala Objects for Incremental  Workflow Control\n",
    "\n",
    "\n",
    "| Object         | Description  |  Notebook | Framework Library |\n",
    "|----------------|-----------------------|--------------------------|--------------------------|\n",
    "| SchemaResolver  | Resolve  env, version, etc. for schema    | SchemaResolver | Yes|\n",
    "|  |           |   | |\n",
    "|  QueryProcessor|  Construct the SQl query for Incrmental logic to hide env, version,  details, etc.         | QueryProcessor  | Yes|\n",
    "|  |           |   | |\n",
    "| Watermarks      | Snapshot the data version for ACS tables for checkpoint  | Watermarks-Control | Yes|\n",
    "| Checkpoints   |  Control pipeline run status and recovery |  Watermarks-Control | Yes |\n",
    "|  |           |   | |\n",
    "|TableResolver  |  Resolve  env, version,  source/target. dataset, etc. for tables   | table_resolver  | Yes|\n",
    "|ACS  |  Auto-generted case object for all ACS tables    | table_resolver  | Yes|\n",
    "|DAP  |   Auto-generted case object for all DAP tables    | table_resolver  | Yes|\n",
    "|UDM  |   Auto-generted case object for all UDM Appendix tables    | table_resolver  | Yes|\n",
    "|  |           |   | |\n",
    "|CdcType2Reader  |a unified CDC Type-2 read function for baseline and incremental, returning deduplicated upserts and deletes |  cdc_reader | Yes |\n",
    "|CdcType1Reader  | a unified CDC Type-1 read function for baseline and incremental, returning deduplicated upserts and deletes  |  cdc_reader | Yes |\n",
    "|  |           |   | |\n",
    "| Registry | Pipeline metadata/lineage operation        |Database-Ops | maybe   |\n",
    "| DapOps           | OPS  schema operation    |  Database-Ops | No |\n",
    "| DapSchemaManager | Schema maintenance        | Database-Ops | No |\n",
    "|  |           |   |   |\n",
    "| DeltaReader | Create incrmental logic and  watermark, read delta data, just for reference  | Pipeline-Example | No |\n",
    "| PipelineRunner | Run the pipeline core logic to handle chckpoint in same approach, just for reference    |  Pipeline-Example | No |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "145104fb-99e5-4eb1-85df-8b453cd3c58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b5033e2-27e4-4f02-9df4-b1696434e9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. SchemaResolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61fa3c9d-381c-475e-80e3-229011d27f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "This object is responsible for retrieving various parameters from the bundle configuration file, including source and target information, environment, data version, pipeline name, and related settings.\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "SchemaResolver.SCHEMA_MAP\n",
    "- Used by the QueryProcessor to correctly bind and resolve schemas.\n",
    "\n",
    "SchemaResolver.ACS_SCHEMAS\n",
    "- Defines the schemas used for reading ACS tables.\n",
    "\n",
    "SchemaResolver.DAP_SCHEMAS\n",
    "- Defines the schemas used for reading from and writing to DAP tables.\n",
    "\n",
    "\n",
    "SchemaResolver.PIPELINE\n",
    "- Defines the pipeline name. This value is used to resolve the version map and to set table ownership during write operations.\n",
    "\n",
    "SchemaResolver.OPS_SCHEMA\n",
    "- Specifies the DAP schema used for workflow-related operational tables (for example, \"dap_ops\").\n",
    "\n",
    "SchemaResolver.OPS_TABLE_PREFIX\n",
    "- Defines an optional prefix for operational (OPS) tables, primarily used for testing. When set, the prefix is applied to OPS table names to isolate test data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "060ed23d-2bea-46de-967a-25bd730bc65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### 2. QueryProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d195883-75d2-47b6-a487-0e38f7904ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "**Features**\n",
    "\n",
    "- Dynamic table_changes for tables with version_param.\n",
    "- Optional per-table filter applied automatically.\n",
    "- Multi-column joins handled correctly.\n",
    "- Union across multiple tables.\n",
    "- Dynamic catalog replacements like ${entity} supported.\n",
    "- Runtime version map allows flexible filtering of selected tables.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Function: renderSqlTemplate,  renderSqlTemplateExtended**\n",
    "\n",
    "This function can be used to check the the generated SQL query before actual test, we can copy the query to SQL editor to run to validate first \n",
    "\n",
    "```scala\n",
    "\n",
    "\n",
    "// Basic function: read master table by start/end version and read appendix tables as end version\n",
    "def renderSqlTemplate(\n",
    "  sqlTemplateConfig: String, \n",
    "  tableVersionMap: Map[String, (Long, Long)] = Map.empty\n",
    "): Unit = { ... }\n",
    "\n",
    "\n",
    "\n",
    "// Extended function(experimental): extend the logic to return the row of the latest version if multiple rows exist\n",
    "def renderSqlTemplateExtended(\n",
    "  sqlTemplateConfig: String, \n",
    "  tableVersionMap: Map[String, (Long, Long)] = Map.empty\n",
    "): Unit = { ... }\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Paramters**:\n",
    "- sqlTemplateConfig: The SQL-query configuration that defines the SQL query. It contains placeholders(catalog) that need to be replaced.\n",
    "- tableVersionMap: A map for rom upstream tables: tableName → (startVersion, endVersion), where `startVersion` and `endVersion` define the inclusive version range to process for each upstream table.  the map data can be created using Watermarks \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**2. Function: runSqlAndSave (Optional)**\n",
    "\n",
    "This function is used to actually create the delta Paffected K table speficied in SQL config and save data to the table\n",
    "\n",
    "```scala\n",
    "\n",
    "\n",
    "def runSqlAndSave(\n",
    "  sqlTemplateConfig: String,\n",
    "  tableVersionMap: Map[String, (Long, Long)] = Map.empty,\n",
    "  targetTableName: String = \"\",\n",
    "  dryRun: Boolean = false\n",
    "): Unit = { ... }\n",
    "\n",
    "```\n",
    "\n",
    "**Paramters**:\n",
    "- sqlTemplateConfig: The SQL-query configuration that defines the SQL query. It contains placeholders(catalog) that need to be replaced.\n",
    "- tableVersionMap: A map for upstream tables: tableName → (startVersion, endVersion), where `startVersion` and `endVersion` define the inclusive version range to process for each upstream table.  the map data can be created using Watermarks \n",
    "- targetTableName: The taget delta tbale to save the output Dataframe\n",
    "- dryRun: Save data to the delta table when dryRun is False\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**3. Schema Map Definition: SchemaResolver.SCHEMA_MAP**\n",
    "\n",
    "This object warp the information on catalog, schema, environment and  version varibable management, which is used to render the SQl query with the dynamic varibale for catalog.\n",
    "\n",
    "\n",
    "```scala\n",
    "val catalogMap = SchemaResolver.SCHEMA_MAP\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91da5502-b8a1-4803-8392-bcffcd9e7681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. Watermarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f423c3fc-9036-4912-b198-fbc0347157f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Watermarks provides centralized management of batch-level processing state for pipelines. It tracks the latest batch ID, maintains version mappings for source tables, and manages watermark lifecycle events (initialization and completion). This class enables consistent, incremental processing by capturing stable snapshots of source data and coordinating checkpoints across pipelines.\n",
    "\n",
    "**1. Functions**\n",
    "\n",
    "**Watermarks.latestBatchId**\n",
    "- Returns the latest batch ID.\n",
    "\n",
    "**Watermarks.getWatermarkForTable**\n",
    "- Retrieves version map information for tables.\n",
    "\n",
    "The version map is represented as:\n",
    "```scala\n",
    "table_name -> (startVersion, endVersion)\n",
    "```\n",
    "\n",
    "Usage options:\n",
    "- Pass a list of table names to retrieve version mappings for those tables, or\n",
    "- If no table list is provided, the version mappings are retrieved by pipeline name by default from the registry table.\n",
    "\n",
    "**Watermarks.initializeWatermark**\n",
    "- Used by the automation pipeline (PPL) to take a snapshot of the latest versions for all ACS tables as processing watermarks.\n",
    "- This method also creates checkpoints for each pipeline, assigning an incremented batch ID that corresponds to the current, up-to-date watermark state.\n",
    "\n",
    "**Watermarks.completeWatermark**\n",
    "- Marks the watermark as complete by recording a successful completion state.\n",
    "\n",
    "\n",
    "**2. Example**\n",
    "\n",
    "\n",
    "2.1 PPL Pipeline\n",
    "\n",
    "```scala\n",
    "\n",
    " // Run this function as the first task in the PPL workflow\n",
    "Watermarks.initializeWatermark()\n",
    "\n",
    "\n",
    " // Run thisfunction  as the last task in the PPL workflow\n",
    "Watermarks.completeWatermark()\n",
    "```\n",
    "\n",
    "\n",
    "2.2 Data Pipeline\n",
    "```scala\n",
    "\n",
    "// Option-1: When upstream linegae data is available int pipeline registry table\n",
    "Watermarks.getWatermarkForTable()\n",
    "\n",
    "\n",
    "//Option-2: Pass a list of upstream tables as parameter to retrieve the resu;t \n",
    "Watermarks.getWatermarkForTable( Seq[\"d_spmster\", \"d_orgmaster\", \"d_publication_spmaster_link\" ] )\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e47618d-78c0-4238-83da-af580f8d4c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4. Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "327f56bd-d256-489d-83ee-1307bd91894b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The Checkpoints object manages pipeline runtime state and execution control. It is used by each pipeline to track the active batch ID, monitor execution status, and update checkpoint states throughout the pipeline lifecycle (Ready, Running, Failed, Success). It also supports explicitly overriding checkpoint status when needed.\n",
    "\n",
    "**1. Functions**\n",
    "\n",
    "**Checkpoints.activeBatchId**\n",
    "- Returns the currently active batch ID for the pipeline.\n",
    "\n",
    "**Checkpoints.activeStatus**\n",
    "- Returns the current execution status of the pipeline (Ready, Running, Failed, Success).\n",
    "\n",
    "**Checkpoints.markCheckpointStarted**\n",
    "- Marks the checkpoint as started and updates the pipeline status to Running.\n",
    "\n",
    "**Checkpoints.markCheckpointCompleted**\n",
    "- Marks the checkpoint as successfully completed and updates the status to Success.\n",
    "\n",
    "**Checkpoints.markCheckpointFailed**\n",
    "- - - Marks the checkpoint as failed and updates the status to Failed.\n",
    "\n",
    "**Checkpoints.updateCheckpointStatus**\n",
    "- Updates the checkpoint status explicitly, allowing forced status changes when required.\n",
    "\n",
    "\n",
    "**2. Example**\n",
    "\n",
    "\n",
    "```scala\n",
    "\n",
    "    // 1. Start the checkpoint\n",
    "    Checkpoints.markRunStarted(pipelineName)\n",
    "\n",
    "    try {\n",
    "      // 2. Load\n",
    "      val rawDF = DeltaReader.readDeltaDataWithWatermark(queryConfigPath)\n",
    "\n",
    "      // optional - skipped\n",
    "      if (rawDF.isEmpty) {\n",
    "        Checkpoints.markCheckpointSkipped(pipelineName)\n",
    "        return\n",
    "      }\n",
    "\n",
    "      // 3. Core pipeline logic \n",
    "      transformAndSave(rawDF)\n",
    "\n",
    "      // 4. Mark success\n",
    "      val params = CheckpointParams(\n",
    "        rowsRead = 200,\n",
    "        rowsWritten = 500\n",
    "      )\n",
    "      Checkpoints.markRunCompleted(Some(params))\n",
    "\n",
    "    } catch {\n",
    "      case e: Throwable =>\n",
    "        // 5. Mark failure\n",
    "        Checkpoints.markRunFailed(pipelineName, Some(e.getMessage))\n",
    "        throw e\n",
    "    }\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d1e94f-63b0-4fea-8008-4230d9b07fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 5. TableResolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee04e1a2-aea9-4683-a4ff-7b2779bf8188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Description:\n",
    "\n",
    "TableResolver provides a consistent way to use predefined case objects as table names for all ACS, DAP, and UDM tables. The case object names are derived from the actual catalog table names. All tables are defined centrally and reused across 20+ pipelines, ensuring standardization and easier maintenance.”\n",
    "\n",
    "##### Benefit:\n",
    "- Automatically resolve catalog, environment, schema, and  version\n",
    "- Centralized table naming reduces duplication and errors.\n",
    "- Enables consistent reference across multiple pipelines.\n",
    "- Simplifies updates when underlying tables change, without modifying individual pipelines.\n",
    "- Detect underlying table name changes automatically through validation\n",
    "- Prevent typos and simplify table name maintenance\n",
    "\n",
    "\n",
    "##### Attributes:\n",
    "\n",
    "- **tableName**: Actual Delta table name.\n",
    "- **schema**: Optional override for schema name (bypasses resolver).\n",
    "- **catalog**: Optional override for catalog name (bypasses resolver).\n",
    "- **role**: Indicates source (ACS, UDM) or target (DAP) table; used by resolver to fetch correct parameters.\n",
    "- **needsSuffix**: Flag to append suffix (e.g., \"woscore\") for WOS Core and ESCI datasets.\n",
    "- **primaryKeyCols**: List of primary key columns for CDC Type-2 deduplication; auto-generated but requires validation.\n",
    "- **fullName**: Complete Delta table name in the format {catalog}.{schema}.{table}.\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "1. Example - ACS Tables\n",
    "```scala\n",
    "\n",
    "val datasetList: Seq[Dataset] = Seq(Dataset.WosCore, Dataset.Pprn, Dataset.WosEsci)\n",
    "\n",
    "datasetList.foreach { t =>\n",
    "\n",
    "    TableResolver.forDataset(t)\n",
    "    println(\"-------------------------\")\n",
    "    println(s\"Dataset: $t\")\n",
    "    println(ACS.DArticleTotalCites)\n",
    "    println(ACS.DAlmaOpenaccess)\n",
    "    println(ACS.AuthorPublicationLink)\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "2. Example - DAP & UDM Tables\n",
    "```scala\n",
    "println(\"---------DAP----------------\")\n",
    "println(DAP.Alma)\n",
    "println(DAP.ApArticle)\n",
    "println(DAP.IncitesRiOrgGrants)\n",
    "\n",
    "println(\"---------UDM----------------\")\n",
    "println(UDM.GrantsTopic)\n",
    "println(UDM.ProfileGrantRelation)\n",
    "println(UDM.ItemTopic)\n",
    "\n",
    "````\n",
    "\n",
    "\n",
    "\n",
    "3. Example  - get data by attribute\n",
    "\n",
    "```scala\n",
    "  def read( tableName: TableMetadata ) : Unit = {\n",
    "        \n",
    "        println(s\"PK: {tableName.primaryKeyCols})\n",
    "        spark.read\n",
    "            .format(\"delta\")\n",
    "            .option(\"versionAsOf\", version)\n",
    "            .table(tableName.fullName)\n",
    "            .select(tableName.primaryKeyCols.map(c => col(c)):_*)  \n",
    "\n",
    "  }\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b55484c2-91b5-4f7a-ba74-bdb6c976e954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6. CdcType2Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "649f5a1e-97e1-457c-8ac5-f7e3d256218b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "A unified CDC Type-2 read function that handles both baseline and incremental data seamlessly. It reads Delta tables consistently and automatically returns a deduplicated dataset, including upserts and deletes, ensuring data integrity and simplifying downstream processing.\n",
    "\n",
    "\n",
    "\n",
    "**1. Functions & Paramters**\n",
    "\n",
    "**1.1 read**\n",
    "\n",
    "- Read baseline or incremental data consistently using start and end versions\n",
    "- Unified interface to both baseline an incremental, Baseline read when start version = end version, otherwise, Incremental read\n",
    "\n",
    "```scala\n",
    "  def read(\n",
    "      tableName: String,\n",
    "      startVersion: Long,\n",
    "      endVersion: Long,\n",
    "      selectedCols: Seq[String] = Seq.empty,\n",
    "      primaryKeyCols: Seq[String] =  Seq.empty\n",
    "  ): (DataFrame, DataFrame)\n",
    "\n",
    "```\n",
    "\n",
    "**1.2  readBaseline**\n",
    "\n",
    "- Reads baseline data, optionally specifying a version\n",
    "- Baseline snapshot AS-OF a given Delta version If endVersion is None => use latest version\n",
    "\n",
    "```scala\n",
    "  def readBaseline(\n",
    "      tableName: String,\n",
    "      endVersion: Option[Long] = None,\n",
    "      selectedCols: Seq[String] = Seq.empty,\n",
    "      primaryKeyCols: Seq[String] = Seq.empty\n",
    "  ): DataFrame\n",
    "\n",
    "```\n",
    "\n",
    "**1.3  readIncremental**\n",
    "\n",
    "- Reads incremental data between a start version and an end version\n",
    "- Incremental read using Delta Change Data Feed. If selectedCols is empty => read all columns\n",
    "\n",
    "```scala\n",
    "  def readIncremental(\n",
    "      tableName: String,\n",
    "      startVersion: Long,\n",
    "      endVersion: Long,\n",
    "      selectedCols: Seq[String] = Seq.empty,\n",
    "      primaryKeyCols: Seq[String] = Seq.empty\n",
    "  ): (DataFrame, DataFrame) \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**2. Exampe**\n",
    "\n",
    "Example with watermarks\n",
    "```scala\n",
    "\n",
    "  val versionMap = Watermarks.getWatermarkForTable()\n",
    "\n",
    "  val (startVersion, endVersion ) = versionMap[ACS.FPublication.ACS.FPublication ]\n",
    "\n",
    "  // 1. example -  read \n",
    "  val (upserts, deletes) = CdcType2Reader.read(\n",
    "        ACS.FPublication,\n",
    "        startVersion = startVersion,\n",
    "        endVersion   = endVersion\n",
    "    )\n",
    "\n",
    "  // 2. example - readIncremental with primaryKeyCols paramter\n",
    "  val (upserts, deletes) = CdcType2Reader.readIncremental(\n",
    "      ACS.FPublication.fullName,\n",
    "      startVersion = startVersion,\n",
    "      endVersion   = endVersion,\n",
    "      Seq(\"uid\")\n",
    "  )\n",
    "\n",
    "  // 3. example - readBaseline without version paramter and with selectedCols paramter\n",
    "  val df =\n",
    "  CdcType2Reader.readBaseline(\n",
    "    ACS.FPublication.fullName,\n",
    "    selectedCols = Seq(\"uid\", \"pub_year\", \"__END_AT\"),\n",
    "  )\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bb67e65-e676-40fd-bdf9-370159d7a121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7. CdcType1Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9fb3e2c-e2b0-4ad2-8f1e-394d9f13e443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "A unified CDC Type-1 read function that handles both baseline and incremental data seamlessly. It reads Delta tables consistently and automatically returns a deduplicated dataset, including upserts and deletes, ensuring data integrity and simplifying downstream processing.\n",
    "\n",
    "\n",
    "\n",
    "**1. Functions & Paramters**\n",
    "\n",
    "**1.1 read**\n",
    "\n",
    "-  Read baseline or incremental data consistently using start and end versions\n",
    "- Unified interface to both baseline an incremental, Baseline read when start version = end version, otherwise, Incremental read\n",
    "\n",
    "```scala\n",
    "  def read(\n",
    "      tableName: String,\n",
    "      startVersion: Long,\n",
    "      endVersion: Long,\n",
    "      selectedCols: Seq[String] = Seq.empty\n",
    "  ): (DataFrame, DataFrame) \n",
    "\n",
    "```\n",
    "\n",
    "**1.2  readBaseline**\n",
    "\n",
    "- Reads baseline data, optionally specifying a version\n",
    "- Read baseline snapshot for Type-1, If selectedCols empty => read all columns\n",
    "\n",
    "```scala\n",
    "  def readBaseline(\n",
    "      tableName: String,\n",
    "      endVersion: Option[Long] = None,\n",
    "      selectedCols: Seq[String] = Seq.empty\n",
    "  ): DataFrame\n",
    "\n",
    "```\n",
    "\n",
    "**1.3  readIncremental**\n",
    "\n",
    "- Reads incremental data between a start version and an end version\n",
    "- Read incremental changes for Type-1 via CDF (if available), Returns (upserts, deletes)\n",
    "\n",
    "```scala\n",
    "  def readIncremental(\n",
    "      tableName: String,\n",
    "      startVersion: Long,\n",
    "      endVersion: Long,\n",
    "      selectedCols: Seq[String] = Seq.empty\n",
    "  ): (DataFrame, DataFrame)\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "README",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}