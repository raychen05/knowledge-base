{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a736d1f6",
   "metadata": {},
   "source": [
    "#### Define DapIO Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96effa54",
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession, Column, Row}\n",
    "import org.apache.spark.sql.functions._\n",
    "import io.delta.tables.DeltaTable\n",
    "import scala.jdk.CollectionConverters._\n",
    "import java.sql.Timestamp\n",
    "import spark.implicits._\n",
    "import org.yaml.snakeyaml.Yaml\n",
    "import java.io.FileWriter\n",
    "import java.nio.file.{Files, Paths}\n",
    "import java.nio.charset.StandardCharsets\n",
    "import java.sql.Timestamp\n",
    "import java.time.Instant\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession, Column}\n",
    "import org.apache.spark.sql.functions._\n",
    "import io.delta.tables.DeltaTable\n",
    "import scala.jdk.CollectionConverters._\n",
    "import spark.implicits._\n",
    "import org.yaml.snakeyaml.DumperOptions\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "import scala.io.Source\n",
    "\n",
    "\n",
    "\n",
    "// 3. Spark Read / Write Utilities\n",
    "object DapIO    {\n",
    "\n",
    "  val basePath =  SchemaResolver.DAP_VOLUME\n",
    "\n",
    "  def readTable(schema: String, table: String): DataFrame =\n",
    "    spark.table(s\"$schema.$table\")\n",
    "\n",
    "  def writeTable(\n",
    "      df: DataFrame, \n",
    "      schema: String, \n",
    "      table: String,\n",
    "      overwrite: Boolean = true\n",
    "    ): Unit =\n",
    "\n",
    "    val mode = if (overwrite) \"overwrite\" else \"append\"\n",
    "    df.write\n",
    "      .mode(\"overwrite\")\n",
    "      .saveAsTable(s\"$schema.$table\")\n",
    "\n",
    "\n",
    "  def readFileAsString(fileName: String): String = {\n",
    "    val fileFullPath = s\"$basePath$fileName\"\n",
    "\n",
    "    // Read the SQL file as a DataFrame of lines\n",
    "    val dfLines = spark.read.text(fileFullPath)\n",
    "\n",
    "    // Convert to a single string\n",
    "    dfLines.collect().map(_.getString(0)).mkString(\"\\n\")\n",
    "  }\n",
    "\n",
    "  def readSQL(fileName: String): List[String] = {\n",
    "    val fileFullPath = s\"$basePath$fileName\"\n",
    "    // Read the SQL file as a DataFrame of lines\n",
    "    val dfLines = spark.read.text(fileFullPath)\n",
    "\n",
    "    // Convert to a single string\n",
    "    dfLines.collect().map(_.getString(0)).mkString(\"\\n\")\n",
    "        .split(\";\")\n",
    "        .map(_.trim)\n",
    "        .filter(_.nonEmpty)\n",
    "        .toList\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "  def readCSV( fileName:String): DataFrame = {\n",
    "      val fileFullPath = s\"$basePath$fileName\"\n",
    "      spark.read\n",
    "          .option(\"header\", \"true\") // first row is header\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .option(\"delimiter\", \",\")   // custom delimiter\n",
    "          .csv(fileFullPath)\n",
    "  }\n",
    "\n",
    "  def writeCSV(df: DataFrame, fileName:String): Unit = {\n",
    "    \n",
    "    val fileFullPath = s\"$basePath$fileName\"\n",
    "    df.write\n",
    "      .mode(SaveMode.Overwrite)   // Overwrite existing file\n",
    "      .option(\"header\", \"true\")   // Include column names\n",
    "      .option(\"delimiter\", \",\")   // Custom delimiter\n",
    "      .csv(fileFullPath)\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "  def readJSON( fileName:String): DataFrame = {\n",
    "      val fullFileName = s\"$basePath$fileName\"\n",
    "      spark.read\n",
    "          .option(\"multiLine\", \"true\")\n",
    "          .json(fullFileName)\n",
    "  }\n",
    "\n",
    "  def  writeJSON(\n",
    "      df: DataFrame,\n",
    "      path: String,\n",
    "      overwrite: Boolean = true\n",
    "    ): DataFrame = {\n",
    "      val fullFileName = s\"$basePath$path\"\n",
    "      val mode = if (overwrite) \"overwrite\" else \"append\"\n",
    "      df.write\n",
    "        .mode(mode)\n",
    "        .json(fullFileName)\n",
    "  }\n",
    "\n",
    "\n",
    "  def readYAML(fileName: String): Map[String, Any] = {\n",
    "    val fileFullPath = s\"$basePath$fileName\"\n",
    "    val yamlContent = spark.read.textFile(fileFullPath).collect().mkString(\"\\n\")\n",
    "\n",
    "    val yaml = new Yaml()\n",
    "    val javaMap =\n",
    "      yaml.load(yamlContent).asInstanceOf[java.util.Map[String, Any]]\n",
    "\n",
    "    javaMap.asScala.toMap\n",
    "  }\n",
    "\n",
    "  def writeYAML(\n",
    "      data: String,\n",
    "      fileName:String\n",
    "    ): Unit = {\n",
    "    \n",
    "    val fileFullPath = s\"$basePath$fileName\"\n",
    "\n",
    "    // Optional: prettier formatting\n",
    "    val options = new DumperOptions()\n",
    "    options.setDefaultFlowStyle(DumperOptions.FlowStyle.BLOCK)\n",
    "\n",
    "    val yamlWriter = new Yaml(options)\n",
    "    val writer = new FileWriter(fileFullPath)\n",
    "\n",
    "    // Write the same data or any Map/Seq\n",
    "    yamlWriter.dump(data, writer)\n",
    "    writer.close()\n",
    "\n",
    "  }\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5459c",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736679b3",
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val df  = DapIO.readJSON(\"dap_job_list.json\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6635a4",
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    " val config = DapIO.readYAML(\"1p-common-analytics-service-doc.yaml\")\n",
    "config.map(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf7d30",
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    " val df = DapIO.readCSV(\"dap_lineage.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e0ddf",
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "val sql_stmts = DapIO.readSQL(\"insert_dap_pipeline_registry.sql\")\n",
    "sql_stmts.foreach(println)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
