{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7be2d318-05a6-4dce-9efc-8b0c2b4cbbf2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "enable paramters for test"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "// the following parameters are from thebundle's databrick yaml config file \n",
    "\n",
    "//  pass the parameters\n",
    "dbutils.widgets.text(\"source_catalog\", \"ag_content_ims_acs\")\n",
    "dbutils.widgets.text(\"source_environment\", \"prod\")\n",
    "dbutils.widgets.text(\"source_version\", \"v1_0_0\")\n",
    "\n",
    "dbutils.widgets.text(\"target_catalog\", \"ag_ra_search_analytics_data\")\n",
    "dbutils.widgets.text(\"target_environment\", \"dev\")\n",
    "dbutils.widgets.text(\"target_version\", \"v1_0\")\n",
    "\n",
    "dbutils.widgets.text(\"pipeline_name\", \"agra-sa-authorprofile-pipeline\")\n",
    "\n",
    "\n",
    "// dynamic paramters\n",
    "val source_catalog = dbutils.widgets.get(\"source_catalog\")\n",
    "val source_environment = dbutils.widgets.get(\"source_environment\")\n",
    "val source_version = dbutils.widgets.get(\"source_version\")\n",
    "val target_catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "val target_environment = dbutils.widgets.get(\"target_environment\")\n",
    "val target_version = dbutils.widgets.get(\"target_version\")\n",
    "val pipeline_name = dbutils.widgets.get(\"pipeline_name\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6acc5ebc-e07e-4c5b-9c0e-1e717411aadd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "include dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Shared/wosri/dap_shared/table_resolver\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2adc56b-5373-4ba0-8984-d829d0d2958b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define CDC Reader - Type 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ab94cefa-e518-408d-88bb-2845abca78cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CdcType2Reader"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import io.delta.tables.DeltaTable\n",
    "\n",
    "object CdcType2Reader {\n",
    "\n",
    "  // CdcType2Reader - for ACS scheam CDC Type2\n",
    "  private val CdfSystemCols =\n",
    "    Seq(\"_commit_version\", \"_change_type\")\n",
    "\n",
    "  // interface\n",
    "  def read(\n",
    "      tableName: String,\n",
    "      startVersion: Long,\n",
    "      endVersion: Long,\n",
    "      selectedCols: Seq[String] = Seq.empty,\n",
    "      primaryKeyCols: Seq[String] =  Seq.empty\n",
    "  ): (DataFrame, DataFrame) = {\n",
    "\n",
    "    if(startVersion == endVersion) {\n",
    "      val df =  readBaseline(\n",
    "         tableName,  \n",
    "          Some(endVersion),\n",
    "          Seq(\"uid\")\n",
    "        )\n",
    "      (df, spark.emptyDataFrame)\n",
    "    }\n",
    "    else {\n",
    "      readIncremental(\n",
    "          tableName,\n",
    "          startVersion = startVersion,\n",
    "          endVersion   = endVersion,\n",
    "          Seq(\"uid\")\n",
    "        )\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * Baseline snapshot AS-OF a given Delta version\n",
    "    * If endVersion is None => use latest version\n",
    "    */\n",
    "  def readBaseline(\n",
    "      tableName: String,\n",
    "      endVersion: Option[Long] = None,\n",
    "      selectedCols: Seq[String] = Seq.empty,\n",
    "      primaryKeyCols: Seq[String] = Seq.empty\n",
    "  ): DataFrame = {\n",
    "\n",
    "    val (df, baselineVersionOpt) =\n",
    "      try {\n",
    "        val deltaTable = DeltaTable.forName(spark, tableName)\n",
    "\n",
    "        val version =\n",
    "          endVersion.getOrElse {\n",
    "            deltaTable\n",
    "              .history(1)\n",
    "              .select(max(col(\"version\")))\n",
    "              .as[Long]\n",
    "              .collect()\n",
    "              .head\n",
    "          }\n",
    "\n",
    "        val df = spark.read\n",
    "          .format(\"delta\")\n",
    "          .option(\"versionAsOf\", version)\n",
    "          .table(tableName)\n",
    "\n",
    "        (df, Some(version))\n",
    "      }\n",
    "      catch {\n",
    "          case _: Exception =>\n",
    "            // Fallback: non-Delta table or view\n",
    "            val df = spark.table(tableName)\n",
    "\n",
    "          (df, None)\n",
    "        }\n",
    "\n",
    "    val projected =\n",
    "      if (selectedCols.isEmpty) df\n",
    "      else {\n",
    "        val requiredCols =\n",
    "          (primaryKeyCols ++ selectedCols).distinct\n",
    "        df.select(requiredCols.map(col): _*)\n",
    "      }\n",
    "\n",
    "    val result = projected\n",
    "      .filter(col(\"__END_AT\").isNull)\n",
    "      .withColumn(\"_op\", lit(\"BASELINE\"))\n",
    "    // .withColumn(\"_baseline_version\", lit(baselineVersion))\n",
    "\n",
    "\n",
    "      baselineVersionOpt match {\n",
    "        case Some(v) => result.withColumn(\"_baseline_version\", lit(v))\n",
    "        case None    => result\n",
    "      }\n",
    "  }\n",
    "\n",
    "\n",
    "  /**\n",
    "   * Incremental read using Delta Change Data Feed\n",
    "   * If selectedCols is empty => read all columns\n",
    "   */\n",
    "  def readIncremental(\n",
    "      tableName: String,\n",
    "      startVersion: Long,\n",
    "      endVersion: Long,\n",
    "      selectedCols: Seq[String] = Seq.empty,\n",
    "      primaryKeyCols: Seq[String] = Seq.empty\n",
    "  ): (DataFrame, DataFrame) = {\n",
    "\n",
    "    if (startVersion > endVersion) {\n",
    "      return (spark.emptyDataFrame, spark.emptyDataFrame)\n",
    "    }\n",
    "\n",
    "    val cdf = spark.read\n",
    "      .format(\"delta\")\n",
    "      .option(\"readChangeFeed\", \"true\")\n",
    "      .option(\"startingVersion\", startVersion)\n",
    "      .option(\"endingVersion\", endVersion)\n",
    "      .table(tableName)\n",
    "      .filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\", \"delete\"))\n",
    "\n",
    "    val projected = projectCols(cdf, primaryKeyCols, selectedCols, includeCdfCols = true)\n",
    "\n",
    "    val latestPerKey = dedupeLatest(projected, primaryKeyCols)\n",
    "\n",
    "    val upserts = latestPerKey\n",
    "      .filter(col(\"__END_AT\").isNull)\n",
    "      .withColumn(\"_op\", lit(\"UPSERT\"))\n",
    "\n",
    "    val deletes = latestPerKey\n",
    "      .filter(col(\"__END_AT\").isNotNull)\n",
    "      .withColumn(\"_op\", lit(\"DELETE\"))\n",
    "\n",
    "    (upserts, deletes)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Deduplicate per business key using commit version ordering\n",
    "   */\n",
    " private def dedupeLatest(\n",
    "    df: DataFrame,\n",
    "    primaryKeyCols: Seq[String] = Seq.empty\n",
    "): DataFrame = {\n",
    "\n",
    "  if (primaryKeyCols.isEmpty) {\n",
    "    // No deduplication requested\n",
    "    df\n",
    "  } else {\n",
    "    val windowSpec = Window\n",
    "      .partitionBy(primaryKeyCols.map(col): _*)\n",
    "      .orderBy(col(\"_commit_version\").desc_nulls_last)\n",
    "\n",
    "    df.withColumn(\"_rn\", row_number().over(windowSpec))\n",
    "      .filter(col(\"_rn\") === 1)\n",
    "      .drop(\"_rn\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Project columns safely:\n",
    "   * - If selectedCols empty => return df as-is\n",
    "   * - Otherwise ensure primary keys + system columns exist\n",
    "   */\n",
    "  private def projectCols(\n",
    "      df: DataFrame,\n",
    "      primaryKeyCols: Seq[String] = Seq.empty,\n",
    "      selectedCols: Seq[String] = Seq.empty,\n",
    "      includeCdfCols: Boolean = false\n",
    "  ): DataFrame = {\n",
    "\n",
    "    if (selectedCols.isEmpty && primaryKeyCols.isEmpty) {\n",
    "      df\n",
    "    } else {\n",
    "      val requiredCols =\n",
    "        (primaryKeyCols ++ selectedCols ++\n",
    "          (if (includeCdfCols) CdfSystemCols else Seq(\"_commit_version\")))\n",
    "          .toSeq\n",
    "          .distinct\n",
    "\n",
    "      val existingCols = df.columns.toSet\n",
    "      val finalCols = requiredCols.filter(existingCols.contains)\n",
    "\n",
    "      df.select(finalCols.map(col): _*)\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "253e530c-3c77-4b2d-b001-2d9e68787725",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CdcType1Reader"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.chaining._\n",
    "\n",
    "\n",
    "object CdcType1Reader {\n",
    "\n",
    "  private val CdfSystemCols = Seq(\"_commit_version\", \"_change_type\")\n",
    "\n",
    "\n",
    " // interface\n",
    "  def read(\n",
    "      tableName: String,\n",
    "      startVersion: Long,\n",
    "      endVersion: Long,\n",
    "      selectedCols: Seq[String] = Seq.empty\n",
    "  ): (DataFrame, DataFrame) = {\n",
    "\n",
    "    if(startVersion == endVersion) {\n",
    "      val df =  readBaseline(\n",
    "         tableName,   \n",
    "          Some(endVersion),\n",
    "          selectedCols\n",
    "        )\n",
    "      (df, spark.emptyDataFrame)\n",
    "    }\n",
    "    else {\n",
    "      readIncremental(\n",
    "          tableName,\n",
    "          startVersion = startVersion,\n",
    "          endVersion   = endVersion,\n",
    "          selectedCols\n",
    "        )\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Read baseline snapshot for Type-1\n",
    "   * If selectedCols empty => read all columns\n",
    "   */\n",
    "  def readBaseline(\n",
    "      tableName: String,\n",
    "      endVersion: Option[Long] = None,\n",
    "      selectedCols: Seq[String] = Seq.empty\n",
    "  ): DataFrame = {\n",
    "\n",
    "    val df =\n",
    "      try {\n",
    "        // Try Delta table\n",
    "        val version = endVersion.getOrElse {\n",
    "          io.delta.tables.DeltaTable.forName(spark, tableName)\n",
    "            .history(1).select(max(\"version\")).as[Long].collect().head\n",
    "        }\n",
    "        spark.read.format(\"delta\").option(\"versionAsOf\", version).table(tableName)\n",
    "      } catch {\n",
    "        case _: Exception =>\n",
    "          // Fallback to path or view\n",
    "          spark.table(tableName)\n",
    "      }\n",
    "\n",
    "    if (selectedCols.isEmpty) df else df.select(selectedCols.map(col): _*)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Read incremental changes for Type-1 via CDF (if available)\n",
    "   * Returns only upserts (no deletes for Type-1)\n",
    "   */\n",
    "  /**\n",
    "   * Incremental CDF for Type-1\n",
    "   * Returns (upserts, deletes)\n",
    "   */\n",
    "  def readIncremental(\n",
    "      tableName: String,\n",
    "      startVersion: Long,\n",
    "      endVersion: Long,\n",
    "      selectedCols: Seq[String] = Seq.empty\n",
    "  ): (DataFrame, DataFrame) = {\n",
    "\n",
    "    if (startVersion > endVersion) return (spark.emptyDataFrame, spark.emptyDataFrame)\n",
    "\n",
    "    val cdfDF = spark.read.format(\"delta\")\n",
    "      .option(\"readChangeFeed\", \"true\")\n",
    "      .option(\"startingVersion\", startVersion)\n",
    "      .option(\"endingVersion\", endVersion)\n",
    "      .table(tableName)\n",
    "      .filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\", \"delete\"))\n",
    "\n",
    "    // Include selected columns + system columns for CDF\n",
    "    val projected =\n",
    "      if (selectedCols.isEmpty) cdfDF\n",
    "      else {\n",
    "        val colsToSelect = (selectedCols ++ CdfSystemCols).distinct.filter(cdfDF.columns.contains)\n",
    "        cdfDF.select(colsToSelect.map(col): _*)\n",
    "      }\n",
    "\n",
    "    // Split upserts vs deletes\n",
    "    val upserts = projected.filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
    "      .withColumn(\"_op\", lit(\"UPSERT\"))\n",
    "\n",
    "    val deletes = projected.filter(col(\"_change_type\") === \"delete\")\n",
    "      .withColumn(\"_op\", lit(\"DELETE\"))\n",
    "\n",
    "    (upserts, deletes)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2df8f1b-c8e7-4401-91d2-61b399291b8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42ab9d23-b2ae-4111-908f-90096d75bf36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TableResolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "844a86b6-b7b7-4126-a794-b57d9eea64d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ACS"
    }
   },
   "outputs": [],
   "source": [
    "val datasetList: Seq[Dataset] = Seq(Dataset.WosCore, Dataset.Pprn, Dataset.WosEsci)\n",
    "\n",
    "datasetList.foreach { t =>\n",
    "\n",
    "    TableResolver.forDataset(t)\n",
    "    println(\"-------------------------\")\n",
    "    println(s\"Dataset: $t\")\n",
    "    println(ACS.DArticleTotalCites)\n",
    "    println(ACS.DAlmaOpenaccess)\n",
    "    println(ACS.AuthorPublicationLink)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c7811a98-6f56-4530-b43d-edb64b57c99a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DAP"
    }
   },
   "outputs": [],
   "source": [
    "println(\"---------DAP----------------\")\n",
    "println(DAP.Alma.fullName)\n",
    "println(DAP.ApArticle.fullName)\n",
    "println(DAP.IncitesRiOrgGrants.fullName)\n",
    "println(DAP.Authorprofile.fullName)\n",
    "\n",
    "println(\"---------UDM----------------\")\n",
    "println(UDM.GrantsTopic.fullName)\n",
    "println(UDM.ProfileGrantRelation.fullName)\n",
    "println(UDM.ItemTopic.fullName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec9cb648-1469-44cc-99d3-538fed49f513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### CdcType2Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87028a7c-52b1-4cea-b12e-9c3058a35bae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CdcType2Reader - readBaseline"
    }
   },
   "outputs": [],
   "source": [
    "// optional: endVersion, selectedCols, primaryKeyCols\n",
    "\n",
    "\n",
    "TableResolver.forDataset(Dataset.WosCore)\n",
    "println(ACS.FPublication)\n",
    "\n",
    "val df =\n",
    "  CdcType2Reader.readBaseline(\n",
    "    ACS.FPublication.fullName,\n",
    "    endVersion = Some(50L),\n",
    "  // selectedCols = Seq(\"uid\", \"pub_year\", \"__END_AT\"),\n",
    "    Seq(\"uid\")\n",
    "  )\n",
    "\n",
    "// display(baseline)\n",
    "println (df.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae5531be-05a7-42c9-aa5e-af0f01d268cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CdcType2Reader - readIncremental"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val (upserts, deletes) =\n",
    "  CdcType2Reader.readIncremental(\n",
    "    ACS.FPublication.fullName,\n",
    "    startVersion = 57L,\n",
    "    endVersion   = 61L,\n",
    "    Seq(\"uid\")\n",
    "  )\n",
    "\n",
    "  println (s\"upserts: ${upserts.count()}\")\n",
    "  println (s\"deletes  ${deletes.count()}\" )\n",
    " // display(upserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38c00352-0cf8-4ace-9bcd-f5bed99d68c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CdcType2Reader - read"
    }
   },
   "outputs": [],
   "source": [
    "// unified read baseline if startVersion = endVersion, otherwise, incrmental \n",
    "\n",
    "val (upserts, deletes) =\n",
    "  CdcType2Reader.read(\n",
    "      ACS.FPublication.fullName,\n",
    "      startVersion = 57L,\n",
    "      endVersion   = 61L\n",
    "  )\n",
    "\n",
    "println (s\"upserts: ${upserts.count()}\")\n",
    "println (s\"deletes  ${deletes.count()}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d5f8176-9356-4446-a213-bc82cd7cb301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### CdcType1Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6049afe-d92c-4219-8b81-7a247d751a16",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CdcType1Reader - readBaseline"
    }
   },
   "outputs": [],
   "source": [
    "// endVersion - optional\n",
    "\n",
    "println(DAP.JcrMetrics.fullName)\n",
    "\n",
    "val df = CdcType1Reader.readBaseline( \n",
    "    DAP.JcrMetrics.fullName,\n",
    "    endVersion = Some(5L)\n",
    "  )\n",
    "\n",
    "println (s\"df  ${df.count()}\" )\n",
    "//display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d214db4c-0706-42f7-838a-2b3d6f437d33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CdcType1Reader - readIncremental"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val (upserts, deletes) = \n",
    "    CdcType1Reader.readIncremental(\n",
    "        DAP.Authorprofile.fullName,\n",
    "        startVersion = 4,\n",
    "        endVersion = 6\n",
    "    )\n",
    "\n",
    "  println (s\"upserts: ${upserts.count()}\")\n",
    "  println (s\"deletes  ${deletes.count()}\" )\n",
    "// display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7789327-d387-4ce5-b2c1-6cde2652af92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CdcType1Reader - read"
    }
   },
   "outputs": [],
   "source": [
    "// unified read baseline if startVersion = endVersion, otherwise, incrmental \n",
    "\n",
    "val (upserts, deletes) =\n",
    "  CdcType1Reader.read(\n",
    "      DAP.Authorprofile.fullName,\n",
    "      startVersion = 3,\n",
    "      endVersion = 3\n",
    "  )\n",
    "\n",
    "println (s\"upserts: ${upserts.count()}\")\n",
    "println (s\"deletes  ${deletes.count()}\" )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CdcTeader",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}