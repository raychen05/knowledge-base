{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7178bb",
   "metadata": {},
   "source": [
    "#### PipelineRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f83afe",
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "object PipelineRunner {\n",
    "\n",
    "  def run(\n",
    "      pipelineName: String,\n",
    "      queryConfigPath: String,\n",
    "      transformFunc: DataFrame => DataFrame,\n",
    "      targetTable: String,\n",
    "      dryRun: Boolean = false\n",
    "  ): Unit = {\n",
    "\n",
    "    // 1. Start the checkpoint\n",
    "    Checkpoints.markRunStarted(pipelineName)\n",
    "\n",
    "    try {\n",
    "      // 2. Extract\n",
    "      val rawDF = DeltaReader.readDeltaWithWatermark(queryConfigPath)\n",
    "      if (rawDF.isEmpty) {\n",
    "        println(s\"No data found for $pipelineName\")\n",
    "        Checkpoints.markCheckpointSkipped(pipelineName)\n",
    "        return\n",
    "      }\n",
    "\n",
    "      // 3. Transform\n",
    "      val processedDF = transformFunc(rawDF)\n",
    "\n",
    "      // 4. Load\n",
    "      if (!dryRun) {\n",
    "        DeltaWriter.writeDelta(processedDF, targetTable)\n",
    "      }\n",
    "\n",
    "      // 5. Mark success\n",
    "      Checkpoints.markRunCompleted(pipelineName)\n",
    "\n",
    "    } catch {\n",
    "      case e: Throwable =>\n",
    "        // 6. Mark failure\n",
    "        Checkpoints.markRunFailed(pipelineName, Some(e.getMessage))\n",
    "        throw e\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57030e",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92830bd4",
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "// A standard pipelien workflow for a single downstream DAP table\n",
    "// the workdflow can be implemented in pipeline core proces workflow \n",
    "\n",
    "object AuthorprofilePipeline {\n",
    "\n",
    "  /* Setion-1: define functions to for business logic */\n",
    "\n",
    "    // def function-1 ...\n",
    "    // def function-2 ...\n",
    "    // def function-3 ...\n",
    "\n",
    "  // task-1: porcess entity data  and related attributes\n",
    "  def processEntity() {\n",
    "    // logic to process meta data for entity\n",
    "   }\n",
    "\n",
    "  // task-2: porcess article level metric data\n",
    "  def processEntityMetrics() {\n",
    "      // logic to process metric data for entity\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "object Main extends App {\n",
    "\n",
    "  PipelineRunner.run(\n",
    "    pipelineName     = \"EntityPipeline\",\n",
    "    queryConfigPath  = \"config/ap_incremental_sql.yml\",\n",
    "    transformFunc    = AuthorprofilePipeline.processEntity,\n",
    "    targetTable      = \"dap_entity_wos.authorprofile\"\n",
    "  )\n",
    "\n",
    "  PipelineRunner.run(\n",
    "    pipelineName     = \"EntityMetricsPipeline\",\n",
    "    queryConfigPath  = \"config/ap_metrics_sql.yml\",\n",
    "    transformFunc    = Transformations.processEntityMetrics,\n",
    "    targetTable      = \"dap_entity_metrics.authorprofile\"\n",
    "  )\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
